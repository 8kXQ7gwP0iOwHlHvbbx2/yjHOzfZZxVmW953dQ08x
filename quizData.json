[
    {
        "ID": "AI-001",
        "Category": "1.人工知能とは",
        "Question": "1956年に開催され「人工知能」という言葉が初めて使われた会議は？",
        "Opt1": "ダートマス会議",
        "Opt2": "スタンフォード会議",
        "Opt3": "アラン・チューリング会議",
        "Opt4": "マッカーシー会議",
        "Opt5": "エニグマ会議",
        "Opt6": "ウェストファリア会議",
        "Answer_Idx": 0,
        "Explanation": "ジョン・マッカーシーらが提唱した歴史的会議です。",
        "Link": "https://example.com/ai-history"
    },
    {
        "ID": "AI-002",
        "Category": "1.人工知能とは",
        "Question": "人間と対話して相手が機械か判別できなければ知性があるとするテストは？",
        "Opt1": "シンボリック・テスト",
        "Opt2": "マッカーシー・テスト",
        "Opt3": "チューリング・テスト",
        "Opt4": "フレーム・テスト",
        "Opt5": "中国の部屋テスト",
        "Opt6": "フェイゲンバウム・テスト",
        "Answer_Idx": 2,
        "Explanation": "アラン・チューリングが1950年に提唱しました。",
        "Link": "https://example.com/turing-test"
    },
    {
        "ID": "AI-003",
        "Category": "1.人工知能とは",
        "Question": "パズル等の単純な問題は解けても現実の複雑な問題を解けない限界を何と呼ぶか？",
        "Opt1": "フレーム問題",
        "Opt2": "トイ・プロブレム",
        "Opt3": "シンボルグラウンディング問題",
        "Opt4": "モラベックのパラドックス",
        "Opt5": "知識獲得のボトルネック",
        "Opt6": "組み合わせの爆発",
        "Answer_Idx": 1,
        "Explanation": "【ひっかけ】原因は組み合わせの爆発ですが問題自体の名称はトイ・プロブレムです。",
        "Link": "https://example.com/toy-problem"
    },
    {
        "ID": "AI-004",
        "Category": "1.人工知能とは",
        "Question": "AIが無限にある事象の中から関係あるものだけを取り出すことが困難な問題は？",
        "Opt1": "シンボルグラウンディング問題",
        "Opt2": "意味ネットワーク問題",
        "Opt3": "フレーム問題",
        "Opt4": "エキスパートシステム",
        "Opt5": "限定合理性問題",
        "Opt6": "モラベックのパラドックス",
        "Answer_Idx": 2,
        "Explanation": "マッカーシーとヘイズが提唱した。ロボットが爆弾を避ける例えが有名です。",
        "Link": "https://example.com/frame-problem"
    },
    {
        "ID": "AI-005",
        "Category": "1.人工知能とは",
        "Question": "記号とそれらが指し示す現実世界の意味が結びつかないという問題を何というか？",
        "Opt1": "フレーム問題",
        "Opt2": "意味ネットワーク問題",
        "Opt3": "知識獲得のボトルネック",
        "Opt4": "シンボルグラウンディング問題",
        "Opt5": "チューリング・テスト",
        "Opt6": "トイ・プロブレム",
        "Answer_Idx": 3,
        "Explanation": "スティーブン・ハルナッドにより提唱された。シマウマの例えなどが有名です。",
        "Link": "https://example.com/symbol-grounding"
    },
    {
        "ID": "AI-006",
        "Category": "1.人工知能とは",
        "Question": "第1次AIブームの主な手法である「探索・推論」においてチェス等を解く手法は？",
        "Opt1": "ニューラルネットワーク",
        "Opt2": "ディープラーニング",
        "Opt3": "機械学習",
        "Opt4": "ヒューリスティクス",
        "Opt5": "バックプロパゲーション",
        "Opt6": "強化学習",
        "Answer_Idx": 3,
        "Explanation": "経験的なルール（指針）を用いて探索を効率化する手法です。",
        "Link": "https://example.com/heuristics"
    },
    {
        "ID": "AI-007",
        "Category": "1.人工知能とは",
        "Question": "第2次AIブームの主役で専門家の知識をルールとして教え込むシステムは？",
        "Opt1": "ニューラルネットワーク",
        "Opt2": "エキスパートシステム",
        "Opt3": "シンボリックAI",
        "Opt4": "生成AI",
        "Opt5": "エージェントシステム",
        "Opt6": "パーセプトロン",
        "Answer_Idx": 1,
        "Explanation": "知識獲得のボトルネックにより、専門家の知識をすべて記述することの限界に直面しました。",
        "Link": "https://example.com/expert-system"
    },
    {
        "ID": "AI-008",
        "Category": "1.人工知能とは",
        "Question": "「中国の部屋」という思考実験を通じてチューリング・テストを批判した哲学者は？",
        "Opt1": "ジョン・マッカーシー",
        "Opt2": "マービン・ミンスキー",
        "Opt3": "ジョン・サール",
        "Opt4": "レイ・カーツワイル",
        "Opt5": "ジェフリー・ヒントン",
        "Opt6": "アラン・チューリング",
        "Answer_Idx": 2,
        "Explanation": "「理解」を伴わない記号処理は知能ではないと主張しました。",
        "Link": "https://example.com/chinese-room"
    },
    {
        "ID": "AI-009",
        "Category": "1.人工知能とは",
        "Question": "特定のタスクのみをこなす「弱いAI」に対し人間のような自意識を持つAIを何と呼ぶか？",
        "Opt1": "汎用AI（AGI）",
        "Opt2": "特化型AI",
        "Opt3": "強いAI",
        "Opt4": "スーパーインテリジェンス",
        "Opt5": "シンボリックAI",
        "Opt6": "自律AI",
        "Answer_Idx": 2,
        "Explanation": "ジョン・サールによる分類。「強いAI」は自意識や精神を持つものを指します。",
        "Link": "https://example.com/strong-ai"
    },
    {
        "ID": "AI-010",
        "Category": "1.人工知能とは",
        "Question": "AIが人間の知能を超える転換点「シンギュラリティ」を2045年と予測した人物は？",
        "Opt1": "ビル・ゲイツ",
        "Opt2": "レイ・カーツワイル",
        "Opt3": "イーロン・マスク",
        "Opt4": "スティーブン・ホーキング",
        "Opt5": "サム・アルトマン",
        "Opt6": "アンドリュー・ン",
        "Answer_Idx": 1,
        "Explanation": "収穫加速の法則に基づき、技術的特異点の到来を予測しました。",
        "Link": "https://example.com/singularity"
    },
    {
        "ID": "AI-011",
        "Category": "1.人工知能とは",
        "Question": "1950年に世界で最初のチャットボットとされる「ELIZA」を開発した人物は？",
        "Opt1": "ジョセフ・ワイゼンバウム",
        "Opt2": "ジョン・マッカーシー",
        "Opt3": "テリー・ウィノグラード",
        "Opt4": "アラン・チューリング",
        "Opt5": "マービン・ミンスキー",
        "Opt6": "ハーブ・サイモン",
        "Answer_Idx": 0,
        "Explanation": "精神療法的な対話を行うELIZAはジョセフ・ワイゼンバウムによって開発されました。",
        "Link": "https://example.com/eliza"
    },
    {
        "ID": "AI-012",
        "Category": "1.人工知能とは",
        "Question": "「積み木の世界」を理解するプログラム「SHRDLU」を開発したのは誰か？",
        "Opt1": "テリー・ウィノグラード",
        "Opt2": "アラン・ニューウェル",
        "Opt3": "ハーブ・サイモン",
        "Opt4": "ジョン・サール",
        "Opt5": "レイ・カーツワイル",
        "Opt6": "エドワード・フェイゲンバウム",
        "Answer_Idx": 0,
        "Explanation": "SHRDLUは自然言語処理と図形操作を組み合わせた初期の画期的なシステムです。",
        "Link": "https://example.com/shrdlu"
    },
    {
        "ID": "AI-013",
        "Category": "1.人工知能とは",
        "Question": "探索において、根ノードから最も近いノードを順に調べていく手法を何というか？",
        "Opt1": "深さ優先探索",
        "Opt2": "幅優先探索",
        "Opt3": "最良優先探索",
        "Opt4": "A*アルゴリズム",
        "Opt5": "ヒューリスティック探索",
        "Opt6": "ランダム探索",
        "Answer_Idx": 1,
        "Explanation": "幅優先探索は階層ごとに順に探索します。メモリ消費量が多いのが特徴です。",
        "Link": "https://example.com/search-algo"
    },
    {
        "ID": "AI-014",
        "Category": "1.人工知能とは",
        "Question": "知識表現において「AはBの一種である」という関係を示す言葉はどれか？",
        "Opt1": "part-of関係",
        "Opt2": "has-a関係",
        "Opt3": "is-a関係",
        "Opt4": "instance-of関係",
        "Opt5": "use-a関係",
        "Opt6": "link-of関係",
        "Answer_Idx": 2,
        "Explanation": "「カラスは鳥の一種である」はis-a関係（継承関係）と呼ばれます。",
        "Link": "https://example.com/is-a"
    },
    {
        "ID": "AI-015",
        "Category": "1.人工知能とは",
        "Question": "「車輪にはタイヤがついている」のように、全体と部分の関係を示す知識表現は？",
        "Opt1": "is-a関係",
        "Opt2": "instance-of関係",
        "Opt3": "link-of関係",
        "Opt4": "part-of関係",
        "Opt5": "class-of関係",
        "Opt6": "general-of関係",
        "Answer_Idx": 3,
        "Explanation": "part-of関係は構成要素（部分）を示す関係です。",
        "Link": "https://example.com/part-of"
    },
    {
        "ID": "AI-016",
        "Category": "1.人工知能とは",
        "Question": "「コンピュータに明示的にプログラムすることなく学習する能力を与える」と機械学習を定義したのは？",
        "Opt1": "アーサー・サミュエル",
        "Opt2": "ジョン・マッカーシー",
        "Opt3": "ジェフリー・ヒントン",
        "Opt4": "ヤン・ルカン",
        "Opt5": "ヨシュア・ベンジオ",
        "Opt6": "アンドリュー・ン",
        "Answer_Idx": 0,
        "Explanation": "1959年にアーサー・サミュエルによって定義されました。彼はチェッカープログラムの開発者でもあります。",
        "Link": "https://example.com/samuel"
    },
    {
        "ID": "AI-017",
        "Category": "1.人工知能とは",
        "Question": "知識の記述において「概念の体系化」を指す、哲学用語を借用した言葉は何か？",
        "Opt1": "エピステモロジー",
        "Opt2": "オントロジー",
        "Opt3": "コスモロジー",
        "Opt4": "メタフィジックス",
        "Opt5": "現象学",
        "Opt6": "論理学",
        "Answer_Idx": 1,
        "Explanation": "AI分野でのオントロジーは「知識をどのように記述し共有するか」の体系を指します。",
        "Link": "https://example.com/ontology"
    },
    {
        "ID": "AI-018",
        "Category": "1.人工知能とは",
        "Question": "人間が日常的に持っている「常識」をすべてコンピュータに教え込もうとするプロジェクトは？",
        "Opt1": "Cycプロジェクト",
        "Opt2": "ELIZAプロジェクト",
        "Opt3": "Deep Blueプロジェクト",
        "Opt4": "第五世代コンピュータ",
        "Opt5": "ダートマス計画",
        "Opt6": "ニューラルネットワーク計画",
        "Answer_Idx": 0,
        "Explanation": "ダグラス・レナートによるCycプロジェクトは1984年から現在も続いています。",
        "Link": "https://example.com/cyc"
    },
    {
        "ID": "AI-019",
        "Category": "1.人工知能とは",
        "Question": "「意味ネットワーク」において、知識をノード（概念）とリンク（関係）で表現する手法を提案したのは？",
        "Opt1": "クィリアン",
        "Opt2": "ミンスキー",
        "Opt3": "マッカーシー",
        "Opt4": "サミュエル",
        "Opt5": "サール",
        "Opt6": "フェイゲンバウム",
        "Answer_Idx": 0,
        "Explanation": "ロリン・クィリアンが提案しました。人間の記憶の構造を模そうとしたものです。",
        "Link": "https://example.com/semantic-net"
    },
    {
        "ID": "AI-020",
        "Category": "1.人工知能とは",
        "Question": "第2次AIブームのきっかけとなった、日本が国家プロジェクトとして推進した計画は？",
        "Opt1": "次世代AI計画",
        "Opt2": "第五世代コンピュータ",
        "Opt3": "スーパーコンピュータ計画",
        "Opt4": "人工知能振興計画",
        "Opt5": "知能ロボット計画",
        "Opt6": "デジタル日本計画",
        "Answer_Idx": 1,
        "Explanation": "1980年代、通商産業省（当時）が推進し、世界に衝撃を与えました。",
        "Link": "https://example.com/5th-gen"
    },
    {
        "ID": "AI-021",
        "Category": "1.人工知能とは",
        "Question": "Web上の情報に意味（セマンティクス）を付加し、コンピュータが処理しやすくする構想は？",
        "Opt1": "Web 2.0",
        "Opt2": "ダークウェブ",
        "Opt3": "セマンティックWeb",
        "Opt4": "知識Web",
        "Opt5": "メタバース",
        "Opt6": "IoT",
        "Answer_Idx": 2,
        "Explanation": "ティム・バーナーズ＝リーが提唱しました。オントロジー技術が活用されています。",
        "Link": "https://example.com/semantic-web"
    },
    {
        "ID": "AI-022",
        "Category": "1.人工知能とは",
        "Question": "「重い（ヘビーウェイト）オントロジー」を提唱した、日本の人工知能学者は？",
        "Opt1": "溝口理一郎",
        "Opt2": "松尾豊",
        "Opt3": "喜連川優",
        "Opt4": "中島秀之",
        "Opt5": "西田豊明",
        "Opt6": "浅川伸一",
        "Answer_Idx": 0,
        "Explanation": "溝口教授はオントロジー工学の世界的権威として知られています。",
        "Link": "https://example.com/mizoguchi"
    },
    {
        "ID": "AI-023",
        "Category": "1.人工知能とは",
        "Question": "2012年の画像認識コンペILSVRCで、圧倒的な精度で優勝し第3次ブームを決定づけたモデルは？",
        "Opt1": "ResNet",
        "Opt2": "VGG",
        "Opt3": "LeNet",
        "Opt4": "AlexNet",
        "Opt5": "Inception",
        "Opt6": "Transformer",
        "Answer_Idx": 3,
        "Explanation": "ジェフリー・ヒントン教授のチームが開発したAlexNetがディープラーニングの威力を証明しました。",
        "Link": "https://example.com/alexnet"
    },
    {
        "ID": "AI-024",
        "Category": "1.人工知能とは",
        "Question": "2016年に囲碁の世界王者を破り、世界に衝撃を与えたAIの名前は？",
        "Opt1": "AlphaFold",
        "Opt2": "AlphaGo",
        "Opt3": "Deep Blue",
        "Opt4": "Watson",
        "Opt5": "Stockfish",
        "Opt6": "Bonanza",
        "Answer_Idx": 1,
        "Explanation": "Google DeepMindが開発したAlphaGoがイ・セドル氏に勝利しました。",
        "Link": "https://example.com/alphago"
    },
    {
        "ID": "AI-025",
        "Category": "1.人工知能とは",
        "Question": "「知識獲得のボトルネック」という言葉を用いて、エキスパートシステムの限界を指摘したのは？",
        "Opt1": "フェイゲンバウム",
        "Opt2": "ミンスキー",
        "Opt3": "サール",
        "Opt4": "ウィノグラード",
        "Opt5": "サミュエル",
        "Opt6": "レナート",
        "Answer_Idx": 0,
        "Explanation": "エドワード・フェイゲンバウムは「知識工学」の父としても知られます。",
        "Link": "https://example.com/bottleneck"
    },
    {
        "ID": "AI-026",
        "Category": "1.人工知能とは",
        "Question": "「フレーム」という概念を用いて、人間の知識が典型的な状況の枠組みとして表現されると主張したのは？",
        "Opt1": "マービン・ミンスキー",
        "Opt2": "ジョン・マッカーシー",
        "Opt3": "アラン・ニューウェル",
        "Opt4": "ハーブ・サイモン",
        "Opt5": "ジョン・サール",
        "Opt6": "ロジャー・シャンク",
        "Answer_Idx": 0,
        "Explanation": "ミンスキーは「フレーム理論」を提唱し、知識表現に大きな影響を与えました。",
        "Link": "https://example.com/minsky"
    },
    {
        "ID": "AI-027",
        "Category": "1.人工知能とは",
        "Question": "IBMのAI「Watson」が2011年に勝利した、米国の有名なクイズ番組の名前は？",
        "Opt1": "ザ・プライス・イズ・ライト",
        "Opt2": "ホイール・オブ・フォーチュン",
        "Opt3": "ジェパディ！",
        "Opt4": "フー・ウォンツ・トゥ・ビー・ア・ミリオネア",
        "Opt5": "サバイバー",
        "Opt6": "アメージング・レース",
        "Answer_Idx": 2,
        "Explanation": "自然言語の質問を理解して回答する能力を示しました。",
        "Link": "https://example.com/watson"
    },
    {
        "ID": "AI-028",
        "Category": "1.人工知能とは",
        "Question": "1997年にチェスの世界王者ガルリ・カスパロフを破ったIBMのコンピュータは？",
        "Opt1": "Deep Thought",
        "Opt2": "Deep Blue",
        "Opt3": "Deep Mind",
        "Opt4": "Blue Gene",
        "Opt5": "Watson",
        "Opt6": "AlphaChess",
        "Answer_Idx": 1,
        "Explanation": "第2次ブームの終わりと第3次ブームの繋ぎ目における重要な出来事です。",
        "Link": "https://example.com/deepblue"
    },
    {
        "ID": "AI-029",
        "Category": "1.人工知能とは",
        "Question": "第3次AIブームの要因として適切でないものはどれか？",
        "Opt1": "ビッグデータの普及",
        "Opt2": "GPUの進化",
        "Opt3": "インターネットの普及",
        "Opt4": "フレーム問題の完全な解決",
        "Opt5": "アルゴリズムの改良",
        "Opt6": "クラウドの普及",
        "Answer_Idx": 3,
        "Explanation": "フレーム問題は現在も「完全な解決」には至っておらず、AIの根本的課題の一つです。",
        "Link": "https://example.com/ai-trends"
    },
    {
        "ID": "AI-030",
        "Category": "1.人工知能とは",
        "Question": "汎用AI（AGI）の定義として、最も適切なものはどれか？",
        "Opt1": "特定のゲームで人間に勝つAI",
        "Opt2": "自動運転に特化したAI",
        "Opt3": "人間と同じように多様なタスクをこなせるAI",
        "Opt4": "画像生成だけを行うAI",
        "Opt5": "翻訳だけを行うAI",
        "Opt6": "大量の計算を高速で行うAI",
        "Answer_Idx": 2,
        "Explanation": "AGI（Artificial General Intelligence）は、人間ができるあらゆる知的な作業を遂行できるAIを指します。",
        "Link": "https://example.com/agi"
    },
    {
        "ID": "AI-031",
        "Category": "1.人工知能とは",
        "Question": "特定の分野（専門知識など）において、人間と同等以上の能力を発揮するAIの分類は？",
        "Opt1": "汎用AI",
        "Opt2": "特化型AI",
        "Opt3": "強いAI",
        "Opt4": "超知能",
        "Opt5": "マルチモーダルAI",
        "Opt6": "自律型AI",
        "Answer_Idx": 1,
        "Explanation": "現在の主流なAI（翻訳、画像診断など）はすべて特化型AIに分類されます。",
        "Link": "https://example.com/narrow-ai"
    },
    {
        "ID": "AI-032",
        "Category": "1.人工知能とは",
        "Question": "「AIが進化し、人間の手で制御できなくなるほど急速に技術が進歩する時点」を指す言葉は？",
        "Opt1": "ビッグバン",
        "Opt2": "シンギュラリティ",
        "Opt3": "イベント・ホライゾン",
        "Opt4": "ティッピング・ポイント",
        "Opt5": "パラダイム・シフト",
        "Opt6": "デジタルトランスフォーメーション",
        "Answer_Idx": 1,
        "Explanation": "レイ・カーツワイルが提唱した「技術的特異点」のことです。",
        "Link": "https://example.com/singularity-detail"
    },
    {
        "ID": "AI-033",
        "Category": "1.人工知能とは",
        "Question": "「知能指数（IQ）」の測定をAIに対して行い、その知性を評価することは現在の主流か？",
        "Opt1": "はい、主流である",
        "Opt2": "いいえ、主流ではない",
        "Opt3": "一部の国で義務化されている",
        "Opt4": "1950年代に廃止された",
        "Opt5": "生成AIにのみ適用される",
        "Opt6": "ロボットにのみ適用される",
        "Answer_Idx": 1,
        "Explanation": "現在のAI評価はタスクの精度（Accuracyなど）で行われ、人間用のIQテストは一般的ではありません。",
        "Link": "https://example.com/ai-eval"
    },
    {
        "ID": "AI-034",
        "Category": "1.人工知能とは",
        "Question": "記号論理学に基づき、記号を操作することで知能を実現しようとする立場を何というか？",
        "Opt1": "コネクショニズム",
        "Opt2": "シンボリズム",
        "Opt3": "ディープラーニング",
        "Opt4": "行動主義",
        "Opt5": "構造主義",
        "Opt6": "経験主義",
        "Answer_Idx": 1,
        "Explanation": "第1次・第2次ブームの主流だった考え方です（記号主義）。",
        "Link": "https://example.com/symbolism"
    },
    {
        "ID": "AI-035",
        "Category": "1.人工知能とは",
        "Question": "神経細胞のネットワークを模倣することで知能を実現しようとする立場を何というか？",
        "Opt1": "シンボリズム",
        "Opt2": "コネクショニズム",
        "Opt3": "論理主義",
        "Opt4": "実存主義",
        "Opt5": "合理主義",
        "Opt6": "還元主義",
        "Answer_Idx": 1,
        "Explanation": "ディープラーニングの基礎となる考え方です（結合主義）。",
        "Link": "https://example.com/connectionism"
    },
    {
        "ID": "AI-036",
        "Category": "1.人工知能とは",
        "Question": "「コンピュータは記号の並べ替えをしているだけで、内容を理解していない」という批判は？",
        "Opt1": "フレーム問題",
        "Opt2": "トイ・プロブレム",
        "Opt3": "中国の部屋",
        "Opt4": "モラベックのパラドックス",
        "Opt5": "シンボルグラウンディング問題",
        "Opt6": "ノー・フリーランチ",
        "Answer_Idx": 2,
        "Explanation": "ジョン・サールが「強いAI」の可能性を否定するために用いた議論です。",
        "Link": "https://example.com/chinese-room"
    },
    {
        "ID": "AI-037",
        "Category": "1.人工知能とは",
        "Question": "「高度な推論よりも、幼児が行うような日常的な知覚や運動の方がコンピュータには難しい」という逆説は？",
        "Opt1": "フレーム問題",
        "Opt2": "モラベックのパラドックス",
        "Opt3": "アムダールの法則",
        "Opt4": "ムーアの法則",
        "Opt5": "フェルミ推定",
        "Opt6": "ゲーデルの不完全性定理",
        "Answer_Idx": 1,
        "Explanation": "ハンス・モラベックらによって指摘されました。",
        "Link": "https://example.com/moravec"
    },
    {
        "ID": "AI-038",
        "Category": "1.人工知能とは",
        "Question": "専門家（医師や弁護士など）の知識を「if-then」形式のルールで蓄積したシステムは？",
        "Opt1": "ニューラルネットワーク",
        "Opt2": "エキスパートシステム",
        "Opt3": "決定木",
        "Opt4": "サポートベクターマシン",
        "Opt5": "ランダムフォレスト",
        "Opt6": "ベイジアンネットワーク",
        "Answer_Idx": 1,
        "Explanation": "第2次AIブームの主役ですが、知識の維持管理が困難という課題がありました。",
        "Link": "https://example.com/expert-system-2"
    },
    {
        "ID": "AI-039",
        "Category": "1.人工知能とは",
        "Question": "第1次AIブームで開発された、初期のニューラルネットワークモデルを何というか？",
        "Opt1": "CNN",
        "Opt2": "RNN",
        "Opt3": "パーセプトロン",
        "Opt4": "LSTM",
        "Opt5": "Transformer",
        "Opt6": "ResNet",
        "Answer_Idx": 2,
        "Explanation": "フランク・ローゼンブラットが1958年に発表しました。",
        "Link": "https://example.com/perceptron"
    },
    {
        "ID": "AI-040",
        "Category": "1.人工知能とは",
        "Question": "1969年に「パーセプトロン」の限界（線形分離不可能な問題が解けない）を指摘し、第1次ブームを終わらせたのは？",
        "Opt1": "マッカーシーとミンスキー",
        "Opt2": "ミンスキーとパパート",
        "Opt3": "サールとサミュエル",
        "Opt4": "ヒントンとルカン",
        "Opt5": "ニューウェルとサイモン",
        "Opt6": "カーツワイルとゲイツ",
        "Answer_Idx": 1,
        "Explanation": "ミンスキーとパパートの共著書により、ニューラルネットワーク研究は一時衰退しました。",
        "Link": "https://example.com/perceptron-limit"
    },
    {
        "ID": "AI-041",
        "Category": "1.人工知能とは",
        "Question": "「猫の認識」において、人間が特徴を教えるのではなくAIが自ら特徴を抽出した2012年の研究成果は？",
        "Opt1": "Googleの猫",
        "Opt2": "Facebookの犬",
        "Opt3": "IBMのチェス",
        "Opt4": "Microsoftの翻訳",
        "Opt5": "OpenAIの文章生成",
        "Opt6": "Appleの音声認識",
        "Answer_Idx": 0,
        "Explanation": "ジェフ・ディーンとアンドリュー・ンによる、大規模ニューラルネットワークの研究です。",
        "Link": "https://example.com/google-cat"
    },
    {
        "ID": "AI-042",
        "Category": "1.人工知能とは",
        "Question": "「AI倫理」において、AIが特定の属性に対して不当な判断を下さないようにする概念を何というか？",
        "Opt1": "透明性",
        "Opt2": "説明責任",
        "Opt3": "バイアスの除去",
        "Opt4": "堅牢性",
        "Opt5": "プライバシー保護",
        "Opt6": "データ利活用",
        "Answer_Idx": 2,
        "Explanation": "学習データに含まれる偏りがAIの判断に影響する問題が重要視されています。",
        "Link": "https://example.com/ai-bias"
    },
    {
        "ID": "AI-043",
        "Category": "1.人工知能とは",
        "Question": "「意味ネットワーク」において、上位概念の性質を下位概念が自動的に引き継ぐことを何というか？",
        "Opt1": "継承（インヘリタンス）",
        "Opt2": "カプセル化",
        "Opt3": "多態性",
        "Opt4": "オーバーライド",
        "Opt5": "抽象化",
        "Opt6": "委譲",
        "Answer_Idx": 0,
        "Explanation": "「鳥は飛ぶ」という性質を「スズメ」が引き継ぐような仕組みです。",
        "Link": "https://example.com/inheritance"
    },
    {
        "ID": "AI-044",
        "Category": "1.人工知能とは",
        "Question": "オントロジーにおいて、誰にとっても共通の不変的な概念体系を目指すものを何というか？",
        "Opt1": "ライトウェイト・オントロジー",
        "Opt2": "ヘビーウェイト・オントロジー",
        "Opt3": "ドメイン・オントロジー",
        "Opt4": "タスク・オントロジー",
        "Opt5": "アプリケーション・オントロジー",
        "Opt6": "汎用オントロジー",
        "Answer_Idx": 1,
        "Explanation": "溝口教授らが提唱した、厳密な意味記述を目指すオントロジーです。",
        "Link": "https://example.com/heavyweight"
    },
    {
        "ID": "AI-045",
        "Category": "1.人工知能とは",
        "Question": "1956年のダートマス会議の提案書において、主要なメンバーとして名を連ねていないのは誰か？",
        "Opt1": "ジョン・マッカーシー",
        "Opt2": "マービン・ミンスキー",
        "Opt3": "クロード・シャノン",
        "Opt4": "ナサニエル・ロチェスター",
        "Opt5": "アラン・チューリング",
        "Opt6": "ハーブ・サイモン",
        "Answer_Idx": 4,
        "Explanation": "チューリングは1954年に他界しており、会議には参加していません。",
        "Link": "https://example.com/dartmouth-members"
    },
    {
        "ID": "AI-046",
        "Category": "1.人工知能とは",
        "Question": "1980年代に提唱された、記号と実世界の身体的経験を結びつける重要性を説くアプローチは？",
        "Opt1": "物理記号システム仮説",
        "Opt2": "身体性（Embodiment）",
        "Opt3": "サブサンプション・アーキテクチャ",
        "Opt4": "知識工学",
        "Opt5": "形式論理学",
        "Opt6": "行動心理学",
        "Answer_Idx": 1,
        "Explanation": "ロドニー・ブルックスらが提唱した、環境との相互作用を重視する考え方です。",
        "Link": "https://example.com/embodiment"
    },
    {
        "ID": "AI-047",
        "Category": "1.人工知能とは",
        "Question": "現在の生成AI（ChatGPT等）の爆発的普及を、AIの歴史上何次ブームと呼ぶことがあるか？",
        "Opt1": "第3次ブームの継続",
        "Opt2": "第4次ブーム",
        "Opt3": "第5次ブーム",
        "Opt4": "第2次ブームの再来",
        "Opt5": "ポストAI時代",
        "Opt6": "AIの特異点",
        "Answer_Idx": 1,
        "Explanation": "まだ定義は定まっていませんが、生成AIの登場を第4次ブームと呼ぶ議論が増えています。",
        "Link": "https://example.com/4th-boom"
    },
    {
        "ID": "AI-048",
        "Category": "1.人工知能とは",
        "Question": "「コンピュータが意識を持つかどうか」ではなく「知的と言える行動をするか」に焦点を当てたテストは？",
        "Opt1": "サール・テスト",
        "Opt2": "チューリング・テスト",
        "Opt3": "ミンスキー・テスト",
        "Opt4": "マッカーシー・テスト",
        "Opt5": "デカルト・テスト",
        "Opt6": "カント・テスト",
        "Answer_Idx": 1,
        "Explanation": "機能主義的な立場から知能を判定するテストです。",
        "Link": "https://example.com/turing-test-focus"
    },
    {
        "ID": "AI-049",
        "Category": "1.人工知能とは",
        "Question": "ダートマス会議のメンバーで、後に「認知心理学」や「限定合理性」でノーベル経済学賞を受賞したのは？",
        "Opt1": "ハーブ・サイモン",
        "Opt2": "ジョン・マッカーシー",
        "Opt3": "マービン・ミンスキー",
        "Opt4": "クロード・シャノン",
        "Opt5": "アーサー・サミュエル",
        "Opt6": "ジョン・フォン・ノイマン",
        "Answer_Idx": 0,
        "Explanation": "ハーブ・サイモンはAIの父の一人でありながら、多岐にわたる分野で功績を残しました。",
        "Link": "https://example.com/herbert-simon"
    },
    {
        "ID": "AI-050",
        "Category": "1.人工知能とは",
        "Question": "AI開発における「ELSI」という概念に含まれないものはどれか？",
        "Opt1": "倫理的（Ethical）",
        "Opt2": "法的（Legal）",
        "Opt3": "社会的（Social）",
        "Opt4": "課題（Issues）",
        "Opt5": "情報的（Informational）",
        "Opt6": "経済的（Economic）",
        "Answer_Idx": 4,
        "Explanation": "ELSIは「倫理的・法的・社会的課題」を指します。経済（Economic）は直接は含まれませんが、選択肢6も文脈によりますが、IはInformationalではありません。",
        "Link": "https://example.com/elsi"
    },
    {
        "ID": "TREND-001",
        "Category": "2.人工知能をめぐる動向",
        "Question": "迷路やパズルの解法で、出発点から順にすべての分岐を網羅的に調べる探索手法は？",
        "Opt1": "深さ優先探索",
        "Opt2": "幅優先探索",
        "Opt3": "山登り法",
        "Opt4": "最良優先探索",
        "Opt5": "A*アルゴリズム",
        "Opt6": "双方向探索",
        "Answer_Idx": 1,
        "Explanation": "幅優先探索は、最短解を確実に見つけられますが、メモリ消費が激しいのが特徴です。",
        "Link": "https://example.com/bfs"
    },
    {
        "ID": "TREND-002",
        "Category": "2.人工知能をめぐる動向",
        "Question": "探索において、ゴールまでの推定距離（コスト）を「ヒューリスティック関数」として用いる手法は？",
        "Opt1": "幅優先探索",
        "Opt2": "深さ優先探索",
        "Opt3": "A*アルゴリズム",
        "Opt4": "均一コスト探索",
        "Opt5": "ランダムウォーク",
        "Opt6": "全探索",
        "Answer_Idx": 2,
        "Explanation": "A*（エースター）は「開始点からのコスト」と「ゴールまでの推定コスト」を合計して判断します。",
        "Link": "https://example.com/astar"
    },
    {
        "ID": "TREND-003",
        "Category": "2.人工知能をめぐる動向",
        "Question": "対戦ゲームにおいて、相手が最善手を打つと仮定して自分の利益を最大化する探索アルゴリズムは？",
        "Opt1": "A*アルゴリズム",
        "Opt2": "ミニマックス法",
        "Opt3": "幅優先探索",
        "Opt4": "山登り法",
        "Opt5": "遺伝的アルゴリズム",
        "Opt6": "Q学習",
        "Answer_Idx": 1,
        "Explanation": "ミニマックス法は、自分が最大化、相手が最小化することからその名がつきました。",
        "Link": "https://example.com/minimax"
    },
    {
        "ID": "TREND-004",
        "Category": "2.人工知能をめぐる動向",
        "Question": "ミニマックス法の探索において、明らかに無駄な枝をカットして高速化する手法を何というか？",
        "Opt1": "枝刈り（αβ法）",
        "Opt2": "ドロップアウト",
        "Opt3": "プーリング",
        "Opt4": "正則化",
        "Opt5": "バックトラッキング",
        "Opt6": "量子化",
        "Answer_Idx": 0,
        "Explanation": "αβ（アルファベータ）法により、探索の効率を劇的に向上させることができます。",
        "Link": "https://example.com/alphabeta"
    },
    {
        "ID": "TREND-005",
        "Category": "2.人工知能をめぐる動向",
        "Question": "「意味ネットワーク」において、ある概念が属性を上位概念から受け継ぐことを何と呼ぶか？",
        "Opt1": "継承（インヘリタンス）",
        "Opt2": "多態性",
        "Opt3": "カプセル化",
        "Opt4": "オーバーライド",
        "Opt5": "抽象化",
        "Opt6": "委譲",
        "Answer_Idx": 0,
        "Explanation": "オブジェクト指向プログラミングの基礎にもなっている概念です。",
        "Link": "https://example.com/inheritance"
    },
    {
        "ID": "TREND-006",
        "Category": "2.人工知能をめぐる動向",
        "Question": "エキスパートシステムの構築において、専門家から知識を聞き出すのが困難な現象を何というか？",
        "Opt1": "フレーム問題",
        "Opt2": "トイ・プロブレム",
        "Opt3": "知識獲得のボトルネック",
        "Opt4": "シンボルグラウンディング問題",
        "Opt5": "モラベックのパラドックス",
        "Opt6": "ノー・フリーランチ",
        "Answer_Idx": 2,
        "Explanation": "エドワード・フェイゲンバウムによって指摘されました。",
        "Link": "https://example.com/bottleneck"
    },
    {
        "ID": "TREND-007",
        "Category": "2.人工知能をめぐる動向",
        "Question": "「知識をコンピュータが扱えるように体系化する」という技術領域の名称は？",
        "Opt1": "データサイエンス",
        "Opt2": "知識工学",
        "Opt3": "システム工学",
        "Opt4": "認知科学",
        "Opt5": "言語工学",
        "Opt6": "情報理論",
        "Answer_Idx": 1,
        "Explanation": "第2次AIブームの中心となった学問領域です。",
        "Link": "https://example.com/knowledge-eng"
    },
    {
        "ID": "TREND-008",
        "Category": "2.人工知能をめぐる動向",
        "Question": "IBMのWatsonがクイズ番組で勝利した際、用いられた膨大な知識のベースとなった百科事典サイトは？",
        "Opt1": "Britannica",
        "Opt2": "Encarta",
        "Opt3": "Wikipedia",
        "Opt4": "MSNサーチ",
        "Opt5": "Google Scholar",
        "Opt6": "Yahoo!知恵袋",
        "Answer_Idx": 2,
        "Explanation": "Wikipedia等のオープンな知識ベースを活用して回答を生成しました。",
        "Link": "https://example.com/watson-wiki"
    },
    {
        "ID": "TREND-009",
        "Category": "2.人工知能をめぐる動向",
        "Question": "オントロジーにおいて、特定のドメイン（領域）に限定せず、汎用的な概念を定義したものを何というか？",
        "Opt1": "ドメイン・オントロジー",
        "Opt2": "タスク・オントロジー",
        "Opt3": "上位オントロジー",
        "Opt4": "下位オントロジー",
        "Opt5": "アプリケーション・オントロジー",
        "Opt6": "ライトウェイト・オントロジー",
        "Answer_Idx": 2,
        "Explanation": "Upper Ontology（上位オントロジー）は、空間や時間などの根本概念を定義します。",
        "Link": "https://example.com/upper-ontology"
    },
    {
        "ID": "TREND-010",
        "Category": "2.人工知能をめぐる動向",
        "Question": "2000年代以降、データから機械が自動的に学習する「統計的機械学習」が普及した主な要因は？",
        "Opt1": "ルールの手書き入力",
        "Opt2": "インターネットとビッグデータ",
        "Opt3": "エキスパートシステムの進化",
        "Opt4": "第5世代コンピュータの完成",
        "Opt5": "線形分離器の発見",
        "Opt6": "探索アルゴリズムの完成",
        "Answer_Idx": 1,
        "Explanation": "大量のデータと計算資源（GPU等）の普及が第3次ブームを支えました。",
        "Link": "https://example.com/ml-history"
    },
    {
        "ID": "TREND-011",
        "Category": "2.人工知能をめぐる動向",
        "Question": "探索において「現在の状態からゴールにどれくらい近いか」を推測する関数の名称は？",
        "Opt1": "コスト関数",
        "Opt2": "活性化関数",
        "Opt3": "ヒューリスティック関数",
        "Opt4": "損失関数",
        "Opt5": "目的関数",
        "Opt6": "シグモイド関数",
        "Answer_Idx": 2,
        "Explanation": "ヒューリスティック関数は、経験則に基づいて「もっともらしさ」を数値化する関数です。",
        "Link": "https://www.ai-gakkai.or.jp/whatsai/AIpoint/AIPoint1.html"
    },
    {
        "ID": "TREND-012",
        "Category": "2.人工知能をめぐる動向",
        "Question": "A*アルゴリズムの評価関数 f(n) = g(n) + h(n) において、h(n) が表すものはどれか？",
        "Opt1": "スタート地点から現在のノードまでの距離",
        "Opt2": "現在のノードからゴールまでの推定距離",
        "Opt3": "これまでの総移動ステップ数",
        "Opt4": "探索の分岐数",
        "Opt5": "ノードの深さ",
        "Opt6": "計算時間",
        "Answer_Idx": 1,
        "Explanation": "g(n)は「過去のコスト」、h(n)は「未来（ゴール）への推定コスト」を指します。",
        "Link": "https://example.com/astar-detail"
    },
    {
        "ID": "TREND-013",
        "Category": "2.人工知能をめぐる動向",
        "Question": "「現在の状態よりも良い状態があれば移動する」という単純な探索手法を何というか？",
        "Opt1": "幅優先探索",
        "Opt2": "山登り法",
        "Opt3": "深さ優先探索",
        "Opt4": "シミュレーテッド・アニーリング",
        "Opt5": "遺伝的アルゴリズム",
        "Opt6": "強化学習",
        "Answer_Idx": 1,
        "Explanation": "山登り法は局所的な最適解（局所解）に陥りやすいという欠点があります。",
        "Link": "https://example.com/hill-climbing"
    },
    {
        "ID": "TREND-014",
        "Category": "2.人工知能をめぐる動向",
        "Question": "山登り法において、周囲にこれ以上高い場所がない状態（局所最適解）に陥ることを何というか？",
        "Opt1": "グローバル・オプティマム",
        "Opt2": "ローカル・オプティマム",
        "Opt3": "鞍点",
        "Opt4": "オーバーフィッティング",
        "Opt5": "バニシング・グラディエント",
        "Opt6": "デッドロック",
        "Answer_Idx": 1,
        "Explanation": "局所解（ローカル・オプティマム）に捕まると、真の最適解（グローバル解）に到達できません。",
        "Link": "https://example.com/local-optima"
    },
    {
        "ID": "TREND-015",
        "Category": "2.人工知能をめぐる動向",
        "Question": "囲碁AI「AlphaGo」において、次に打つべき手を選択するために用いられた手法は？",
        "Opt1": "幅優先探索",
        "Opt2": "深さ優先探索",
        "Opt3": "モンテカルロ木探索",
        "Opt4": "A*アルゴリズム",
        "Opt5": "線形探索",
        "Opt6": "二分探索",
        "Answer_Idx": 2,
        "Explanation": "ランダムなシミュレーションを繰り返して勝率の高い手を探る手法です。",
        "Link": "https://example.com/alphago-tech"
    },
    {
        "ID": "TREND-016",
        "Category": "2.人工知能をめぐる動向",
        "Question": "「一度通った道に戻る」ことで、行き止まりに当たっても別の道を探す探索の仕組みを何というか？",
        "Opt1": "バックプロパゲーション",
        "Opt2": "バックトラッキング",
        "Opt3": "フォワードチェイニング",
        "Opt4": "リダイレクト",
        "Opt5": "ドロップアウト",
        "Opt6": "プーリング",
        "Answer_Idx": 1,
        "Explanation": "深さ優先探索などで、行き止まりから戻って再探索することをバックトラッキング（後戻り）と言います。",
        "Link": "https://example.com/backtracking"
    },
    {
        "ID": "TREND-017",
        "Category": "2.人工知能をめぐる動向",
        "Question": "知識表現の「フレーム」において、属性（項目）のことを「スロット」と呼ぶが、その値を何と呼ぶか？",
        "Opt1": "ビット",
        "Opt2": "ノード",
        "Opt3": "フィラー",
        "Opt4": "リンク",
        "Opt5": "インスタンス",
        "Opt6": "クラス",
        "Answer_Idx": 2,
        "Explanation": "スロット（属性名）に対して、具体的な値が入る場所を「フィラー」と呼びます。",
        "Link": "https://example.com/frame-theory"
    },
    {
        "ID": "TREND-018",
        "Category": "2.人工知能をめぐる動向",
        "Question": "1980年代に提唱された「物理記号システム仮説」を提唱したのは？",
        "Opt1": "ヒントンとルカン",
        "Opt2": "ニューウェルとサイモン",
        "Opt3": "マッカーシーとミンスキー",
        "Opt4": "サールとサミュエル",
        "Opt5": "ウィノグラードとワイゼンバウム",
        "Opt6": "カーツワイルとゲイツ",
        "Answer_Idx": 1,
        "Explanation": "「知的な行動は記号処理によって実現できる」という記号主義の根本原理です。",
        "Link": "https://example.com/physical-symbol"
    },
    {
        "ID": "TREND-019",
        "Category": "2.人工知能をめぐる動向",
        "Question": "「知識を共有し、再利用可能にするための概念の明示的な仕様」とオントロジーを定義したのは？",
        "Opt1": "トム・グルーバー",
        "Opt2": "ジョン・サール",
        "Opt3": "レイ・カーツワイル",
        "Opt4": "ジェフリー・ヒントン",
        "Opt5": "松尾豊",
        "Opt6": "溝口理一郎",
        "Answer_Idx": 0,
        "Explanation": "トム・グルーバーによるこの定義がAI分野では最も一般的です。",
        "Link": "https://example.com/gruber-ontology"
    },
    {
        "ID": "TREND-020",
        "Category": "2.人工知能をめぐる動向",
        "Question": "意味ネットワークにおいて「上位概念の性質を自動的に受け継ぐ」仕組みを何というか？",
        "Opt1": "オーバーライド",
        "Opt2": "ポリモーフィズム",
        "Opt3": "インヘリタンス（継承）",
        "Opt4": "カプセル化",
        "Opt5": "インダクション",
        "Opt6": "デダクション",
        "Answer_Idx": 2,
        "Explanation": "「動物は動く」→「犬は動物である」→「犬は動く」という推論が可能になります。",
        "Link": "https://example.com/inheritance-ai"
    },
    {
        "ID": "TREND-021",
        "Category": "2.人工知能をめぐる動向",
        "Question": "探索の計算量が状態数の増加に伴って指数関数的に増大し、解けなくなる現象を何というか？",
        "Opt1": "データの爆発",
        "Opt2": "次元の呪い",
        "Opt3": "組み合わせの爆発",
        "Opt4": "計算資源の枯渇",
        "Opt5": "オーバーフロー",
        "Opt6": "勾配消失問題",
        "Answer_Idx": 2,
        "Explanation": "トイ・プロブレムの主な原因となった現象です。",
        "Link": "https://example.com/combinatorial-explosion"
    },
    {
        "ID": "TREND-022",
        "Category": "2.人工知能をめぐる動向",
        "Question": "「3つのディスクと3本の杭」を使い、最小の手順でディスクを移動させるパズルを何というか？",
        "Opt1": "エイト・クイーン",
        "Opt2": "ハノイの塔",
        "Opt3": "数独",
        "Opt4": "モンティ・ホール問題",
        "Opt5": "巡回セールスマン問題",
        "Opt6": "ナップサック問題",
        "Answer_Idx": 1,
        "Explanation": "第1次AIブームで「探索」の例題としてよく使われました。",
        "Link": "https://example.com/hanoi"
    },
    {
        "ID": "TREND-023",
        "Category": "2.人工知能をめぐる動向",
        "Question": "チェスや将棋の探索において、ゲームの終了（勝ち負け）まで行かずに途中の盤面を評価する値を何というか？",
        "Opt1": "損失値",
        "Opt2": "報酬",
        "Opt3": "評価値",
        "Opt4": "確率",
        "Opt5": "バイアス",
        "Opt6": "重み",
        "Answer_Idx": 2,
        "Explanation": "評価関数によって算出される、その局面の有利・不利を示す数値です。",
        "Link": "https://example.com/eval-value"
    },
    {
        "ID": "TREND-024",
        "Category": "2.人工知能をめぐる動向",
        "Question": "ある知識から新しい知識を導き出す「推論」において、一般的な法則を個別の事例に適用する手法は？",
        "Opt1": "帰納的推論",
        "Opt2": "演繹的推論",
        "Opt3": "アブダクション",
        "Opt4": "類推（アナロジー）",
        "Opt5": "統計的推論",
        "Opt6": "ベイズ推論",
        "Answer_Idx": 1,
        "Explanation": "「すべての人は死ぬ」「ソクラテスは人である」故に「ソクラテスは死ぬ」が演繹法です。",
        "Link": "https://example.com/deduction"
    },
    {
        "ID": "TREND-025",
        "Category": "2.人工知能をめぐる動向",
        "Question": "多数の個別の事例から、共通する一般的な法則を導き出す推論手法を何というか？",
        "Opt1": "演繹的推論",
        "Opt2": "帰納的推論",
        "Opt3": "アブダクション",
        "Opt4": "逆向き推論",
        "Opt5": "仮説推論",
        "Opt6": "モンテカルロ法",
        "Answer_Idx": 1,
        "Explanation": "データからルールを見つける機械学習は、帰納的推論の一種と言えます。",
        "Link": "https://example.com/induction"
    },
    {
        "ID": "TREND-026",
        "Category": "2.人工知能をめぐる動向",
        "Question": "第1次AIブームの頃に提唱された、人間のように考え、話すコンピュータという概念を指す言葉は？",
        "Opt1": "機械学習",
        "Opt2": "サイバネティックス",
        "Opt3": "認知科学",
        "Opt4": "データサイエンス",
        "Opt5": "ディープラーニング",
        "Opt6": "ロボティクス",
        "Answer_Idx": 1,
        "Explanation": "ノーバート・ウィーナーが提唱した、通信と制御の統一的な理論です。",
        "Link": "https://example.com/cybernetics"
    },
    {
        "ID": "TREND-027",
        "Category": "2.人工知能をめぐる動向",
        "Question": "探索において、評価値の高い順にノードを調べていくが、最短経路を保証しない手法は？",
        "Opt1": "幅優先探索",
        "Opt2": "最良優先探索",
        "Opt3": "ダイクストラ法",
        "Opt4": "全探索",
        "Opt5": "深さ優先探索",
        "Opt6": "ランダム探索",
        "Answer_Idx": 1,
        "Explanation": "現在のノードからゴールへの近さ（h(n)）だけを見る手法を指すことが多いです。",
        "Link": "https://example.com/best-first"
    },
    {
        "ID": "TREND-028",
        "Category": "2.人工知能をめぐる動向",
        "Question": "「すべての可能な経路を調べれば必ず正解にたどり着く」という探索の性質を何というか？",
        "Opt1": "完全性",
        "Opt2": "最適性",
        "Opt3": "効率性",
        "Opt4": "頑健性",
        "Opt5": "透明性",
        "Opt6": "再現性",
        "Answer_Idx": 0,
        "Explanation": "幅優先探索は完全性を持ちますが、深さ優先探索は無限ループがあると持ちません。",
        "Link": "https://example.com/completeness"
    },
    {
        "ID": "TREND-029",
        "Category": "2.人工知能をめぐる動向",
        "Question": "チェスの探索において、盤面の駒の配置をグラフ構造として表現した際、各状態を何と呼ぶか？",
        "Opt1": "エッジ",
        "Opt2": "リンク",
        "Opt3": "ノード",
        "Opt4": "スロット",
        "Opt5": "フィラー",
        "Opt6": "フレーム",
        "Answer_Idx": 2,
        "Explanation": "盤面の状態を「ノード」、指し手（移動）を「エッジ」で表現します。",
        "Link": "https://example.com/graph-search"
    },
    {
        "ID": "TREND-030",
        "Category": "2.人工知能をめぐる動向",
        "Question": "探索の途中で「同じ状態を二度調べない」ように記録しておくリストを何と呼ぶか？",
        "Opt1": "Openリスト",
        "Opt2": "Closedリスト",
        "Opt3": "Waitリスト",
        "Opt4": "Checkリスト",
        "Opt5": "Blackリスト",
        "Opt6": "Indexリスト",
        "Answer_Idx": 1,
        "Explanation": "Openリストはこれから調べるノード、Closedリストは調査済みのノードです。",
        "Link": "https://example.com/open-closed-list"
    },
    {
        "ID": "TREND-031",
        "Category": "2.人工知能をめぐる動向",
        "Question": "Web上のリソースを「主語・述語・目的語」の3つの要素で記述するデータモデルは？",
        "Opt1": "XML",
        "Opt2": "JSON",
        "Opt3": "RDF",
        "Opt4": "HTML",
        "Opt5": "RSS",
        "Opt6": "SQL",
        "Answer_Idx": 2,
        "Explanation": "RDF（Resource Description Framework）はセマンティックWebの基本となる記述形式です。",
        "Link": "https://www.w3.org/RDF/"
    },
    {
        "ID": "TREND-032",
        "Category": "2.人工知能をめぐる動向",
        "Question": "RDFにおいて記述される「主語・述語・目的語」のセットを何と呼ぶか？",
        "Opt1": "セット",
        "Opt2": "トリオ",
        "Opt3": "トリプル",
        "Opt4": "ユニット",
        "Opt5": "ノード",
        "Opt6": "リンク",
        "Answer_Idx": 2,
        "Explanation": "この3つ組（トリプル）を組み合わせることで、巨大な知識グラフが形成されます。",
        "Link": "https://example.com/rdf-triple"
    },
    {
        "ID": "TREND-033",
        "Category": "2.人工知能をめぐる動向",
        "Question": "Web上のデータを、特定のアプリケーションに依存せず相互にリンクさせる仕組みを何というか？",
        "Opt1": "SaaS",
        "Opt2": "LOD（Linked Open Data）",
        "Opt3": "API",
        "Opt4": "RSS",
        "Opt5": "Cookie",
        "Opt6": "P2P",
        "Answer_Idx": 1,
        "Explanation": "ティム・バーナーズ＝リーが提唱した、データを繋いで活用する概念です。",
        "Link": "https://example.com/lod"
    },
    {
        "ID": "TREND-034",
        "Category": "2.人工知能をめぐる動向",
        "Question": "「文書のメタデータ（作成者や日付など）」を記述するための標準的な語彙体系は？",
        "Opt1": "RDF",
        "Opt2": "ダブリン・コア",
        "Opt3": "JSON-LD",
        "Opt4": "SPARQL",
        "Opt5": "HTML5",
        "Opt6": "Markdown",
        "Answer_Idx": 1,
        "Explanation": "15の基本要素からなる、メタデータの国際標準規格です。",
        "Link": "https://example.com/dublin-core"
    },
    {
        "ID": "TREND-035",
        "Category": "2.人工知能をめぐる動向",
        "Question": "RDF形式で記述されたデータを検索するためのクエリ言語はどれか？",
        "Opt1": "SQL",
        "Opt2": "NoSQL",
        "Opt3": "SPARQL",
        "Opt4": "XPath",
        "Opt5": "GraphQL",
        "Opt6": "XQuery",
        "Answer_Idx": 2,
        "Explanation": "スパークル(SPARQL)は、RDFデータを検索・操作するための標準言語です。",
        "Link": "https://example.com/sparql"
    },
    {
        "ID": "TREND-036",
        "Category": "2.人工知能をめぐる動向",
        "Question": "自律的に行動し、他の要素と協調して目的を達成するソフトウェアプログラムを何というか？",
        "Opt1": "オブジェクト",
        "Opt2": "関数",
        "Opt3": "エージェント",
        "Opt4": "サブルーチン",
        "Opt5": "モジュール",
        "Opt6": "ライブラリ",
        "Answer_Idx": 2,
        "Explanation": "環境を認識し、自ら判断して行動する単位を「エージェント」と呼びます。",
        "Link": "https://example.com/agent"
    },
    {
        "ID": "TREND-037",
        "Category": "2.人工知能をめぐる動向",
        "Question": "複数のエージェントが相互作用し、全体として高度な知能を実現するシステムを何というか？",
        "Opt1": "集中管理システム",
        "Opt2": "マルチエージェントシステム",
        "Opt3": "バッチ処理システム",
        "Opt4": "分散データベース",
        "Opt5": "クラウドシステム",
        "Opt6": "ニューラルネットワーク",
        "Answer_Idx": 1,
        "Explanation": "アリの行列や交通シミュレーションなど、個々の単純な動きから複雑な社会現象を模します。",
        "Link": "https://example.com/multi-agent"
    },
    {
        "ID": "TREND-038",
        "Category": "2.人工知能をめぐる動向",
        "Question": "「知識は身体を通じて得られる」という、身体の存在が知能に不可欠だとする考え方は？",
        "Opt1": "記号主義",
        "Opt2": "身体性（Embodiment）",
        "Opt3": "機能主義",
        "Opt4": "二元論",
        "Opt5": "行動主義",
        "Opt6": "構造主義",
        "Answer_Idx": 1,
        "Explanation": "物理的な体を持つことで初めて「痛い」「重い」といった意味が理解できるという主張です。",
        "Link": "https://example.com/embodiment"
    },
    {
        "ID": "TREND-039",
        "Category": "2.人工知能をめぐる動向",
        "Question": "昆虫のような単純な行動を組み合わせて複雑な振る舞いを実現する「サブサンプション・アーキテクチャ」を提案したのは？",
        "Opt1": "ジョン・サール",
        "Opt2": "ロドニー・ブルックス",
        "Opt3": "レイ・カーツワイル",
        "Opt4": "ジェフリー・ヒントン",
        "Opt5": "松尾豊",
        "Opt6": "マービン・ミンスキー",
        "Answer_Idx": 1,
        "Explanation": "お掃除ロボット「ルンバ」の生みの親の一人としても有名です。",
        "Link": "https://example.com/brooks"
    },
    {
        "ID": "TREND-040",
        "Category": "2.人工知能をめぐる動向",
        "Question": "オントロジーの構築において、人間が理解できる言葉で概念を整理した「軽い」定義を何というか？",
        "Opt1": "ヘビーウェイト・オントロジー",
        "Opt2": "ライトウェイト・オントロジー",
        "Opt3": "ドメイン・オントロジー",
        "Opt4": "タスク・オントロジー",
        "Opt5": "汎用オントロジー",
        "Opt6": "コア・オントロジー",
        "Answer_Idx": 1,
        "Explanation": "厳密な論理定義よりも、情報の共有や検索の効率化を重視したものです。",
        "Link": "https://example.com/lightweight"
    },
    {
        "ID": "TREND-041",
        "Category": "2.人工知能をめぐる動向",
        "Question": "1990年代に主流となった、大量のデータからパターンを見つけ出すアプローチは？",
        "Opt1": "記号的AI",
        "Opt2": "統計的機械学習",
        "Opt3": "エキスパートシステム",
        "Opt4": "フレーム理論",
        "Opt5": "意味ネットワーク",
        "Opt6": "述語論理",
        "Answer_Idx": 1,
        "Explanation": "手書きのルールではなく、確率・統計に基づいた学習が普及しました。",
        "Link": "https://example.com/statistical-ml"
    },
    {
        "ID": "TREND-042",
        "Category": "2.人工知能をめぐる動向",
        "Question": "「将来、機械が全人類の知能を合わせたよりも賢くなる」という予測を広めたカーツワイルの著書は？",
        "Opt1": "人工知能の未来",
        "Opt2": "ポスト・ヒューマン誕生",
        "Opt3": "シンギュラリティは近い",
        "Opt4": "2045年の衝撃",
        "Opt5": "AIの遺言",
        "Opt6": "ホモ・デウス",
        "Answer_Idx": 2,
        "Explanation": "原題は『The Singularity Is Near』です。",
        "Link": "https://example.com/kurzweil-book"
    },
    {
        "ID": "TREND-043",
        "Category": "2.人工知 destinyをめぐる動向",
        "Question": "Webサイトの重要度を、他のサイトからのリンク数に基づいて決定するアルゴリズム（Googleの基礎）は？",
        "Opt1": "エッジランク",
        "Opt2": "ページランク",
        "Opt3": "ハブスコア",
        "Opt4": "TF-IDF",
        "Opt5": "協調フィルタリング",
        "Opt6": "コサイン類似度",
        "Answer_Idx": 1,
        "Explanation": "リンクを「投票」と見なす、初期Googleの画期的なアルゴリズムです。",
        "Link": "https://example.com/pagerank"
    },
    {
        "ID": "TREND-044",
        "Category": "2.人工知能をめぐる動向",
        "Question": "「ある特定の領域において、専門家と同等の意思決定を行う」システムの総称は？",
        "Opt1": "汎用AI",
        "Opt2": "エキスパートシステム",
        "Opt3": "意思決定支援システム",
        "Opt4": "ニューラルネットワーク",
        "Opt5": "遺伝的アルゴリズム",
        "Opt6": "マルチエージェント",
        "Answer_Idx": 1,
        "Explanation": "IF-THENルールの塊で構成される、第2次ブームの主役です。",
        "Link": "https://example.com/expert-system-rep"
    },
    {
        "ID": "TREND-045",
        "Category": "2.人工知能をめぐる動向",
        "Question": "「意味ネットワーク」において、Is-a関係やPart-of関係の他に、特定の個体を示す関係を何というか？",
        "Opt1": "Has-a関係",
        "Opt2": "Instance-of関係",
        "Opt3": "Link-of関係",
        "Opt4": "Member-of関係",
        "Opt5": "Part-of関係",
        "Opt6": "Use-a関係",
        "Answer_Idx": 1,
        "Explanation": "「ソクラテス（個体）は人間（クラス）のインスタンスである」という関係です。",
        "Link": "https://example.com/instance-of"
    },
    {
        "ID": "TREND-046",
        "Category": "2.人工知能をめぐる動向",
        "Question": "1980年代、米国のDARPA主導で行われたAI研究の大型プロジェクト名は？",
        "Opt1": "サイク計面",
        "Opt2": "第五世代コンピュータ",
        "Opt3": "戦略的コンピューティング・イニシアティブ（SCI）",
        "Opt4": "ダートマス計画",
        "Opt5": "マンハッタン計画",
        "Opt6": "アポロ計画",
        "Answer_Idx": 2,
        "Explanation": "日本の第五世代コンピュータに対抗して米国が進めた大規模プロジェクトです。",
        "Link": "https://example.com/sci-project"
    },
    {
        "ID": "TREND-047",
        "Category": "2.人工知能をめぐる動向",
        "Question": "複数の手法を組み合わせて一つの強力なモデルを作る手法（バギングやブースティングなど）を何というか？",
        "Opt1": "シングル学習",
        "Opt2": "アンサンブル学習",
        "Opt3": "転移学習",
        "Opt4": "強化学習",
        "Opt5": "深層学習",
        "Opt6": "マルチタスク学習",
        "Answer_Idx": 1,
        "Explanation": "複数の弱学習器を組み合わせて、精度を向上させる手法です。",
        "Link": "https://example.com/ensemble-learning"
    },
    {
        "ID": "TREND-048",
        "Category": "2.人工知能をめぐる動向",
        "Question": "レコメンドシステムで「この商品を買った人はこんな商品も買っています」という手法は？",
        "Opt1": "内容ベースフィルタリング",
        "Opt2": "協調フィルタリング",
        "Opt3": "ルールベース抽出",
        "Opt4": "クラスター分析",
        "Opt5": "主成分分析",
        "Opt6": "回帰分析",
        "Answer_Idx": 1,
        "Explanation": "ユーザー同士の行動の類似性に基づいて推薦を行う手法です。",
        "Link": "https://example.com/collab-filtering"
    },
    {
        "ID": "TREND-049",
        "Category": "2.人工知能をめぐる動向",
        "Question": "「データそれ自体に構造や意味を持たせる」ことでWebを巨大なデータベースにする構想は？",
        "Opt1": "Web 1.0",
        "Opt2": "Web 2.0",
        "Opt3": "セマンティックWeb",
        "Opt4": "メタバース",
        "Opt5": "IoT",
        "Opt6": "エッジコンピューティング",
        "Answer_Idx": 2,
        "Explanation": "コンピュータがWeb上の情報を直接「理解」できるようにする試みです。",
        "Link": "https://example.com/semantic-vision"
    },
    {
        "ID": "TREND-050",
        "Category": "2.人工知能をめぐる動向",
        "Question": "情報検索において、単語の出現頻度（TF）と逆文書頻度（IDF）を掛け合わせた指標は？",
        "Opt1": "TF-IDF",
        "Opt2": "BM25",
        "Opt3": "PageRank",
        "Opt4": "Word2Vec",
        "Opt5": "BERT",
        "Opt6": "Attention",
        "Answer_Idx": 0,
        "Explanation": "文書内での単語の重要度を評価するための基本的な指標です。",
        "Link": "https://example.com/tf-idf"
    },
    {
        "ID": "TREND-051",
        "Category": "2.人工知能をめぐる動向",
        "Question": "2012年のILSVRCでAlexNetが示した、従来の画像認識手法に対する最大の違いは？",
        "Opt1": "特徴量を人間が設計した",
        "Opt2": "特徴量をAIが自ら学習した",
        "Opt3": "ルールの数を増やした",
        "Opt4": "計算速度を遅くした",
        "Opt5": "白黒画像のみに対応した",
        "Opt6": "インターネットを使わなかった",
        "Answer_Idx": 1,
        "Explanation": "ディープラーニングは「特徴量設計」を自動化したことが最大のブレイクスルーです。",
        "Link": "https://example.com/feature-learning"
    },
    {
        "ID": "TREND-052",
        "Category": "2.人工知能をめぐる動向",
        "Question": "2017年にGoogleが発表した論文「Attention Is All You Need」で提案されたモデルは？",
        "Opt1": "RNN",
        "Opt2": "CNN",
        "Opt3": "LSTM",
        "Opt4": "Transformer",
        "Opt5": "GAN",
        "Opt6": "BERT",
        "Answer_Idx": 3,
        "Explanation": "Transformerは、現在のLLM（大規模言語モデル）のすべての基礎となっている技術です。",
        "Link": "https://example.com/transformer-paper"
    },
    {
        "ID": "TREND-053",
        "Category": "2.人工知能をめぐる動向",
        "Question": "Transformerにおいて、入力データの各単語間の関係性を並列に計算する仕組みを何というか？",
        "Opt1": "畳み込み",
        "Opt2": "プーリング",
        "Opt3": "Self-Attention",
        "Opt4": "再帰処理",
        "Opt5": "ドロップアウト",
        "Opt6": "バッチ正規化",
        "Answer_Idx": 2,
        "Explanation": "Self-Attentionにより、文脈の長距離依存関係を効率的に学習できるようになりました。",
        "Link": "https://example.com/self-attention"
    },
    {
        "ID": "TREND-054",
        "Category": "2.人工知能をめぐる動向",
        "Question": "2018年にGoogleが発表した、文脈を双方向に理解することで自然言語処理の精度を劇的に高めたモデルは？",
        "Opt1": "GPT",
        "Opt2": "ELMo",
        "Opt3": "BERT",
        "Opt4": "Word2Vec",
        "Opt5": "ResNet",
        "Opt6": "T5",
        "Answer_Idx": 2,
        "Explanation": "BERT（Bidirectional Encoder Representations from Transformers）は双方向の学習が特徴です。",
        "Link": "https://example.com/bert-model"
    },
    {
        "ID": "TREND-055",
        "Category": "2.人工知能をめぐる動向",
        "Question": "OpenAIが開発したGPTシリーズにおいて、モデルの規模を示す指標として最も一般的なものは？",
        "Opt1": "層の深さ",
        "Opt2": "データの容量",
        "Opt3": "パラメータ数",
        "Opt4": "学習時間",
        "Opt5": "ユーザー数",
        "Opt6": "GPUの台数",
        "Answer_Idx": 2,
        "Explanation": "パラメータ数が増えるほど、より複雑な知識や推論能力を獲得する傾向があります。",
        "Link": "https://example.com/gpt-params"
    },
    {
        "ID": "TREND-056",
        "Category": "2.人工知能をめぐる動向",
        "Question": "特定のタスク用に訓練されていないAIが、わずかな例示（プロンプト）だけで回答する学習形態を何というか？",
        "Opt1": "ゼロショット学習",
        "Opt2": "フューショット学習",
        "Opt3": "転移学習",
        "Opt4": "ファインチューニング",
        "Opt5": "蒸留",
        "Opt6": "強化学習",
        "Answer_Idx": 1,
        "Explanation": "Few-shot学習は、数個の例を与えるだけで未知のタスクに対応させる手法です。",
        "Link": "https://example.com/few-shot"
    },
    {
        "ID": "TREND-057",
        "Category": "2.人工知能をめぐる動向",
        "Question": "大規模言語モデルにおいて、もっともらしいが事実とは異なる情報を生成してしまう現象を何というか？",
        "Opt1": "オーバーフィッティング",
        "Opt2": "勾配消失",
        "Opt3": "ハルシネーション（幻覚）",
        "Opt4": "カタストロフィック忘却",
        "Opt5": "バイアス",
        "Opt6": "モード崩壊",
        "Answer_Idx": 2,
        "Explanation": "もっともらしい嘘をつく「ハルシネーション」は、LLMの大きな課題の一つです。",
        "Link": "https://example.com/hallucination"
    },
    {
        "ID": "TREND-058",
        "Category": "2.人工知能をめぐる動向",
        "Question": "画像生成AI「Stable Diffusion」などで用いられている、ノイズを除去する過程で画像を生成する手法は？",
        "Opt1": "GAN（敵対的生成ネットワーク）",
        "Opt2": "VAE（変分自己符号化器）",
        "Opt3": "拡散モデル（Diffusion Model）",
        "Opt4": "RNN",
        "Opt5": "CNN",
        "Opt6": "決定木",
        "Answer_Idx": 2,
        "Explanation": "逆拡散過程を用いて、ノイズから高品質な画像を復元・生成する手法です。",
        "Link": "https://example.com/diffusion-model"
    },
    {
        "ID": "TREND-059",
        "Category": "2.人工知能をめぐる動向",
        "Question": "AIが生成した回答に対し、人間がフィードバックを与えて好ましい出力に調整する手法を何というか？",
        "Opt1": "SGD",
        "Opt2": "Adam",
        "Opt3": "RLHF",
        "Opt4": "dropout",
        "Opt5": "Normalization",
        "Opt6": "Augmentation",
        "Answer_Idx": 2,
        "Explanation": "RLHF（人間からのフィードバックによる強化学習）は、ChatGPTの調整に不可欠な技術です。",
        "Link": "https://example.com/rlhf"
    },
    {
        "ID": "TREND-060",
        "Category": "2.人工知能をめぐる動向",
        "Question": "「AIの知能が急速に進化し、人間に制御できなくなる時点」を2045年と予測した議論の名称は？",
        "Opt1": "AIの冬",
        "Opt2": "シンギュラリティ（技術的特異点）",
        "Opt3": "デジタルトランスフォーメーション",
        "Opt4": "第四次産業革命",
        "Opt5": "ムーアの法則",
        "Opt6": "アムダールの法則",
        "Answer_Idx": 1,
        "Explanation": "レイ・カーツワイルが提唱しました。",
        "Link": "https://example.com/singularity-check"
    },
    {
        "ID": "TREND-061",
        "Category": "2.人工知能をめぐる動向",
        "Question": "文章、画像、音声など、異なる種類のデータを統合して扱うAIの性質を何と呼ぶか？",
        "Opt1": "マルチタスク",
        "Opt2": "マルチモーダル",
        "Opt3": "マルチスレッド",
        "Opt4": "マルチコア",
        "Opt5": "マルチチャンネル",
        "Opt6": "マルチメディア",
        "Answer_Idx": 1,
        "Explanation": "異なる「モダリティ（形式）」を横断して処理するAIのことです。",
        "Link": "https://example.com/multimodal"
    },
    {
        "ID": "TREND-062",
        "Category": "2.人工知能をめぐる動向",
        "Question": "AI開発において、計算リソースを大量に投入することで性能が向上するという経験則を何というか？",
        "Opt1": "ムーアの法則",
        "Opt2": "スケーリング則（べき乗則）",
        "Opt3": "グリコ曲線",
        "Opt4": "パレートの法則",
        "Opt5": "ベルの法則",
        "Opt6": "ジップの法則",
        "Answer_Idx": 1,
        "Explanation": "データ量、計算量、パラメータ数を増やすほど精度が向上するという法則です。",
        "Link": "https://example.com/scaling-law"
    },
    {
        "ID": "TREND-063",
        "Category": "2.人工知能をめぐる動向",
        "Question": "生成AIに適切な指示（プロンプト）を与えることで、望む出力を得るための技術を何というか？",
        "Opt1": "データマイニング",
        "Opt2": "プロンプトエンジニアリング",
        "Opt3": "バックエンド開発",
        "Opt4": "フロントエンド開発",
        "Opt5": "クエリ最適化",
        "Opt6": "インデックス作成",
        "Answer_Idx": 1,
        "Explanation": "モデルの性能を引き出すための「問いかけ」の技術です。",
        "Link": "https://example.com/prompt-eng"
    },
    {
        "ID": "TREND-064",
        "Category": "2.人工知能をめぐる動向",
        "Question": "2024年に日本政府が公開した、AIの適切な利用と開発に関する指針の名称は？",
        "Opt1": "AI開発宣言",
        "Opt2": "AI事業者ガイドライン",
        "Opt3": "人工知能基本法",
        "Opt4": "デジタルAI憲章",
        "Opt5": "AI安全保障法",
        "Opt6": "スマートAI指針",
        "Answer_Idx": 1,
        "Explanation": "総務省・経済産業省が「AI利活用ガイドライン」等を統合して策定しました。",
        "Link": "https://www.soumu.go.jp/menu_news/s-news/01houdou00000001_00021.html"
    },
    {
        "ID": "TREND-065",
        "Category": "2.人工知能をめぐる動向",
        "Question": "欧州連合（EU）で採択された、AIをリスクレベルに応じて規制する世界初の包括的な法案は？",
        "Opt1": "GDPR",
        "Opt2": "DMA",
        "Opt3": "EU AI法（AI Act）",
        "Opt4": "DSA",
        "Opt5": "CCPA",
        "Opt6": "APECプライバシー指針",
        "Answer_Idx": 2,
        "Explanation": "リスクに応じて「禁止」「高リスク」「限定的」等に分類して規制します。",
        "Link": "https://example.com/eu-ai-act"
    },
    {
        "ID": "TREND-066",
        "Category": "2.人工知能をめぐる動向",
        "Question": "AIが自分の学習データとして自分の生成物を利用し続け、精度が低下していく現象を何というか？",
        "Opt1": "モデル崩壊（Model Collapse）",
        "Opt2": "過学習",
        "Opt3": "勾配爆発",
        "Opt4": "リーク",
        "Opt5": "バイアス増幅",
        "Opt6": "忘却現象",
        "Answer_Idx": 0,
        "Explanation": "生成AIのデータで生成AIを訓練し続けることで起きる問題として注目されています。",
        "Link": "https://example.com/model-collapse"
    },
    {
        "ID": "TREND-067",
        "Category": "2.人工知能をめぐる動向",
        "Question": "2014年にイアン・グッドフェローが考案した、生成器と識別器を競わせる生成モデルは？",
        "Opt1": "VAE",
        "Opt2": "GAN",
        "Opt3": "CNN",
        "Opt4": "RNN",
        "Opt5": "Transformer",
        "Opt6": "Diffusion",
        "Answer_Idx": 1,
        "Explanation": "GAN（敵対的生成ネットワーク）は「偽造者」と「警察」の例えで有名です。",
        "Link": "https://example.com/gan-intro"
    },
    {
        "ID": "TREND-068",
        "Category": "2.人工知能をめぐる動向",
        "Question": "ディープラーニング以前の画像認識で主流だった、画像から局所的な特徴を抽出する手法は？",
        "Opt1": "SIFT",
        "Opt2": "DeepBlue",
        "Opt3": "Word2Vec",
        "Opt4": "Attention",
        "Opt5": "PageRank",
        "Opt6": "AlphaZero",
        "Answer_Idx": 0,
        "Explanation": "SIFTやSURFなど、人間が設計した特徴量（ハンドクラフト特徴量）が使われていました。",
        "Link": "https://example.com/sift"
    },
    {
        "ID": "TREND-069",
        "Category": "2.人工知能をめぐる動向",
        "Question": "Googleが開発した、囲碁だけでなく将棋やチェスでも世界一になったAIの名称は？",
        "Opt1": "AlphaGo",
        "Opt2": "AlphaGo Zero",
        "Opt3": "AlphaZero",
        "Opt4": "AlphaChess",
        "Opt5": "AlphaMaster",
        "Opt6": "AlphaShogi",
        "Answer_Idx": 2,
        "Explanation": "事前知識なしの「自己対局」のみで学習し、複数のゲームに対応した進化版です。",
        "Link": "https://example.com/alphazero"
    },
    {
        "ID": "TREND-070",
        "Category": "2.人工知能をめぐる動向",
        "Question": "AI開発において、人間が理解できる形で判断の根拠を示す技術の総称は？",
        "Opt1": "XAI（説明可能なAI）",
        "Opt2": "AGI（汎用AI）",
        "Opt3": "NLP（自然言語処理）",
        "Opt4": "CV（コンピュータビジョン）",
        "Opt5": "RL（強化学習）",
        "Opt6": "ML（機械学習）",
        "Answer_Idx": 0,
        "Explanation": "Explainable AI（説明可能なAI）は、ブラックボックス化を避けるための重要な技術です。",
        "Link": "https://example.com/xai"
    },
    {
        "ID": "TREND-051",
        "Category": "2.人工知能をめぐる動向",
        "Question": "2012年のILSVRCでAlexNetが示した、従来の画像認識手法に対する最大の違いは？",
        "Opt1": "特徴量を人間が設計した",
        "Opt2": "特徴量をAIが自ら学習した",
        "Opt3": "ルールの数を増やした",
        "Opt4": "計算速度を遅くした",
        "Opt5": "白黒画像のみに対応した",
        "Opt6": "インターネットを使わなかった",
        "Answer_Idx": 1,
        "Explanation": "ディープラーニングは「特徴量設計」を自動化したことが最大のブレイクスルーです。",
        "Link": "https://example.com/feature-learning"
    },
    {
        "ID": "TREND-052",
        "Category": "2.人工知能をめぐる動向",
        "Question": "2017年にGoogleが発表した論文「Attention Is All You Need」で提案されたモデルは？",
        "Opt1": "RNN",
        "Opt2": "CNN",
        "Opt3": "LSTM",
        "Opt4": "Transformer",
        "Opt5": "GAN",
        "Opt6": "BERT",
        "Answer_Idx": 3,
        "Explanation": "Transformerは、現在のLLM（大規模言語モデル）のすべての基礎となっている技術です。",
        "Link": "https://example.com/transformer-paper"
    },
    {
        "ID": "TREND-053",
        "Category": "2.人工知能をめぐる動向",
        "Question": "Transformerにおいて、入力データの各単語間の関係性を並列に計算する仕組みを何というか？",
        "Opt1": "畳み込み",
        "Opt2": "プーリング",
        "Opt3": "Self-Attention",
        "Opt4": "再帰処理",
        "Opt5": "ドロップアウト",
        "Opt6": "バッチ正規化",
        "Answer_Idx": 2,
        "Explanation": "Self-Attentionにより、文脈の長距離依存関係を効率的に学習できるようになりました。",
        "Link": "https://example.com/self-attention"
    },
    {
        "ID": "TREND-054",
        "Category": "2.人工知能をめぐる動向",
        "Question": "2018年にGoogleが発表した、文脈を双方向に理解することで自然言語処理の精度を劇的に高めたモデルは？",
        "Opt1": "GPT",
        "Opt2": "ELMo",
        "Opt3": "BERT",
        "Opt4": "Word2Vec",
        "Opt5": "ResNet",
        "Opt6": "T5",
        "Answer_Idx": 2,
        "Explanation": "BERT（Bidirectional Encoder Representations from Transformers）は双方向の学習が特徴です。",
        "Link": "https://example.com/bert-model"
    },
    {
        "ID": "TREND-055",
        "Category": "2.人工知能をめぐる動向",
        "Question": "OpenAIが開発したGPTシリーズにおいて、モデルの規模を示す指標として最も一般的なものは？",
        "Opt1": "層の深さ",
        "Opt2": "データの容量",
        "Opt3": "パラメータ数",
        "Opt4": "学習時間",
        "Opt5": "ユーザー数",
        "Opt6": "GPUの台数",
        "Answer_Idx": 2,
        "Explanation": "パラメータ数が増えるほど、より複雑な知識や推論能力を獲得する傾向があります。",
        "Link": "https://example.com/gpt-params"
    },
    {
        "ID": "TREND-056",
        "Category": "2.人工知能をめぐる動向",
        "Question": "特定のタスク用に訓練されていないAIが、わずかな例示（プロンプト）だけで回答する学習形態を何というか？",
        "Opt1": "ゼロショット学習",
        "Opt2": "フューショット学習",
        "Opt3": "転移学習",
        "Opt4": "ファインチューニング",
        "Opt5": "蒸留",
        "Opt6": "強化学習",
        "Answer_Idx": 1,
        "Explanation": "Few-shot学習は、数個の例を与えるだけで未知のタスクに対応させる手法です。",
        "Link": "https://example.com/few-shot"
    },
    {
        "ID": "TREND-057",
        "Category": "2.人工知能をめぐる動向",
        "Question": "大規模言語モデルにおいて、もっともらしいが事実とは異なる情報を生成してしまう現象を何というか？",
        "Opt1": "オーバーフィッティング",
        "Opt2": "勾配消失",
        "Opt3": "ハルシネーション（幻覚）",
        "Opt4": "カタストロフィック忘却",
        "Opt5": "バイアス",
        "Opt6": "モード崩壊",
        "Answer_Idx": 2,
        "Explanation": "もっともらしい嘘をつく「ハルシネーション」は、LLMの大きな課題の一つです。",
        "Link": "https://example.com/hallucination"
    },
    {
        "ID": "TREND-058",
        "Category": "2.人工知能をめぐる動向",
        "Question": "画像生成AI「Stable Diffusion」などで用いられている、ノイズを除去する過程で画像を生成する手法は？",
        "Opt1": "GAN（敵対的生成ネットワーク）",
        "Opt2": "VAE（変分自己符号化器）",
        "Opt3": "拡散モデル（Diffusion Model）",
        "Opt4": "RNN",
        "Opt5": "CNN",
        "Opt6": "決定木",
        "Answer_Idx": 2,
        "Explanation": "逆拡散過程を用いて、ノイズから高品質な画像を復元・生成する手法です。",
        "Link": "https://example.com/diffusion-model"
    },
    {
        "ID": "TREND-059",
        "Category": "2.人工知能をめぐる動向",
        "Question": "AIが生成した回答に対し、人間がフィードバックを与えて好ましい出力に調整する手法を何というか？",
        "Opt1": "SGD",
        "Opt2": "Adam",
        "Opt3": "RLHF",
        "Opt4": "dropout",
        "Opt5": "Normalization",
        "Opt6": "Augmentation",
        "Answer_Idx": 2,
        "Explanation": "RLHF（人間からのフィードバックによる強化学習）は、ChatGPTの調整に不可欠な技術です。",
        "Link": "https://example.com/rlhf"
    },
    {
        "ID": "TREND-060",
        "Category": "2.人工知能をめぐる動向",
        "Question": "「AIの知能が急速に進化し、人間に制御できなくなる時点」を2045年と予測した議論の名称は？",
        "Opt1": "AIの冬",
        "Opt2": "シンギュラリティ（技術的特異点）",
        "Opt3": "デジタルトランスフォーメーション",
        "Opt4": "第四次産業革命",
        "Opt5": "ムーアの法則",
        "Opt6": "アムダールの法則",
        "Answer_Idx": 1,
        "Explanation": "レイ・カーツワイルが提唱しました。",
        "Link": "https://example.com/singularity-check"
    },
    {
        "ID": "TREND-061",
        "Category": "2.人工知能をめぐる動向",
        "Question": "文章、画像、音声など、異なる種類のデータを統合して扱うAIの性質を何と呼ぶか？",
        "Opt1": "マルチタスク",
        "Opt2": "マルチモーダル",
        "Opt3": "マルチスレッド",
        "Opt4": "マルチコア",
        "Opt5": "マルチチャンネル",
        "Opt6": "マルチメディア",
        "Answer_Idx": 1,
        "Explanation": "異なる「モダリティ（形式）」を横断して処理するAIのことです。",
        "Link": "https://example.com/multimodal"
    },
    {
        "ID": "TREND-062",
        "Category": "2.人工知能をめぐる動向",
        "Question": "AI開発において、計算リソースを大量に投入することで性能が向上するという経験則を何というか？",
        "Opt1": "ムーアの法則",
        "Opt2": "スケーリング則（べき乗則）",
        "Opt3": "グリコ曲線",
        "Opt4": "パレートの法則",
        "Opt5": "ベルの法則",
        "Opt6": "ジップの法則",
        "Answer_Idx": 1,
        "Explanation": "データ量、計算量、パラメータ数を増やすほど精度が向上するという法則です。",
        "Link": "https://example.com/scaling-law"
    },
    {
        "ID": "TREND-063",
        "Category": "2.人工知能をめぐる動向",
        "Question": "生成AIに適切な指示（プロンプト）を与えることで、望む出力を得るための技術を何というか？",
        "Opt1": "データマイニング",
        "Opt2": "プロンプトエンジニアリング",
        "Opt3": "バックエンド開発",
        "Opt4": "フロントエンド開発",
        "Opt5": "クエリ最適化",
        "Opt6": "インデックス作成",
        "Answer_Idx": 1,
        "Explanation": "モデルの性能を引き出すための「問いかけ」の技術です。",
        "Link": "https://example.com/prompt-eng"
    },
    {
        "ID": "TREND-064",
        "Category": "2.人工知能をめぐる動向",
        "Question": "2024年に日本政府が公開した、AIの適切な利用と開発に関する指針の名称は？",
        "Opt1": "AI開発宣言",
        "Opt2": "AI事業者ガイドライン",
        "Opt3": "人工知能基本法",
        "Opt4": "デジタルAI憲章",
        "Opt5": "AI安全保障法",
        "Opt6": "スマートAI指針",
        "Answer_Idx": 1,
        "Explanation": "総務省・経済産業省が「AI利活用ガイドライン」等を統合して策定しました。",
        "Link": "https://www.soumu.go.jp/menu_news/s-news/01houdou00000001_00021.html"
    },
    {
        "ID": "TREND-065",
        "Category": "2.人工知能をめぐる動向",
        "Question": "欧州連合（EU）で採択された、AIをリスクレベルに応じて規制する世界初の包括的な法案は？",
        "Opt1": "GDPR",
        "Opt2": "DMA",
        "Opt3": "EU AI法（AI Act）",
        "Opt4": "DSA",
        "Opt5": "CCPA",
        "Opt6": "APECプライバシー指針",
        "Answer_Idx": 2,
        "Explanation": "リスクに応じて「禁止」「高リスク」「限定的」等に分類して規制します。",
        "Link": "https://example.com/eu-ai-act"
    },
    {
        "ID": "TREND-066",
        "Category": "2.人工知能をめぐる動向",
        "Question": "AIが自分の学習データとして自分の生成物を利用し続け、精度が低下していく現象を何というか？",
        "Opt1": "モデル崩壊（Model Collapse）",
        "Opt2": "過学習",
        "Opt3": "勾配爆発",
        "Opt4": "リーク",
        "Opt5": "バイアス増幅",
        "Opt6": "忘却現象",
        "Answer_Idx": 0,
        "Explanation": "生成AIのデータで生成AIを訓練し続けることで起きる問題として注目されています。",
        "Link": "https://example.com/model-collapse"
    },
    {
        "ID": "TREND-067",
        "Category": "2.人工知能をめぐる動向",
        "Question": "2014年にイアン・グッドフェローが考案した、生成器と識別器を競わせる生成モデルは？",
        "Opt1": "VAE",
        "Opt2": "GAN",
        "Opt3": "CNN",
        "Opt4": "RNN",
        "Opt5": "Transformer",
        "Opt6": "Diffusion",
        "Answer_Idx": 1,
        "Explanation": "GAN（敵対的生成ネットワーク）は「偽造者」と「警察」の例えで有名です。",
        "Link": "https://example.com/gan-intro"
    },
    {
        "ID": "TREND-068",
        "Category": "2.人工知能をめぐる動向",
        "Question": "ディープラーニング以前の画像認識で主流だった、画像から局所的な特徴を抽出する手法は？",
        "Opt1": "SIFT",
        "Opt2": "DeepBlue",
        "Opt3": "Word2Vec",
        "Opt4": "Attention",
        "Opt5": "PageRank",
        "Opt6": "AlphaZero",
        "Answer_Idx": 0,
        "Explanation": "SIFTやSURFなど、人間が設計した特徴量（ハンドクラフト特徴量）が使われていました。",
        "Link": "https://example.com/sift"
    },
    {
        "ID": "TREND-069",
        "Category": "2.人工知能をめぐる動向",
        "Question": "Googleが開発した、囲碁だけでなく将棋やチェスでも世界一になったAIの名称は？",
        "Opt1": "AlphaGo",
        "Opt2": "AlphaGo Zero",
        "Opt3": "AlphaZero",
        "Opt4": "AlphaChess",
        "Opt5": "AlphaMaster",
        "Opt6": "AlphaShogi",
        "Answer_Idx": 2,
        "Explanation": "事前知識なしの「自己対局」のみで学習し、複数のゲームに対応した進化版です。",
        "Link": "https://example.com/alphazero"
    },
    {
        "ID": "TREND-070",
        "Category": "2.人工知能をめぐる動向",
        "Question": "AI開発において、人間が理解できる形で判断の根拠を示す技術の総称は？",
        "Opt1": "XAI（説明可能なAI）",
        "Opt2": "AGI（汎用AI）",
        "Opt3": "NLP（自然言語処理）",
        "Opt4": "CV（コンピュータビジョン）",
        "Opt5": "RL（強化学習）",
        "Opt6": "ML（機械学習）",
        "Answer_Idx": 0,
        "Explanation": "Explainable AI（説明可能なAI）は、ブラックボックス化を避けるための重要な技術です。",
        "Link": "https://example.com/xai"
    },
    {
        "ID": "ML-001",
        "Category": "3.機械学習の手法",
        "Question": "機械学習においてデータセットを「学習用」と「テスト用」に分割して評価する最も基本的な手法は？",
        "Opt1": "ホールドアウト法",
        "Opt2": "クロスバリデーション",
        "Opt3": "リーブワンアウト法",
        "Opt4": "ブートストラップ法",
        "Opt5": "アンサンブル学習",
        "Opt6": "正規化",
        "Answer_Idx": 0,
        "Explanation": "データを2つに分ける単純な手法をホールドアウト法と呼びます。データが少ないと偏りが出る欠点があります。",
        "Link": "https://example.com/hold-out"
    },
    {
        "ID": "ML-002",
        "Category": "3.機械学習の手法",
        "Question": "データをK個に分割し学習と評価を交互に繰り返して平均をとる手法を何というか？",
        "Opt1": "ホールドアウト法",
        "Opt2": "K-means法",
        "Opt3": "K-分割交差検証（クロスバリデーション）",
        "Opt4": "主成分分析",
        "Opt5": "正則化",
        "Opt6": "標準化",
        "Answer_Idx": 2,
        "Explanation": "クロスバリデーションは、データの偏りを防ぎ、モデルの汎化性能をより正確に評価できます。",
        "Link": "https://example.com/cross-validation"
    },
    {
        "ID": "ML-003",
        "Category": "3.機械学習の手法",
        "Question": "モデルが学習データに過剰に適合しすぎて未知のデータで精度が下がる現象を何というか？",
        "Opt1": "未学習",
        "Opt2": "過学習（オーバーフィッティング）",
        "Opt3": "勾配消失",
        "Opt4": "次元の呪い",
        "Opt5": "ハルシネーション",
        "Opt6": "モード崩壊",
        "Answer_Idx": 1,
        "Explanation": "学習データに対してのみ「丸暗記」したような状態で、汎用性が失われた状態です。",
        "Link": "https://example.com/overfitting"
    },
    {
        "ID": "ML-004",
        "Category": "3.機械学習の手法",
        "Question": "入力変数（説明変数）から連続値（数値）を予測する機械学習のタスクを何というか？",
        "Opt1": "分類",
        "Opt2": "回帰",
        "Opt3": "クラスタリング",
        "Opt4": "次元圧縮",
        "Opt5": "異常検知",
        "Opt6": "強化学習",
        "Answer_Idx": 1,
        "Explanation": "株価予測や気温予測など、数値を当てるものは「回帰」に分類されます。",
        "Link": "https://example.com/regression"
    },
    {
        "ID": "ML-005",
        "Category": "3.機械学習の手法",
        "Question": "回帰分析において、予測値と実測値の差の2乗の平均をとった評価指標はどれか？",
        "Opt1": "MAE（平均絶対誤差）",
        "Opt2": "MSE（平均二乗誤差）",
        "Opt3": "RMSE（平方根平均二乗誤差）",
        "Opt4": "決定係数（R2）",
        "Opt5": "正解率",
        "Opt6": "適合率",
        "Answer_Idx": 1,
        "Explanation": "Mean Squared Error（MSE）です。2乗するため、誤差が大きいほどペナルティが強く反映されます。",
        "Link": "https://example.com/mse"
    },
    {
        "ID": "ML-006",
        "Category": "3.機械学習の手法",
        "Question": "単回帰分析において、予測モデルの当てはまりの良さ（0〜1）を示す指標を何というか？",
        "Opt1": "相関係数",
        "Opt2": "決定係数（R2）",
        "Opt3": "標準偏差",
        "Opt4": "分散",
        "Opt5": "F値",
        "Opt6": "AUC",
        "Answer_Idx": 1,
        "Explanation": "決定係数は、モデルがどれくらいデータを説明できているかを示す指標です。",
        "Link": "https://example.com/r-squared"
    },
    {
        "ID": "ML-007",
        "Category": "3.機械学習の手法",
        "Question": "過学習を防ぐために、損失関数にパラメータの大きさ（重み）を加える手法を何というか？",
        "Opt1": "標準化",
        "Opt2": "正規化",
        "Opt3": "正則化",
        "Opt4": "平滑化",
        "Opt5": "量子化",
        "Opt6": "白色化",
        "Answer_Idx": 2,
        "Explanation": "L1正則化（Lasso）やL2正則化（Ridge）などがあり、モデルの複雑さを抑えます。",
        "Link": "https://example.com/regularization"
    },
    {
        "ID": "ML-008",
        "Category": "3.機械学習の手法",
        "Question": "L1正則化（Lasso回帰）の特徴として、適切でないものはどれか？",
        "Opt1": "重みの絶対値の和を加える",
        "Opt2": "一部の重みを完全に0にする能力がある",
        "Opt3": "特徴量選択の効果がある",
        "Opt4": "重みの2乗の和を加える",
        "Opt5": "過学習を抑制する",
        "Opt6": "スパースなモデルを作成できる",
        "Answer_Idx": 3,
        "Explanation": "「重みの2乗の和」を加えるのはL2正則化（Ridge回帰）の特徴です。",
        "Link": "https://example.com/lasso-ridge"
    },
    {
        "ID": "ML-009",
        "Category": "3.機械学習の手法",
        "Question": "二値分類の評価指標において「正と予測したもののうち実際に正だった割合」はどれか？",
        "Opt1": "正解率（Accuracy）",
        "Opt2": "適合率（Precision）",
        "Opt3": "再現率（Recall）",
        "Opt4": "F値",
        "Opt5": "特異度",
        "Opt6": "特異性",
        "Answer_Idx": 1,
        "Explanation": "Precision（精度/適合率）です。誤検知（偽陽性）を少なくしたい場合に重視します。",
        "Link": "https://example.com/precision"
    },
    {
        "ID": "ML-010",
        "Category": "3.機械学習の手法",
        "Question": "二値分類の評価指標において「実際に正であるもののうち正と予測できた割合」はどれか？",
        "Opt1": "正解率",
        "Opt2": "適合率",
        "Opt3": "再現率（Recall）",
        "Opt4": "F値",
        "Opt5": "決定係数",
        "Opt6": "AUC",
        "Answer_Idx": 2,
        "Explanation": "Recall（再現率/感度）です。見逃し（偽陰性）を少なくしたい場合に重視します。",
        "Link": "https://example.com/recall"
    },
    {
        "ID": "ML-011",
        "Category": "3.機械学習の手法",
        "Question": "適合率と再現率のバランスをとるために用いられる「F値」の計算方法はどれか？",
        "Opt1": "算術平均",
        "Opt2": "相加平均",
        "Opt3": "調和平均",
        "Opt4": "移動平均",
        "Opt5": "重み付き平均",
        "Opt6": "幾何平均",
        "Answer_Idx": 2,
        "Explanation": "【ひっかけ頻出】F値は適合率と再現率の「調和平均」です。算術平均ではありません。",
        "Link": "https://example.com/f-score"
    },
    {
        "ID": "ML-012",
        "Category": "3.機械学習の手法",
        "Question": "分類問題において、予測値を変化させた時の「真陽性率」と「偽陽性率」の軌跡を描いた曲線は？",
        "Opt1": "シグモイド曲線",
        "Opt2": "学習曲線",
        "Opt3": "ROC曲線",
        "Opt4": "ベル曲線",
        "Opt5": "損失曲線",
        "Opt6": "回帰直線",
        "Answer_Idx": 2,
        "Explanation": "Receiver Operating Characteristic曲線の略です。この下の面積がAUCとなります。",
        "Link": "https://example.com/roc-curve"
    },
    {
        "ID": "ML-013",
        "Category": "3.機械学習の手法",
        "Question": "ROC曲線の下側の面積を示し、1に近いほど分類性能が高いことを示す指標は？",
        "Opt1": "RMSE",
        "Opt2": "F値",
        "Opt3": "AUC",
        "Opt4": "R2",
        "Opt5": "MAPE",
        "Opt6": "IoU",
        "Answer_Idx": 2,
        "Explanation": "Area Under the Curveの略です。ランダムな予測の場合は0.5になります。",
        "Link": "https://example.com/auc"
    },
    {
        "ID": "ML-014",
        "Category": "3.機械学習の手法",
        "Question": "教師あり学習において、データに付与されている「正解」のことを何と呼ぶか？",
        "Opt1": "特徴量",
        "Opt2": "ラベル",
        "Opt3": "カーネル",
        "Opt4": "ハイパーパラメータ",
        "Opt5": "バイアス",
        "Opt6": "エポック",
        "Answer_Idx": 1,
        "Explanation": "学習の目標となる正解データ（教師信号）をラベルと呼びます。",
        "Link": "https://example.com/labeling"
    },
    {
        "ID": "ML-015",
        "Category": "3.機械学習の手法",
        "Question": "機械学習のアルゴリズム自体が学習するのではなく、人間が事前に設定すべき変数を何というか？",
        "Opt1": "パラメータ",
        "Opt2": "重み",
        "Opt3": "ハイパーパラメータ",
        "Opt4": "バイアス",
        "Opt5": "損失",
        "Opt6": "メタデータ",
        "Answer_Idx": 2,
        "Explanation": "学習率や正則化の強さ、決定木の深さなどがこれに該当します。",
        "Link": "https://example.com/hyper-parameter"
    },
    {
        "ID": "ML-016",
        "Category": "3.機械学習の手法",
        "Question": "「実際は負だが、モデルが正と予測してしまった」ケースを混同行列で何と呼ぶか？",
        "Opt1": "真陽性（TP）",
        "Opt2": "偽陽性（FP）",
        "Opt3": "真陰性（TN）",
        "Opt4": "偽陰性（FN）",
        "Opt5": "正解率",
        "Opt6": "再現率",
        "Answer_Idx": 1,
        "Explanation": "False Positive（偽陽性）です。「オオカミ少年」が「狼が来た（正）」と嘘をついた状態です。",
        "Link": "https://example.com/fp-fn"
    },
    {
        "ID": "ML-017",
        "Category": "3.機械学習の手法",
        "Question": "癌の検診のように「病気の人を見逃すこと（偽陰性）」を最も避けたい場合、重視すべき指標は？",
        "Opt1": "正解率",
        "Opt2": "適合率",
        "Opt3": "再現率",
        "Opt4": "F値",
        "Opt5": "決定係数",
        "Opt6": "標準偏差",
        "Answer_Idx": 2,
        "Explanation": "見逃しを減らすためには、再現率（Recall）を最大化する必要があります。",
        "Link": "https://example.com/medical-recall"
    },
    {
        "ID": "ML-018",
        "Category": "3.機械学習の手法",
        "Question": "スパムメール判定のように「普通のメールをスパムと誤検知すること」を避けたい場合に重視すべきは？",
        "Opt1": "正解率",
        "Opt2": "適合率",
        "Opt3": "再現率",
        "Opt4": "F値",
        "Opt5": "RMSE",
        "Opt6": "決定係数",
        "Answer_Idx": 1,
        "Explanation": "誤検知を減らすためには、適合率（Precision）を重視して設定します。",
        "Link": "https://example.com/spam-precision"
    },
    {
        "ID": "ML-019",
        "Category": "3.機械学習の手法",
        "Question": "モデルの複雑さと、データの予測誤差の間に存在する「あちらを立てればこちらが立たぬ」関係は？",
        "Opt1": "バイアス・バリアンスのトレードオフ",
        "Opt2": "ノー・フリーランチの定理",
        "Opt3": "醜いアヒルの子の定理",
        "Opt4": "次元の呪い",
        "Opt5": "大数の法則",
        "Opt6": "中心極限定理",
        "Answer_Idx": 0,
        "Explanation": "モデルを単純にするとバイアスが増え、複雑にするとバリアンス（過学習）が増えます。",
        "Link": "https://example.com/bias-variance"
    },
    {
        "ID": "ML-020",
        "Category": "3.機械学習の手法",
        "Question": "「あらゆる問題において、常に他のアルゴリズムより優れたアルゴリズムは存在しない」とする定理は？",
        "Opt1": "次元の呪い",
        "Opt2": "ムーアの法則",
        "Opt3": "ノー・フリーランチ定理",
        "Opt4": "中心極限定理",
        "Opt5": "ベイズの定理",
        "Opt6": "パレートの法則",
        "Answer_Idx": 2,
        "Explanation": "問題に合わせて適切な手法を選択することの重要性を説いています。",
        "Link": "https://example.com/no-free-lunch"
    },
    {
        "ID": "ML-021",
        "Category": "3.機械学習の手法",
        "Question": "「回帰」という名称が含まれるが、実際には二値分類などの「分類」に用いられる手法はどれか？",
        "Opt1": "線形回帰",
        "Opt2": "ロジスティック回帰",
        "Opt3": "リッジ回帰",
        "Opt4": "ラッソ回帰",
        "Opt5": "多項式回帰",
        "Opt6": "ステップワイズ回帰",
        "Answer_Idx": 1,
        "Explanation": "【ひっかけ頻出】ロジスティック回帰は、あるクラスに属する「確率」を計算し、分類を行う手法です。",
        "Link": "https://example.com/logistic-regression"
    },
    {
        "ID": "ML-022",
        "Category": "3.機械学習の手法",
        "Question": "ロジスティック回帰において、出力を0から1の範囲に収めるために用いられる関数はどれか？",
        "Opt1": "ステップ関数",
        "Opt2": "サイン関数",
        "Opt3": "シグモイド関数",
        "Opt4": "ReLU関数",
        "Opt5": "タンジェント関数",
        "Opt6": "ソフトプラス関数",
        "Answer_Idx": 2,
        "Explanation": "シグモイド関数は $1 / (1 + \\exp(-x))$ で表され、出力を確率として解釈可能にします。",
        "Link": "https://example.com/sigmoid"
    },
    {
        "ID": "ML-023",
        "Category": "3.機械学習の手法",
        "Question": "サポートベクターマシン（SVM）において、クラス間の境界線と最も近いデータ点との距離を何というか？",
        "Opt1": "カーネル",
        "Opt2": "サポートベクトル",
        "Opt3": "マージン",
        "Opt4": "スラッシュ",
        "Opt5": "バイアス",
        "Opt6": "ラグランジュ",
        "Answer_Idx": 2,
        "Explanation": "SVMはこの「マージン」を最大化するように境界線を決定します。",
        "Link": "https://example.com/svm-margin"
    },
    {
        "ID": "ML-024",
        "Category": "3.機械学習の手法",
        "Question": "SVMにおいて、境界線の決定に直接寄与する、境界に最も近いデータ点のことを何と呼ぶか？",
        "Opt1": "決定境界",
        "Opt2": "特徴点",
        "Opt3": "サポートベクトル",
        "Opt4": "コアポイント",
        "Opt5": "セントロイド",
        "Opt6": "外れ値",
        "Answer_Idx": 2,
        "Explanation": "この少数のデータ点（サポートベクトル）だけでモデルが定義されるのがSVMの特徴です。",
        "Link": "https://example.com/support-vector"
    },
    {
        "ID": "ML-025",
        "Category": "3.機械学習の手法",
        "Question": "SVMで線形分離不可能なデータを、高次元に写像して分類可能にする手法を何というか？",
        "Opt1": "次元削減",
        "Opt2": "正規化",
        "Opt3": "カーネル法",
        "Opt4": "アンサンブル法",
        "Opt5": "正則化",
        "Opt6": "標準化",
        "Answer_Idx": 2,
        "Explanation": "カーネルトリックを用いることで、複雑な境界線も効率的に計算できます。",
        "Link": "https://example.com/kernel-method"
    },
    {
        "ID": "ML-026",
        "Category": "3.機械学習の手法",
        "Question": "データを条件分岐の積み重ね（木構造）で分類する手法を何というか？",
        "Opt1": "決定木",
        "Opt2": "ランダムフォレスト",
        "Opt3": "K近傍法",
        "Opt4": "ナイーブベイズ",
        "Opt5": "ロジスティック回帰",
        "Opt6": "主成分分析",
        "Answer_Idx": 0,
        "Explanation": "解釈性が高く、人間が判断プロセスを理解しやすいのが特徴です。",
        "Link": "https://example.com/decision-tree"
    },
    {
        "ID": "ML-027",
        "Category": "3.機械学習の手法",
        "Question": "決定木において、分割の良さを測る指標（不純度）として一般的に用いられるのはどれか？",
        "Opt1": "ジニ係数",
        "Opt2": "決定係数",
        "Opt3": "相関係数",
        "Opt4": "F値",
        "Opt5": "AUC",
        "Opt6": "標準偏差",
        "Answer_Idx": 0,
        "Explanation": "ジニ不純度や情報利得（エントロピー）が低いほど、クラスが綺麗に分かれていることを示します。",
        "Link": "https://example.com/gini-index"
    },
    {
        "ID": "ML-028",
        "Category": "3.機械学習の手法",
        "Question": "判定したいデータの近くにあるK個のデータの多数決でクラスを決定する手法はどれか？",
        "Opt1": "K-means法",
        "Opt2": "K近傍法（k-NN）",
        "Opt3": "決定木",
        "Opt4": "SVM",
        "Opt5": "ロジスティック回帰",
        "Opt6": "リッジ回帰",
        "Answer_Idx": 1,
        "Explanation": "「近所の人がそうなら自分もそうだろう」という単純なアルゴリズムです。K-meansとの混同に注意。",
        "Link": "https://example.com/knn"
    },
    {
        "ID": "ML-029",
        "Category": "3.機械学習の手法",
        "Question": "K近傍法において、Kの値を小さくしすぎた場合に起こりやすい現象はどれか？",
        "Opt1": "未学習",
        "Opt2": "過学習（ノイズに敏感になる）",
        "Opt3": "計算速度の向上",
        "Opt4": "モデルの単純化",
        "Opt5": "バイアスの増大",
        "Opt6": "変化なし",
        "Answer_Idx": 1,
        "Explanation": "K=1にすると、すぐ隣にあるノードの影響を強く受けすぎ、ノイズに過剰適合してしまいます。",
        "Link": "https://example.com/knn-overfitting"
    },
    {
        "ID": "ML-030",
        "Category": "3.機械学習の手法",
        "Question": "多数の決定木を構築し、それらの予測結果を多数決で統合するアンサンブル学習の手法は？",
        "Opt1": "ロジスティック回帰",
        "Opt2": "ランダムフォレスト",
        "Opt3": "単回帰分析",
        "Opt4": "PCA",
        "Opt5": "K-means",
        "Opt6": "SVM",
        "Answer_Idx": 1,
        "Explanation": "バギング（Bagging）という手法を決定木に応用した代表例です。",
        "Link": "https://example.com/random-forest"
    },
    {
        "ID": "ML-031",
        "Category": "3.機械学習の手法",
        "Question": "ランダムフォレストにおいて、データのサブセットをランダムに抽出して複数のモデルを作る手法を何というか？",
        "Opt1": "ブースティング",
        "Opt2": "バギング",
        "Opt3": "スタッキング",
        "Opt4": "プーリング",
        "Opt5": "正則化",
        "Opt6": "ホールドアウト",
        "Answer_Idx": 1,
        "Explanation": "Bootstrap Aggregatingの略です。バリアンス（過学習）を抑える効果があります。",
        "Link": "https://example.com/bagging"
    },
    {
        "ID": "ML-032",
        "Category": "3.機械学習の手法",
        "Question": "前段のモデルが間違えたデータに重みを置き、順次モデルを改良していくアンサンブル手法は？",
        "Opt1": "バギング",
        "Opt2": "ブースティング",
        "Opt3": "ホールドアウト",
        "Opt4": "クロスバリデーション",
        "Opt5": "PCA",
        "Opt6": "K-means",
        "Answer_Idx": 1,
        "Explanation": "勾配ブースティングやAdaBoostがこれに該当します。バイアスを減らす効果が強いです。",
        "Link": "https://example.com/boosting"
    },
    {
        "ID": "ML-033",
        "Category": "3.機械学習の手法",
        "Question": "SVMにおいて、マージン内部に一部のデータが入り込むことを許容する設定を何というか？",
        "Opt1": "ハードマージン",
        "Opt2": "ソフトマージン",
        "Opt3": "ラージマージン",
        "Opt4": "スモールマージン",
        "Opt5": "ワイドマージン",
        "Opt6": "ディープマージン",
        "Answer_Idx": 1,
        "Explanation": "ソフトマージンにすることで、外れ値に強い（過学習しにくい）モデルになります。",
        "Link": "https://example.com/soft-margin"
    },
    {
        "ID": "ML-034",
        "Category": "3.機械学習の手法",
        "Question": "決定木が複雑になりすぎて過学習するのを防ぐため、不要な枝を削除する操作を何というか？",
        "Opt1": "枝打ち",
        "Opt2": "剪定（プルーニング）",
        "Opt3": "正規化",
        "Opt4": "正則化",
        "Opt5": "ドロップアウト",
        "Opt6": "プーリング",
        "Answer_Idx": 1,
        "Explanation": "木を適度な深さで止める、または後から削ることで汎化性能を高めます。",
        "Link": "https://example.com/pruning"
    },
    {
        "ID": "ML-035",
        "Category": "3.機械学習の手法",
        "Question": "「事象Aが起きた下で事象Bが起きる確率」を利用する、ベイズの定理に基づいた分類器は？",
        "Opt1": "SVM",
        "Opt2": "決定木",
        "Opt3": "ナイーブベイズ",
        "Opt4": "K近傍法",
        "Opt5": "ロジスティック回帰",
        "Opt6": "ランダムフォレスト",
        "Answer_Idx": 2,
        "Explanation": "スパムメール判定などで伝統的に使われてきた高速なアルゴリズムです。",
        "Link": "https://example.com/naive-beys"
    },
    {
        "ID": "ML-036",
        "Category": "3.機械学習の手法",
        "Question": "ナイーブベイズ分類器において「ナイーブ（素朴）」と呼ばれる理由はどれか？",
        "Opt1": "計算が非常に単純だから",
        "Opt2": "各特徴量が独立であると仮定するから",
        "Opt3": "学習データが少なくて済むから",
        "Opt4": "1950年代に作られたから",
        "Opt5": "深層学習を使わないから",
        "Opt6": "パラメータが0だから",
        "Answer_Idx": 1,
        "Explanation": "実際には特徴量間に相関がある場合でも、独立であると仮定して計算を簡略化するためです。",
        "Link": "https://example.com/naive-reason"
    },
    {
        "ID": "ML-037",
        "Category": "3.機械学習の手法",
        "Question": "回帰問題において、誤差の2乗和を最小にするようにパラメータを決定する手法を何というか？",
        "Opt1": "最尤法",
        "Opt2": "最小二乗法",
        "Opt3": "勾配降下法",
        "Opt4": "主成分分析",
        "Opt5": "ランダムウォーク",
        "Opt6": "ラグランジュ未定乗数法",
        "Answer_Idx": 1,
        "Explanation": "単回帰や重回帰分析で最も一般的に使われる推定法です。",
        "Link": "https://example.com/least-squares"
    },
    {
        "ID": "ML-038",
        "Category": "3.機械学習の手法",
        "Question": "複数の説明変数を用いる回帰分析を「重回帰分析」と呼ぶが、説明変数同士に強い相関がある問題は？",
        "Opt1": "多重共線性（マルチコリニアリティ）",
        "Opt2": "過学習",
        "Opt3": "勾配消失",
        "Opt4": "自己相関",
        "Opt5": "ヘテロスセダスティシティ",
        "Opt6": "次元の呪い",
        "Answer_Idx": 0,
        "Explanation": "マルチコが起きると係数の推定が不安定になり、解釈が困難になります。",
        "Link": "https://example.com/multicolliniarity"
    },
    {
        "ID": "ML-039",
        "Category": "3.機械学習の手法",
        "Question": "SVMにおいて、高次元空間への写像を具体的に計算せずに、内積のみで計算を済ませる手法は？",
        "Opt1": "カーネルトリック",
        "Opt2": "プーリングトリック",
        "Opt3": "ドロップアウトトリック",
        "Opt4": "モンテカルロ法",
        "Opt5": "誤差逆伝播法",
        "Opt6": "ソフトマックス関数",
        "Answer_Idx": 0,
        "Explanation": "計算負荷を劇的に抑えつつ、非線形な境界線を引くための数学的な工夫です。",
        "Link": "https://example.com/kernel-trick"
    },
    {
        "ID": "ML-040",
        "Category": "3.機械学習の手法",
        "Question": "データセットの中から、予測に寄与する重要な変数だけを選び出す作業を何というか？",
        "Opt1": "特徴量抽出",
        "Opt2": "特徴量選択",
        "Opt3": "次元削減",
        "Opt4": "ハイパーパラメータ調整",
        "Opt5": "アノテーション",
        "Opt6": "クレンジング",
        "Answer_Idx": 1,
        "Explanation": "不必要な変数を削ることで、モデルをシンプルにし過学習を防ぎます。",
        "Link": "https://example.com/feature-selection"
    },
    {
        "ID": "ML-041",
        "Category": "3.機械学習の手法",
        "Question": "データにラベルがない状態で、データの構造やパターンを見つけ出す学習形態は？",
        "Opt1": "教師あり学習",
        "Opt2": "教師なし学習",
        "Opt3": "強化学習",
        "Opt4": "半教師あり学習",
        "Opt5": "転移学習",
        "Opt6": "蒸留",
        "Answer_Idx": 1,
        "Explanation": "正解（ラベル）を与えずに、データ自体の類似性などでグループ分け等を行う手法です。",
        "Link": "https://example.com/unsupervised"
    },
    {
        "ID": "ML-042",
        "Category": "3.機械学習の手法",
        "Question": "データを似たもの同士のグループに分ける手法を何というか？",
        "Opt1": "回帰",
        "Opt2": "分類",
        "Opt3": "クラスタリング",
        "Opt4": "次元圧縮",
        "Opt5": "異常検知",
        "Opt6": "強化学習",
        "Answer_Idx": 2,
        "Explanation": "クラスタリングは教師なし学習の代表的なタスクです。正解クラスが決まっている「分類」との違いに注意。",
        "Link": "https://example.com/clustering"
    },
    {
        "ID": "ML-043",
        "Category": "3.機械学習の手法",
        "Question": "あらかじめ決めた数（K個）のグループに、重心からの距離に基づいてデータを分ける手法は？",
        "Opt1": "K近傍法",
        "Opt2": "K-means法",
        "Opt3": "主成分分析",
        "Opt4": "階層クラスタリング",
        "Opt5": "サポートベクターマシン",
        "Opt6": "決定木",
        "Answer_Idx": 1,
        "Explanation": "K-means法（K平均法）は、重心（セントロイド）を繰り返し更新してクラスタを決定します。",
        "Link": "https://example.com/k-means"
    },
    {
        "ID": "ML-044",
        "Category": "3.機械学習の手法",
        "Question": "K-means法において、最初にランダムに設定されるグループの重心のことを何と呼ぶか？",
        "Opt1": "サポートベクトル",
        "Opt2": "エッジ",
        "Opt3": "セントロイド",
        "Opt4": "カーネル",
        "Opt5": "フィラー",
        "Opt6": "バイアス",
        "Answer_Idx": 2,
        "Explanation": "セントロイド（重心）と各データ点の距離を計算し、最も近いグループに割り当てます。",
        "Link": "https://example.com/centroid"
    },
    {
        "ID": "ML-045",
        "Category": "3.機械学習の手法",
        "Question": "似たもの同士を順に結合し、樹形図（デンドログラム）を作成するクラスタリング手法は？",
        "Opt1": "非階層クラスタリング",
        "Opt2": "階層クラスタリング",
        "Opt3": "K-means法",
        "Opt4": "主成分分析",
        "Opt5": "ロジスティック回帰",
        "Opt6": "ナイーブベイズ",
        "Answer_Idx": 1,
        "Explanation": "あらかじめクラスタ数を決めなくても、トーナメント表のような図で構造を確認できます。",
        "Link": "https://example.com/hierarchical-clustering"
    },
    {
        "ID": "ML-046",
        "Category": "3.機械学習の手法",
        "Question": "階層クラスタリングの結果を視覚化した、木構造の図のことを何と呼ぶか？",
        "Opt1": "ヒストグラム",
        "Opt2": "散布図",
        "Opt3": "デンドログラム",
        "Opt4": "ROC曲線",
        "Opt5": "混同行列",
        "Opt6": "決定木",
        "Answer_Idx": 2,
        "Explanation": "デンドログラムを見ることで、どの程度の類似度でグループを分けるか判断できます。",
        "Link": "https://example.com/dendrogram"
    },
    {
        "ID": "ML-047",
        "Category": "3.機械学習の手法",
        "Question": "多次元のデータを、情報の損失を最小限に抑えつつ低次元に変換する手法を何というか？",
        "Opt1": "次元増大",
        "Opt2": "次元圧縮",
        "Opt3": "クラスタリング",
        "Opt4": "回帰",
        "Opt5": "バッチ正規化",
        "Opt6": "アンサンブル学習",
        "Answer_Idx": 1,
        "Explanation": "データの可視化や計算コスト削減のために行われます。主成分分析（PCA）が代表例です。",
        "Link": "https://example.com/dim-reduction"
    },
    {
        "ID": "ML-048",
        "Category": "3.機械学習の手法",
        "Question": "主成分分析（PCA）において、元のデータの分散を最も大きく保つ新しい軸を何というか？",
        "Opt1": "第1主成分",
        "Opt2": "第2主成分",
        "Opt3": "固有ベクトル",
        "Opt4": "寄与率",
        "Opt5": "累積寄与率",
        "Opt6": "主成分得点",
        "Answer_Idx": 0,
        "Explanation": "第1主成分は、データのばらつきを最もよく説明する方向を指します。",
        "Link": "https://example.com/pca-1st"
    },
    {
        "ID": "ML-049",
        "Category": "3.機械学習の手法",
        "Question": "PCAにおいて、ある主成分が元のデータの情報をどの程度保持しているかを示す指標は？",
        "Opt1": "相関係数",
        "Opt2": "寄与率",
        "Opt3": "標準偏差",
        "Opt4": "決定係数",
        "Opt5": "ジニ係数",
        "Opt6": "F値",
        "Answer_Idx": 1,
        "Explanation": "各主成分が元のデータの分散をどれだけカバーしているかを割合で示したものです。",
        "Link": "https://example.com/contribution-rate"
    },
    {
        "ID": "ML-050",
        "Category": "3.機械学習の手法",
        "Question": "PCAにおいて、第1主成分から順に寄与率を足し合わせていった合計値を何というか？",
        "Opt1": "累積寄与率",
        "Opt2": "決定係数",
        "Opt3": "総分散",
        "Opt4": "固有値和",
        "Opt5": "情報利得",
        "Opt6": "AUC",
        "Answer_Idx": 0,
        "Explanation": "累積寄与率が0.8（80%）程度になるまで主成分を採用するのが一般的です。",
        "Link": "https://example.com/accumulated-contribution"
    },
    {
        "ID": "ML-051",
        "Category": "3.機械学習の手法",
        "Question": "「K-means法」と「K近傍法（k-NN）」の決定的な違いとして正しいものはどれか？",
        "Opt1": "K-meansは分類でk-NNは回帰",
        "Opt2": "K-meansは教師ありでk-NNは教師なし",
        "Opt3": "K-meansは教師なしでk-NNは教師あり",
        "Opt4": "両方とも回帰の手法である",
        "Opt5": "両方とも次元圧縮の手法である",
        "Opt6": "違いはない",
        "Answer_Idx": 2,
        "Explanation": "【超頻出】K-meansは「クラスタリング（教師なし）」、k-NNは「分類（教師あり）」です。",
        "Link": "https://example.com/kmeans-vs-knn"
    },
    {
        "ID": "ML-052",
        "Category": "3.機械学習の手法",
        "Question": "高次元データにおいて、データの密度が希薄になり、距離の概念が意味をなさなくなる問題を何というか？",
        "Opt1": "組み合わせの爆発",
        "Opt2": "次元の呪い",
        "Opt3": "フレーム問題",
        "Opt4": "ハルシネーション",
        "Opt5": "勾配消失",
        "Opt6": "過学習",
        "Answer_Idx": 1,
        "Explanation": "次元が増えすぎると、すべての点が「遠く」なり、学習が困難になる現象です。",
        "Link": "https://example.com/curse-of-dimensionality"
    },
    {
        "ID": "ML-053",
        "Category": "3.機械学習の手法",
        "Question": "大量の正常データのみから学習し、そこから外れたデータを検出するタスクを何というか？",
        "Opt1": "回帰",
        "Opt2": "分類",
        "Opt3": "異常検知（外れ値検知）",
        "Opt4": "次元圧縮",
        "Opt5": "クラスタリング",
        "Opt6": "強化学習",
        "Answer_Idx": 2,
        "Explanation": "故障予兆検知や不正利用検知などに使われます。1クラスSVMなどが用いられます。",
        "Link": "https://example.com/anomaly-detection"
    },
    {
        "ID": "ML-054",
        "Category": "3.機械学習の手法",
        "Question": "異常検知において、あるデータが「どの程度異常か」を数値化したものを何というか？",
        "Opt1": "異常度（異常スコア）",
        "Opt2": "損失値",
        "Opt3": "不純度",
        "Opt4": "寄与率",
        "Opt5": "重要度",
        "Opt6": "尤度",
        "Answer_Idx": 0,
        "Explanation": "設定したしきい値を異常度が超えると「異常」と判定します。",
        "Link": "https://example.com/anomaly-score"
    },
    {
        "ID": "ML-055",
        "Category": "3.機械学習の手法",
        "Question": "多数の変数からなるデータの相関関係を整理し、少数の共通因子を見つけ出す手法は？",
        "Opt1": "主成分分析",
        "Opt2": "因子分析",
        "Opt3": "クラスター分析",
        "Opt4": "決定木",
        "Opt5": "ロジスティック回帰",
        "Opt6": "アソシエーション分析",
        "Answer_Idx": 1,
        "Explanation": "PCAが「データをまとめる」のに対し、因子分析は「背後にある原因（因子）を探る」手法です。",
        "Link": "https://example.com/factor-analysis"
    },
    {
        "ID": "ML-056",
        "Category": "3.機械学習の手法",
        "Question": "購買データなどから「Aを買った人はBも買う」といったルールを抽出する手法は？",
        "Opt1": "アソシエーション分析",
        "Opt2": "回帰分析",
        "Opt3": "主成分分析",
        "Opt4": "異常検知",
        "Opt5": "強化学習",
        "Opt6": "分類",
        "Answer_Idx": 0,
        "Explanation": "マーケットバスケット分析とも呼ばれます。「おむつとビール」の例が有名です。",
        "Link": "https://example.com/association-analysis"
    },
    {
        "ID": "ML-057",
        "Category": "3.機械学習の手法",
        "Question": "アソシエーション分析において、全データの中でその組み合わせが発生する割合を何というか？",
        "Opt1": "信頼度（Confidence）",
        "Opt2": "支持度（Support）",
        "Opt3": "リフト値",
        "Opt4": "期待度",
        "Opt5": "寄与率",
        "Opt6": "適合率",
        "Answer_Idx": 1,
        "Explanation": "Support（支持度）は、そのルールがどの程度一般的かを示します。",
        "Link": "https://example.com/support"
    },
    {
        "ID": "ML-058",
        "Category": "3.機械学習の手法",
        "Question": "アソシエーション分析で「Aを買った人のうち、Bも買った人の割合」を示す指標は？",
        "Opt1": "支持度",
        "Opt2": "信頼度（Confidence）",
        "Opt3": "リフト値",
        "Opt4": "F値",
        "Opt5": "再現率",
        "Opt6": "正解率",
        "Answer_Idx": 1,
        "Explanation": "Confidence（信頼度）は、ルールの「強さ」を示します。",
        "Link": "https://example.com/confidence"
    },
    {
        "ID": "ML-059",
        "Category": "3.機械学習の手法",
        "Question": "アソシエーション分析で、あるルールが「偶然以上の相関があるか」を判断する指標は？",
        "Opt1": "支持度",
        "Opt2": "信頼度",
        "Opt3": "リフト値",
        "Opt4": "ジニ係数",
        "Opt5": "累積寄与率",
        "Opt6": "RMSE",
        "Answer_Idx": 2,
        "Explanation": "リフト値が1を超えると、偶然よりも高い確率で併買されていることを示します。",
        "Link": "https://example.com/lift"
    },
    {
        "ID": "ML-060",
        "Category": "3.機械学習の手法",
        "Question": "非線形な次元圧縮手法で、データの局所的な構造を維持しつつ低次元化する、可視化によく使われる手法は？",
        "Opt1": "PCA",
        "Opt2": "t-SNE",
        "Opt3": "Lasso",
        "Opt4": "Ridge",
        "Opt5": "K-means",
        "Opt6": "LDA",
        "Answer_Idx": 1,
        "Explanation": "高次元データのクラスター構造を2次元や3次元で綺麗に描画するのに適しています。",
        "Link": "https://example.com/t-sne"
    },
    {
        "ID": "ML-061",
        "Category": "3.機械学習の手法",
        "Question": "エージェントが環境と試行錯誤しながら、報酬を最大化するように学習する形態は？",
        "Opt1": "教師あり学習",
        "Opt2": "教師なし学習",
        "Opt3": "強化学習",
        "Opt4": "転移学習",
        "Opt5": "自己学習",
        "Opt6": "蒸留",
        "Answer_Idx": 2,
        "Explanation": "報酬（スコア）を最大化する「行動」を学習するのが強化学習の特徴です。",
        "Link": "https://example.com/rl-intro"
    },
    {
        "ID": "ML-062",
        "Category": "3.機械学習の手法",
        "Question": "強化学習において、行動を決定する主体（学習者）のことを何と呼ぶか？",
        "Opt1": "環境",
        "Opt2": "エージェント",
        "Opt3": "報酬",
        "Opt4": "ステート",
        "Opt5": "ポリシー",
        "Opt6": "アクション",
        "Answer_Idx": 1,
        "Explanation": "エージェントは環境を観測し、報酬を得るために行動を選択します。",
        "Link": "https://example.com/agent-def"
    },
    {
        "ID": "ML-063",
        "Category": "3.機械学習の手法",
        "Question": "強化学習において、エージェントが「どの状態でどの行動をとるか」を定めたルールは？",
        "Opt1": "報酬",
        "Opt2": "環境",
        "Opt3": "方策（ポリシー）",
        "Opt4": "価値関数",
        "Opt5": "Q値",
        "Opt6": "エピソード",
        "Answer_Idx": 2,
        "Explanation": "ポリシー（Policy）は、ある状態における行動の選択確率を定義したものです。",
        "Link": "https://example.com/policy"
    },
    {
        "ID": "ML-064",
        "Category": "3.機械学習の手法",
        "Question": "強化学習で、将来得られる報酬を現在の価値に換算する際に用いる係数を何というか？",
        "Opt1": "学習率",
        "Opt2": "割引率",
        "Opt3": "正則化パラメータ",
        "Opt4": "減衰率",
        "Opt5": "効率係数",
        "Opt6": "評価指数",
        "Answer_Idx": 1,
        "Explanation": "割引率（γ）は0〜1の値をとり、将来の報酬をどれだけ重視するかを決定します。",
        "Link": "https://example.com/discount-rate"
    },
    {
        "ID": "ML-065",
        "Category": "3.機械学習の手法",
        "Question": "「未知の行動を試すこと」と「過去の経験から最善の行動をとること」の両立の難しさを何というか？",
        "Opt1": "バイアス・バリアンスのジレンマ",
        "Opt2": "探索と利用のトレードオフ",
        "Opt3": "次元の呪い",
        "Opt4": "フレーム問題",
        "Opt5": "ノー・フリーランチ",
        "Opt6": "局所最適解",
        "Answer_Idx": 1,
        "Explanation": "新しい可能性を探る（探索）と、今の知識を使う（利用）のバランスが強化学習の鍵です。",
        "Link": "https://example.com/exploration-exploitation"
    },
    {
        "ID": "ML-066",
        "Category": "3.機械学習の手法",
        "Question": "強化学習の代表的なアルゴリズムで、状態と行動の組み合わせごとの価値を更新する手法は？",
        "Opt1": "Q学習",
        "Opt2": "K-means法",
        "Opt3": "SVM",
        "Opt4": "決定木",
        "Opt5": "主成分分析",
        "Opt6": "回帰分析",
        "Answer_Idx": 0,
        "Explanation": "Q学習は、Qテーブルと呼ばれる表を更新していく最も基本的なアルゴリズムの一つです。",
        "Link": "https://example.com/q-learning"
    },
    {
        "ID": "ML-067",
        "Category": "3.機械学習の手法",
        "Question": "強化学習において、開始から終了（ゲームセットなど）までの一連の過程を何と呼ぶか？",
        "Opt1": "サイクル",
        "Opt2": "エピソード",
        "Opt3": "ステップ",
        "Opt4": "セッション",
        "Opt5": "ターム",
        "Opt6": "ラウンド",
        "Answer_Idx": 1,
        "Explanation": "1エピソード終了ごとに学習を進めたり、累積報酬を計算したりします。",
        "Link": "https://example.com/episode"
    },
    {
        "ID": "ML-068",
        "Category": "3.機械学習の手法",
        "Question": "「現在の状態が次の方策にのみ影響し、過去の経緯には依存しない」という強化学習の前提は？",
        "Opt1": "ベルマン方程式",
        "Opt2": "マルコフ決定過程（MDP）",
        "Opt3": "ベイズの定理",
        "Opt4": "大数の法則",
        "Opt5": "勾配降下法",
        "Opt6": "中心極限定理",
        "Answer_Idx": 1,
        "Explanation": "「今の状態だけ見れば次の判断ができる」という性質をマルコフ性と呼びます。",
        "Link": "https://example.com/mdp"
    },
    {
        "ID": "ML-069",
        "Category": "3.機械学習の手法",
        "Question": "強化学習の応用例で、複数のスロットマシンのどれを引けば利益が最大になるかを探る問題は？",
        "Opt1": "巡回セールスマン問題",
        "Opt2": "ナップサック問題",
        "Opt3": "多腕バンディット問題",
        "Opt4": "ハノイの塔",
        "Opt5": "最短経路問題",
        "Opt6": "囚人のジレンマ",
        "Answer_Idx": 2,
        "Explanation": "「探索と利用のトレードオフ」を考える上で最もシンプルな例題です。",
        "Link": "https://example.com/multi-armed-bandit"
    },
    {
        "ID": "ML-070",
        "Category": "3.機械学習の手法",
        "Question": "ナイーブベイズ分類器の基礎となる、結果から原因を推定する確率の定理は？",
        "Opt1": "ピタゴラスの定理",
        "Opt2": "ベイズの定理",
        "Opt3": "フェルマの最終定理",
        "Opt4": "中心極限定理",
        "Opt5": "ガウスの定理",
        "Opt6": "テイラー展開",
        "Answer_Idx": 1,
        "Explanation": "条件付き確率 P(A|B) を求めるための、機械学習に不可欠な定理です。",
        "Link": "https://example.com/bayes-theorem"
    },
    {
        "ID": "ML-071",
        "Category": "3.機械学習の手法",
        "Question": "「明日雨が降る」確率に対し、「空に雲がある」という情報を得た後の確率を何というか？",
        "Opt1": "事前確率",
        "Opt2": "事後確率",
        "Opt3": "最尤推定量",
        "Opt4": "期待値",
        "Opt5": "周辺確率",
        "Opt6": "同時確率",
        "Answer_Idx": 1,
        "Explanation": "情報を得る前の確率が「事前確率」、得た後の確率が「事後確率」です。",
        "Link": "https://example.com/posterior-probability"
    },
    {
        "ID": "ML-072",
        "Category": "3.機械学習の手法",
        "Question": "観測できない変数（潜在変数）を含むモデルのパラメータを推定するためのアルゴリズムは？",
        "Opt1": "Q学習",
        "Opt2": "EMアルゴリズム",
        "Opt3": "K-means法",
        "Opt4": "A*アルゴリズム",
        "Opt5": "ミニマックス法",
        "Opt6": "勾配降下法",
        "Answer_Idx": 1,
        "Explanation": "Expectation-Maximization（期待値最大化）アルゴリズム。混合ガウスモデルなどで使われます。",
        "Link": "https://example.com/em-algorithm"
    },
    {
        "ID": "ML-073",
        "Category": "3.機械学習の手法",
        "Question": "複数の学習器を並列に学習させ、それらの平均や多数決で予測を行うアンサンブル手法は？",
        "Opt1": "バギング",
        "Opt2": "ブースティング",
        "Opt3": "スタッキング",
        "Opt4": "ラッソ",
        "Opt5": "リッジ",
        "Opt6": "ホールドアウト",
        "Answer_Idx": 0,
        "Explanation": "代表例はランダムフォレスト。バリアンス（過学習）の抑制に効果的です。",
        "Link": "https://example.com/bagging-intro"
    },
    {
        "ID": "ML-074",
        "Category": "3.機械学習の手法",
        "Question": "複数の学習器を直列に学習させ、前の学習器のミスを修正するように学習する手法は？",
        "Opt1": "バギング",
        "Opt2": "ブースティング",
        "Opt3": "スタッキング",
        "Opt4": "ドロップアウト",
        "Opt5": "プーリング",
        "Opt6": "正則化",
        "Answer_Idx": 1,
        "Explanation": "代表例はXGBoostやLightGBM。バイアス（未学習）の抑制に効果的です。",
        "Link": "https://example.com/boosting-intro"
    },
    {
        "ID": "ML-075",
        "Category": "3.機械学習の手法",
        "Question": "異なる種類の複数のモデルの予測結果を、別のモデル（メタモデル）の入力として学習させる手法は？",
        "Opt1": "バギング",
        "Opt2": "ブースティング",
        "Opt3": "スタッキング",
        "Opt4": "ホールドアウト",
        "Opt5": "クロスバリデーション",
        "Opt6": "転移学習",
        "Answer_Idx": 2,
        "Explanation": "モデルを「積み重ねる（Stacking）」手法。コンペティション等でよく使われます。",
        "Link": "https://example.com/stacking"
    },
    {
        "ID": "ML-076",
        "Category": "3.機械学習の手法",
        "Question": "データのスケール（単位）を、平均0、標準偏差1になるように変換する操作を何というか？",
        "Opt1": "正規化（Normalization）",
        "Opt2": "標準化（Standardization）",
        "Opt3": "正則化（Regularization）",
        "Opt4": "二値化",
        "Opt5": "量子化",
        "Opt6": "白色化",
        "Answer_Idx": 1,
        "Explanation": "SVMやPCAなど、距離を計算するアルゴリズムでは必須の処理です。",
        "Link": "https://example.com/standardization"
    },
    {
        "ID": "ML-077",
        "Category": "3.機械学習の手法",
        "Question": "データの値を0から1の範囲に収めるように変換する操作を一般的に何というか？",
        "Opt1": "標準化",
        "Opt2": "正規化（Normalization）",
        "Opt3": "正則化",
        "Opt4": "平滑化",
        "Opt5": "平坦化",
        "Opt6": "バッチ処理",
        "Answer_Idx": 1,
        "Explanation": "最小値を0、最大値を1にするMin-Max Scalingが代表的です。",
        "Link": "https://example.com/normalization"
    },
    {
        "ID": "ML-078",
        "Category": "3.機械学習の手法",
        "Question": "「特徴量の数が多すぎると、モデルが複雑になり精度が落ちる」ことを防ぐための手法は？",
        "Opt1": "次元の呪い",
        "Opt2": "特徴量選択",
        "Opt3": "データの水増し",
        "Opt4": "オーバーサンプリング",
        "Opt5": "アンダーサンプリング",
        "Opt6": "アノテーション",
        "Answer_Idx": 1,
        "Explanation": "重要な特徴量だけを絞り込むことで、汎化性能を高めます。",
        "Link": "https://example.com/feature-selection-2"
    },
    {
        "ID": "ML-079",
        "Category": "3.機械学習の手法",
        "Question": "サポートベクターマシンにおいて、誤差（マージン侵害）をどれだけ許容するかを調整する変数は？",
        "Opt1": "Cパラメータ",
        "Opt2": "学習率",
        "Opt3": "重み",
        "Opt4": "バイアス",
        "Opt5": "モーメンタム",
        "Opt6": "エントロピー",
        "Answer_Idx": 0,
        "Explanation": "Cが大きいほど誤分類を許さず（ハード）、小さいほど許容します（ソフト）。",
        "Link": "https://example.com/svm-c-param"
    },
    {
        "ID": "ML-080",
        "Category": "3.機械学習の手法",
        "Question": "決定木において、分類後に各グループ内のデータの種類がどれだけ混ざっているかを示す指標は？",
        "Opt1": "ジニ不純度",
        "Opt2": "寄与率",
        "Opt3": "決定係数",
        "Opt4": "累積寄与率",
        "Opt5": "リフト値",
        "Opt6": "信頼度",
        "Answer_Idx": 0,
        "Explanation": "不純度が低い（0に近い）ほど、特定のクラスに綺麗に分かれていることを意味します。",
        "Link": "https://example.com/gini-impurity"
    },
    {
        "ID": "ML-081",
        "Category": "3.機械学習の手法",
        "Question": "「癌患者が1%、健康な人が99%」のような、クラスの割合が大きく偏ったデータを何というか？",
        "Opt1": "不均衡データ",
        "Opt2": "ビッグデータ",
        "Opt3": "欠損データ",
        "Opt4": "構造化データ",
        "Opt5": "スパースデータ",
        "Opt6": "多次元データ",
        "Answer_Idx": 0,
        "Explanation": "このようなデータでは「正解率（Accuracy）」だけを見ると、モデルの性能を見誤る危険があります。",
        "Link": "https://example.com/imbalanced-data"
    },
    {
        "ID": "ML-082",
        "Category": "3.機械学習の手法",
        "Question": "不均衡データの対策として、少ない方のクラスのデータを人工的に増やす手法を何というか？",
        "Opt1": "アンダーサンプリング",
        "Opt2": "オーバーサンプリング",
        "Opt3": "正則化",
        "Opt4": "標準化",
        "Opt5": "次元削減",
        "Opt6": "主成分分析",
        "Answer_Idx": 1,
        "Explanation": "SMOTE（Synthetic Minority Over-sampling Technique）などが代表的なアルゴリズムです。",
        "Link": "https://example.com/oversampling"
    },
    {
        "ID": "ML-083",
        "Category": "3.機械学習の手法",
        "Question": "不均衡データの対策として、多い方のクラスのデータを削って割合を調整する手法を何というか？",
        "Opt1": "オーバーサンプリング",
        "Opt2": "アンダーサンプリング",
        "Opt3": "バギング",
        "Opt4": "ブースティング",
        "Opt5": "正規化",
        "Opt6": "二値化",
        "Answer_Idx": 1,
        "Explanation": "データ全体の件数は減りますが、計算コストを抑えつつクラス比を調整できます。",
        "Link": "https://example.com/undersampling"
    },
    {
        "ID": "ML-084",
        "Category": "3.機械学習の手法",
        "Question": "ハイパーパラメータの候補を網羅的にすべて試して、最適な組み合わせを見つける手法は？",
        "Opt1": "ランダムサーチ",
        "Opt2": "グリッドサーチ",
        "Opt3": "ベイズ最適化",
        "Opt4": "ホールドアウト法",
        "Opt5": "クロスバリデーション",
        "Opt6": "剪定",
        "Answer_Idx": 1,
        "Explanation": "しらみつぶしに調べるため確実ですが、候補が多いと計算時間が膨大になります。",
        "Link": "https://example.com/grid-search"
    },
    {
        "ID": "ML-085",
        "Category": "3.機械学習の手法",
        "Question": "ハイパーパラメータを範囲の中からランダムに選択して試行し、効率的に探索する手法は？",
        "Opt1": "グリッドサーチ",
        "Opt2": "ランダムサーチ",
        "Opt3": "決定木",
        "Opt4": "K近傍法",
        "Opt5": "勾配降下法",
        "Opt6": "主成分分析",
        "Answer_Idx": 1,
        "Explanation": "グリッドサーチよりも計算資源を節約でき、重要なパラメータに集中できる場合があります。",
        "Link": "https://example.com/random-search"
    },
    {
        "ID": "ML-086",
        "Category": "3.機械学習の手法",
        "Question": "データを複数の正規分布（ガウス分布）の重み付き和としてモデル化する教師なし学習手法は？",
        "Opt1": "K-means法",
        "Opt2": "混合ガウスモデル（GMM）",
        "Opt3": "主成分分析",
        "Opt4": "ロジスティック回帰",
        "Opt5": "サポートベクターマシン",
        "Opt6": "決定木",
        "Answer_Idx": 1,
        "Explanation": "クラスタリング手法の一つで、点ではなく「分布」としてグループを捉えます。",
        "Link": "https://example.com/gmm"
    },
    {
        "ID": "ML-087",
        "Category": "3.機械学習の手法",
        "Question": "回帰問題において、外れ値の影響を受けにくい（ロバストな）性質を持つ損失関数はどれか？",
        "Opt1": "MSE（平均二乗誤差）",
        "Opt2": "MAE（平均絶対誤差）",
        "Opt3": "RMSE",
        "Opt4": "決定係数",
        "Opt5": "ジニ不純度",
        "Opt6": "エントロピー",
        "Answer_Idx": 1,
        "Explanation": "MAEは誤差を2乗しないため、外れ値による極端なペナルティを防ぐことができます。",
        "Link": "https://example.com/mae-robust"
    },
    {
        "ID": "ML-088",
        "Category": "3.機械学習の手法",
        "Question": "データの分布が「左に寄っている」や「右に寄っている」といった、非対称性を示す指標は？",
        "Opt1": "分散",
        "Opt2": "標準偏差",
        "Opt3": "歪度（わいど）",
        "Opt4": "尖度（せんど）",
        "Opt5": "相関係数",
        "Opt6": "共分散",
        "Answer_Idx": 2,
        "Explanation": "正規分布では0となり、左右どちらに裾が長いかを示します。",
        "Link": "https://example.com/skewness"
    },
    {
        "ID": "ML-089",
        "Category": "3.機械学習の手法",
        "Question": "データの分布が「中央に集中して尖っている」か「平坦か」を示す指標は？",
        "Opt1": "歪度",
        "Opt2": "尖度（せんど）",
        "Opt3": "平均値",
        "Opt4": "中央値",
        "Opt5": "最頻値",
        "Opt6": "範囲",
        "Answer_Idx": 1,
        "Explanation": "尖度が大きいほど、分布のピークが鋭く、裾が厚いことを示します。",
        "Link": "https://example.com/kurtosis"
    },
    {
        "ID": "ML-090",
        "Category": "3.機械学習の手法",
        "Question": "2つの変数の関係において、一方の増加に伴って他方が増加する傾向を示す指標（-1〜1）は？",
        "Opt1": "標準偏差",
        "Opt2": "共分散",
        "Opt3": "相関係数",
        "Opt4": "決定係数",
        "Opt5": "寄与率",
        "Opt6": "リフト値",
        "Answer_Idx": 2,
        "Explanation": "ピアソンの積率相関係数が一般的です。0は無相関を意味します。",
        "Link": "https://example.com/correlation"
    },
    {
        "ID": "ML-091",
        "Category": "3.機械学習の手法",
        "Question": "多クラス分類において、各クラスを「1」か「それ以外（0）」として扱う分類手法の総称は？",
        "Opt1": "One-vs-Rest (OvR)",
        "Opt2": "One-vs-One (OvO)",
        "Opt3": "マルチラベル分類",
        "Opt4": "回帰分析",
        "Opt5": "クラスタリング",
        "Opt6": "次元圧縮",
        "Answer_Idx": 0,
        "Explanation": "「犬 vs 犬以外」「猫 vs 猫以外」のように二値分類器を組み合わせて多クラスを扱います。",
        "Link": "https://example.com/ovr"
    },
    {
        "ID": "ML-092",
        "Category": "3.機械学習の手法",
        "Question": "二つの変数の関係が、直線的な関係（線形）でない場合を何と呼ぶか？",
        "Opt1": "線形",
        "Opt2": "非線形",
        "Opt3": "多重共線性",
        "Opt4": "自己相関",
        "Opt5": "正規分布",
        "Opt6": "一様分布",
        "Answer_Idx": 1,
        "Explanation": "現実世界のデータの多くは非線形であり、SVMのカーネル法などが有効です。",
        "Link": "https://example.com/non-linear"
    },
    {
        "ID": "ML-093",
        "Category": "3.機械学習の手法",
        "Question": "モデルの予測値 $y$ と正解値 $t$ の一致度を測るための関数 $L(y",
        "Opt1": "t)$ を何というか？",
        "Opt2": "活性化関数",
        "Opt3": "損失関数（誤差関数）",
        "Opt4": "シグモイド関数",
        "Opt5": "ステップ関数",
        "Opt6": "基底関数",
        "Answer_Idx": "ハッシュ関数",
        "Explanation": 1,
        "Link": "この損失関数の値を最小化するようにモデルのパラメータを学習させます。"
    },
    {
        "ID": "ML-094",
        "Category": "3.機械学習の手法",
        "Question": "学習の過程で、損失関数の値が徐々に減少していく様子をグラフ化したものを何というか？",
        "Opt1": "ROC曲線",
        "Opt2": "学習曲線（ラーニングカーブ）",
        "Opt3": "散布図",
        "Opt4": "ヒストグラム",
        "Opt5": "デンドログラム",
        "Opt6": "決定境界",
        "Answer_Idx": 1,
        "Explanation": "学習曲線を見ることで、過学習や未学習の兆候を判断できます。",
        "Link": "https://example.com/learning-curve"
    },
    {
        "ID": "ML-095",
        "Category": "3.機械学習の手法",
        "Question": "機械学習において、データの値を扱いやすくするために一定の間隔で区切る（離散化する）操作は？",
        "Opt1": "正規化",
        "Opt2": "標準化",
        "Opt3": "ビン化（ビニング）",
        "Opt4": "正則化",
        "Opt5": "二値化",
        "Opt6": "量子化",
        "Answer_Idx": 2,
        "Explanation": "連続値を「年代（10代、20代…）」のようにカテゴリ分けする手法です。",
        "Link": "https://example.com/binning"
    },
    {
        "ID": "ML-096",
        "Category": "3.機械学習の手法",
        "Question": "決定木において、一つのノードで分割を行う際に選ばれる、最も不純度を下げる変数の選び方は？",
        "Opt1": "ランダム選択",
        "Opt2": "全変数試行",
        "Opt3": "情報利得（利得比）最大化",
        "Opt4": "相関最大化",
        "Opt5": "分散最大化",
        "Opt6": "平均値選択",
        "Answer_Idx": 2,
        "Explanation": "エントロピーやジニ係数の減少量が最も大きい変数で分割を行います。",
        "Link": "https://example.com/info-gain"
    },
    {
        "ID": "ML-097",
        "Category": "3.機械学習の手法",
        "Question": "「ある条件のもとで、ターゲットとなる事象が発生する確率」をモデル化する手法は？",
        "Opt1": "判別分析",
        "Opt2": "ロジスティック回帰",
        "Opt3": "K-means法",
        "Opt4": "主成分分析",
        "Opt5": "アソシエーション分析",
        "Opt6": "異常検知",
        "Answer_Idx": 1,
        "Explanation": "ロジスティック回帰は「正である確率」を出力するため、確率モデルとして扱えます。",
        "Link": "https://example.com/logistic-prob"
    },
    {
        "ID": "ML-098",
        "Category": "3.機械学習の手法",
        "Question": "アンサンブル学習のバギングにおいて、元のデータから重複を許してランダムに抽出した標本を何というか？",
        "Opt1": "テストデータ",
        "Opt2": "検証データ",
        "Opt3": "ブートストラップ標本",
        "Opt4": "ホールドアウト標本",
        "Opt5": "外れ値",
        "Opt6": "ラベルデータ",
        "Answer_Idx": 2,
        "Explanation": "この標本を用いて複数のモデルを学習させるのがランダムフォレストの基本です。",
        "Link": "https://example.com/bootstrap-sample"
    },
    {
        "ID": "ML-099",
        "Category": "3.機械学習の手法",
        "Question": "SVMで「マージンの外側にデータが一つも存在しない」ように厳しく分類する設定を何というか？",
        "Opt1": "ソフトマージン",
        "Opt2": "ハードマージン",
        "Opt3": "ラージマージン",
        "Opt4": "スモールマージン",
        "Opt5": "クロスバリデーション",
        "Opt6": "正則化",
        "Answer_Idx": 1,
        "Explanation": "ハードマージンは外れ値に非常に弱く、実データでは過学習しやすいため注意が必要です。",
        "Link": "https://example.com/hard-margin"
    },
    {
        "ID": "ML-100",
        "Category": "3.機械学習の手法",
        "Question": "機械学習のプロジェクトにおいて、データの収集、加工、学習、評価を繰り返す標準的なプロセスは？",
        "Opt1": "SDLC",
        "Opt2": "CRISP-DM",
        "Opt3": "Waterfall",
        "Opt4": "Agile",
        "Opt5": "Scrum",
        "Opt6": "ISO9001",
        "Answer_Idx": 1,
        "Explanation": "ビジネス理解から共有まで、データマイニングの標準的な手順を定義したものです。",
        "Link": "https://example.com/crisp-dm"
    },
    {
        "ID": "ML-101",
        "Category": "3.機械学習の手法",
        "Question": "「あなたと好みが似ている他の人はこれも買っています」という推薦手法を何というか？",
        "Opt1": "内容ベースフィルタリング",
        "Opt2": "協調フィルタリング",
        "Opt3": "ハイブリッド型推薦",
        "Opt4": "ルールベース推薦",
        "Opt5": "ランダム推薦",
        "Opt6": "深層学習",
        "Answer_Idx": 1,
        "Explanation": "ユーザーの行動履歴（購買・評価）の類似性に基づいて推薦を行う手法です。",
        "Link": "https://example.com/cf-intro"
    },
    {
        "ID": "ML-102",
        "Category": "3.機械学習の手法",
        "Question": "アイテム自体の特徴（ジャンル、色、価格など）に基づいて、似たアイテムを推薦する手法は？",
        "Opt1": "ユーザーベース協調フィルタリング",
        "Opt2": "内容ベース（コンテンツベース）フィルタリング",
        "Opt3": "アイテムベース協調フィルタリング",
        "Opt4": "行列分解",
        "Opt5": "強化学習",
        "Opt6": "主成分分析",
        "Answer_Idx": 1,
        "Explanation": "過去に見た映画と「同じジャンル」を勧めるような手法です。新規ユーザーにも適用しやすい利点があります。",
        "Link": "https://example.com/cb-filtering"
    },
    {
        "ID": "ML-103",
        "Category": "3.機械学習の手法",
        "Question": "推薦システムにおいて、データが少なすぎて適切な推薦ができない初期状態の問題を何というか？",
        "Opt1": "次元の呪い",
        "Opt2": "コールドスタート問題",
        "Opt3": "ハルシネーション",
        "Opt4": "過学習",
        "Opt5": "勾配消失",
        "Opt6": "未学習",
        "Answer_Idx": 1,
        "Explanation": "新機能や新ユーザーに対し、履歴がないためレコメンドが機能しない状態を指します。",
        "Link": "https://example.com/cold-start"
    },
    {
        "ID": "ML-104",
        "Category": "3.機械学習の手法",
        "Question": "ユーザーとアイテムの巨大な評価行列を、より小さい二つの行列に分解して予測する手法は？",
        "Opt1": "主成分分析",
        "Opt2": "行列分解（マトリクス・ファクタライゼーション）",
        "Opt3": "クラスタリング",
        "Opt4": "ロジスティック回帰",
        "Opt5": "決定木",
        "Opt6": "正規化",
        "Answer_Idx": 1,
        "Explanation": "NetflixやAmazonなどのレコメンドで広く使われた手法です。",
        "Link": "https://example.com/matrix-fact"
    },
    {
        "ID": "ML-105",
        "Category": "3.機械学習の手法",
        "Question": "時間経過とともに変化するデータを扱う「時系列解析」において、周期的な変動成分を何というか？",
        "Opt1": "トレンド",
        "Opt2": "定常性",
        "Opt3": "季節性（季節変動）",
        "Opt4": "ノイズ",
        "Opt5": "自己相関",
        "Opt6": "外れ値",
        "Answer_Idx": 2,
        "Explanation": "売上の曜日変動や、季節による気温変化などが「季節性」に該当します。",
        "Link": "https://example.com/seasonality"
    },
    {
        "ID": "ML-106",
        "Category": "3.機械学習の手法",
        "Question": "時系列データの統計的性質（平均や分散）が、時間によらず一定であることを何というか？",
        "Opt1": "非定常性",
        "Opt2": "定常性",
        "Opt3": "自己相関",
        "Opt4": "ラグ",
        "Opt5": "移動平均",
        "Opt6": "平滑化",
        "Answer_Idx": 1,
        "Explanation": "多くの時系列モデル（ARIMA等）は、データが定常性を持つことを前提としています。",
        "Link": "https://example.com/stationarity"
    },
    {
        "ID": "ML-107",
        "Category": "3.機械学習の手法",
        "Question": "過去の自分自身の値と現在の値の相関関係を示す指標を何というか？",
        "Opt1": "相互相関",
        "Opt2": "自己相関",
        "Opt3": "寄与率",
        "Opt4": "決定係数",
        "Opt5": "偏差",
        "Opt6": "歪度",
        "Answer_Idx": 1,
        "Explanation": "「昨日の気温が高いと今日の気温も高い」というような関係を自己相関と呼びます。",
        "Link": "https://example.com/autocorrelation"
    },
    {
        "ID": "ML-108",
        "Category": "3.機械学習の手法",
        "Question": "時系列モデルの一つで、過去の値の線形結合で現在の値を予測するモデルはどれか？",
        "Opt1": "MAモデル",
        "Opt2": "ARモデル（自己回帰モデル）",
        "Opt3": "ARMAモデル",
        "Opt4": "CNN",
        "Opt5": "SVM",
        "Opt6": "K-means",
        "Answer_Idx": 1,
        "Explanation": "Autoregressive（AR）モデル。直近の数日分のデータから未来を予測します。",
        "Link": "https://example.com/ar-model"
    },
    {
        "ID": "ML-109",
        "Category": "3.機械学習の手法",
        "Question": "少量のラベル付きデータと、大量のラベルなしデータを組み合わせて学習する手法は？",
        "Opt1": "教師なし学習",
        "Opt2": "強化学習",
        "Opt3": "半教師あり学習",
        "Opt4": "自己教師あり学習",
        "Opt5": "転移学習",
        "Opt6": "能動学習",
        "Answer_Idx": 2,
        "Explanation": "コストの高いラベル付け作業を節約しつつ、大量のデータから構造を学ぶ手法です。",
        "Link": "https://example.com/semi-supervised"
    },
    {
        "ID": "ML-110",
        "Category": "3.機械学習の手法",
        "Question": "ラベルなしデータから、データの一部を隠してそれを予測させることで、自律的に特徴を学ぶ手法は？",
        "Opt1": "半教師あり学習",
        "Opt2": "自己教師あり学習",
        "Opt3": "強化学習",
        "Opt4": "教師あり学習",
        "Opt5": "次元圧縮",
        "Opt6": "アンサンブル学習",
        "Answer_Idx": 1,
        "Explanation": "Self-supervised learning。BERTやGPTなどの事前学習で使われる現代の主流手法です。",
        "Link": "https://example.com/self-supervised-learning"
    },
    {
        "ID": "ML-111",
        "Category": "3.機械学習の手法",
        "Question": "学習データが不足している場合、既存の訓練済みモデルを別のタスクに再利用する手法を何というか？",
        "Opt1": "蒸留",
        "Opt2": "転移学習",
        "Opt3": "メタ学習",
        "Opt4": "強化学習",
        "Opt5": "バギング",
        "Opt6": "ブースティング",
        "Answer_Idx": 1,
        "Explanation": "「一般知識」を持つ大規模モデルを「特定タスク」に微調整（ファインチューニング）します。",
        "Link": "https://example.com/transfer-learning"
    },
    {
        "ID": "ML-112",
        "Category": "3.機械学習の手法",
        "Question": "巨大な親モデル（教師）の知識を、より軽量な子モデル（生徒）に継承させる手法は？",
        "Opt1": "転移学習",
        "Opt2": "蒸留（ディスティレーション）",
        "Opt3": "剪定",
        "Opt4": "量子化",
        "Opt5": "データの水増し",
        "Opt6": "オーバーサンプリング",
        "Answer_Idx": 1,
        "Explanation": "軽量化しつつ精度を維持できるため、モバイル端末へのAI実装などで重要です。",
        "Link": "https://example.com/distillation"
    },
    {
        "ID": "ML-113",
        "Category": "3.機械学習の手法",
        "Question": "モデルが予測に自信がないデータを選び出し、人間にラベル付けを依頼して効率的に学習する手法は？",
        "Opt1": "半教師あり学習",
        "Opt2": "能動学習（アクティブラーニング）",
        "Opt3": "転移学習",
        "Opt4": "自己学習",
        "Opt5": "深層学習",
        "Opt6": "次元削減",
        "Answer_Idx": 1,
        "Explanation": "全データにラベルを貼る手間を省き、重要なデータのみを人間に確認させます。",
        "Link": "https://example.com/active-learning"
    },
    {
        "ID": "ML-114",
        "Category": "3.機械学習の手法",
        "Question": "画像データに対して「反転・回転・拡大」などを行い、疑似的にデータを増やす手法は？",
        "Opt1": "正規化",
        "Opt2": "標準化",
        "Opt3": "データオーギュメンテーション（データの水増し）",
        "Opt4": "次元圧縮",
        "Opt5": "正則化",
        "Opt6": "スムージング",
        "Answer_Idx": 2,
        "Explanation": "少ない画像枚数でもモデルを頑健にし、過学習を防ぐ効果があります。",
        "Link": "https://example.com/data-aug"
    },
    {
        "ID": "ML-115",
        "Category": "3.機械学習の手法",
        "Question": "推薦システムで「特定の人気アイテムばかりが推薦され、多様性が失われる」現象を何というか？",
        "Opt1": "フィルタバブル（エコーチェンバー）",
        "Opt2": "過学習",
        "Opt3": "コールドスタート",
        "Opt4": "未学習",
        "Opt5": "ハルシネーション",
        "Opt6": "欠損値",
        "Answer_Idx": 0,
        "Explanation": "ユーザーが興味のある情報だけに囲まれ、新しい発見ができなくなる状態です。",
        "Link": "https://example.com/filter-bubble"
    },
    {
        "ID": "ML-116",
        "Category": "3.機械学習の手法",
        "Question": "時系列解析において、長期的な上昇または下落の傾向を何というか？",
        "Opt1": "季節性",
        "Opt2": "ノイズ",
        "Opt3": "トレンド",
        "Opt4": "周期性",
        "Opt5": "定常性",
        "Opt6": "自己相関",
        "Answer_Idx": 2,
        "Explanation": "数ヶ月〜数年単位で続く緩やかな変化の方向性のことです。",
        "Link": "https://example.com/trend"
    },
    {
        "ID": "ML-117",
        "Category": "3.機械学習の手法",
        "Question": "時系列解析で「過去の予測誤差」を現在の値に反映させるモデルを何というか？",
        "Opt1": "ARモデル",
        "Opt2": "MAモデル（移動平均モデル）",
        "Opt3": "ARMAモデル",
        "Opt4": "ARIMAモデル",
        "Opt5": "指数平滑法",
        "Opt6": "状態空間モデル",
        "Answer_Idx": 1,
        "Explanation": "Moving Average（MA）モデル。誤差の揺り戻しを考慮します。",
        "Link": "https://example.com/ma-model"
    },
    {
        "ID": "ML-118",
        "Category": "3.機械学習の手法",
        "Question": "非定常なデータに対し「差分（階差）」をとって定常化してから適用する時系列モデルは？",
        "Opt1": "ARモデル",
        "Opt2": "MAモデル",
        "Opt3": "ARMAモデル",
        "Opt4": "ARIMAモデル",
        "Opt5": "SVM",
        "Opt6": "PCA",
        "Answer_Idx": 3,
        "Explanation": "ARモデル、MAモデル、および「和分（Integration）」を組み合わせた高度なモデルです。",
        "Link": "https://example.com/arima"
    },
    {
        "ID": "ML-119",
        "Category": "3.機械学習の手法",
        "Question": "異なる性質を持つ複数の特徴量を組み合わせて、新しい特徴量を作り出す工程を何というか？",
        "Opt1": "データクレンジング",
        "Opt2": "特徴量エンジニアリング",
        "Opt3": "アノテーション",
        "Opt4": "スケーリング",
        "Opt5": "標準化",
        "Opt6": "正規化",
        "Answer_Idx": 1,
        "Explanation": "「ドメイン知識」を用いて、AIが学習しやすい形にデータを加工する重要なステップです。",
        "Link": "https://example.com/feature-engineering"
    },
    {
        "ID": "ML-120",
        "Category": "3.機械学習の手法",
        "Question": "「ある日の気温の変化」と「その日のアイスの売上」のように、同時刻の二つの系列の相関は？",
        "Opt1": "自己相関",
        "Opt2": "相互相関",
        "Opt3": "階差",
        "Opt4": "トレンド",
        "Opt5": "ノイズ",
        "Opt6": "バイアス",
        "Answer_Idx": 1,
        "Explanation": "異なる二つの時系列データの関連性を測る指標です。",
        "Link": "https://example.com/cross-correlation"
    },
    {
        "ID": "ML-121",
        "Category": "3.機械学習の手法",
        "Question": "2点間の距離を計算する際、各成分の差の絶対値を合計したものを何距離というか？",
        "Opt1": "ユークリッド距離",
        "Opt2": "マンハッタン距離",
        "Opt3": "チェビシェフ距離",
        "Opt4": "マハラノビス距離",
        "Opt5": "コサイン類似度",
        "Opt6": "ハミング距離",
        "Answer_Idx": 1,
        "Explanation": "L1距離とも呼ばれます。碁盤目状の道路を移動するような距離の測り方です。",
        "Link": "https://example.com/manhattan-dist"
    },
    {
        "ID": "ML-122",
        "Category": "3.機械学習の手法",
        "Question": "2点間の直線距離（各成分の差の2乗和の平方根）を計算する手法を何というか？",
        "Opt1": "ユークリッド距離",
        "Opt2": "マンハッタン距離",
        "Opt3": "ミンコフスキー距離",
        "Opt4": "マハラノビス距離",
        "Opt5": "ハミング距離",
        "Opt6": "ジャカード指数",
        "Answer_Idx": 0,
        "Explanation": "L2距離とも呼ばれ、物理的な直線距離に相当する最も一般的な指標です。",
        "Link": "https://example.com/euclidean-dist"
    },
    {
        "ID": "ML-123",
        "Category": "3.機械学習の手法",
        "Question": "データの相関（分散）を考慮し、中心からの隔たりを統計的に測る距離の指標はどれか？",
        "Opt1": "ユークリッド距離",
        "Opt2": "マンハッタン距離",
        "Opt3": "マハラノビス距離",
        "Opt4": "ハミング距離",
        "Opt5": "コサイン類似度",
        "Opt6": "L1ノルム",
        "Answer_Idx": 2,
        "Explanation": "異常検知（ホテリングの理論など）で、外れ値を判定する際によく使われます。",
        "Link": "https://example.com/mahalanobis"
    },
    {
        "ID": "ML-124",
        "Category": "3.機械学習の手法",
        "Question": "2つのベクトルのなす角の余弦（cos）を用いて類似度を測る、文書検索などで多用される指標は？",
        "Opt1": "ユークリッド距離",
        "Opt2": "マンハッタン距離",
        "Opt3": "コサイン類似度",
        "Opt4": "ハミング距離",
        "Opt5": "ピアソンの相関係数",
        "Opt6": "ジニ係数",
        "Answer_Idx": 2,
        "Explanation": "ベクトルの長さではなく「向き」の近さを重視するため、単語の分散表現などで有効です。",
        "Link": "https://example.com/cosine-similarity"
    },
    {
        "ID": "ML-125",
        "Category": "3.機械学習の手法",
        "Question": "「ブラックボックス化したAI」に対し、特定の予測結果の理由を局所的に近似して説明する手法は？",
        "Opt1": "SHAP",
        "Opt2": "LIME",
        "Opt3": "PCA",
        "Opt4": "t-SNE",
        "Opt5": "SVM",
        "Opt6": "CNN",
        "Answer_Idx": 1,
        "Explanation": "Local Interpretable Model-agnostic Explanationsの略。個別のデータ点周辺で単純なモデルを作り、根拠を可視化します。",
        "Link": "https://example.com/lime-ai"
    },
    {
        "ID": "ML-126",
        "Category": "3.機械学習の手法",
        "Question": "協力ゲーム理論の概念を応用し、各特徴量が予測値に与えた貢献度を公平に算出する手法は？",
        "Opt1": "LIME",
        "Opt2": "SHAP",
        "Opt3": "Grad-CAM",
        "Opt4": "決定木",
        "Opt5": "ジニ不純度",
        "Opt6": "寄与率",
        "Answer_Idx": 1,
        "Explanation": "SHapley Additive exPlanationsの略。LIMEよりも計算負荷は高いですが、理論的基盤が強固です。",
        "Link": "https://example.com/shap-ai"
    },
    {
        "ID": "ML-127",
        "Category": "3.機械学習の手法",
        "Question": "データの特徴空間において、異なるクラスを分ける境界線のことを何と呼ぶか？",
        "Opt1": "サポートベクトル",
        "Opt2": "マージン",
        "Opt3": "決定境界",
        "Opt4": "セントロイド",
        "Opt5": "デンドログラム",
        "Opt6": "エッジ",
        "Answer_Idx": 2,
        "Explanation": "モデルが「ここから先はクラスA、ここからはクラスB」と判定するラインのことです。",
        "Link": "https://example.com/decision-boundary"
    },
    {
        "ID": "ML-128",
        "Category": "3.機械学習の手法",
        "Question": "SVMで線形分離不可能な問題を解く際、暗黙的に高次元へ写像を行う関数を何というか？",
        "Opt1": "損失関数",
        "Opt2": "カーネル関数",
        "Opt3": "活性化関数",
        "Opt4": "シグモイド関数",
        "Opt5": "基底関数",
        "Opt6": "ハッシュ関数",
        "Answer_Idx": 1,
        "Explanation": "ガウスカーネル（RBFカーネル）や多項式カーネルなどが代表的です。",
        "Link": "https://example.com/kernel-function"
    },
    {
        "ID": "ML-129",
        "Category": "3.機械学習の手法",
        "Question": "線形回帰において、パラメータを推定するために「尤度（ゆうど）」を最大化する手法を何というか？",
        "Opt1": "最小二乗法",
        "Opt2": "最尤推定法",
        "Opt3": "ベイズ推定",
        "Opt4": "勾配降下法",
        "Opt5": "主成分分析",
        "Opt6": "正則化",
        "Answer_Idx": 1,
        "Explanation": "観測されたデータが得られる確率（尤度）を最大にするパラメータを探す統計的手法です。",
        "Link": "https://example.com/maximum-likelihood"
    },
    {
        "ID": "ML-130",
        "Category": "3.機械学習の手法",
        "Question": "主成分分析（PCA）において、新しい次元（主成分）を選ぶ際の基準として正しいものはどれか？",
        "Opt1": "平均の最大化",
        "Opt2": "分散の最大化",
        "Opt3": "誤差の最小化",
        "Opt4": "クラス間距離の最大化",
        "Opt5": "不純度の最小化",
        "Opt6": "相関の最大化",
        "Answer_Idx": 1,
        "Explanation": "データの情報を最も保持するために、データのばらつき（分散）が最大になる軸を探します。",
        "Link": "https://example.com/pca-variance"
    },
    {
        "ID": "ML-131",
        "Category": "3.機械学習の手法",
        "Question": "クラスタリングにおいて、事前にクラスタ数Kを決めず、樹形図を作る手法はどれか？",
        "Opt1": "K-means法",
        "Opt2": "階層クラスタリング",
        "Opt3": "主成分分析",
        "Opt4": "ロジスティック回帰",
        "Opt5": "ガウス混合モデル",
        "Opt6": "DB SCAN",
        "Answer_Idx": 1,
        "Explanation": "最短距離法や群平均法などの「連結法」を用いてクラスタを結合していきます。",
        "Link": "https://example.com/hierarchical"
    },
    {
        "ID": "ML-132",
        "Category": "3.機械学習の手法",
        "Question": "階層クラスタリングにおいて、2つのクラスタ間で「最も遠いデータ点同士の距離」をクラス間距離とする手法は？",
        "Opt1": "最短距離法",
        "Opt2": "最長距離法（完全連結法）",
        "Opt3": "群平均法",
        "Opt4": "ウォード法",
        "Opt5": "重心法",
        "Opt6": "メジアン法",
        "Answer_Idx": 1,
        "Explanation": "最長距離法は、クラスタをコンパクトにまとめる傾向があります。",
        "Link": "https://example.com/complete-linkage"
    },
    {
        "ID": "ML-133",
        "Category": "3.機械学習の手法",
        "Question": "階層クラスタリングにおいて、クラスタ内の分散の増加を最小にするように結合する手法は？",
        "Opt1": "最短距離法",
        "Opt2": "最長距離法",
        "Opt3": "群平均法",
        "Opt4": "ウォード法",
        "Opt5": "重心法",
        "Opt6": "メジアン法",
        "Answer_Idx": 3,
        "Explanation": "ウォード法は計算量は多いですが、分類感度が良く、バランスの良いクラスタが得られやすいです。",
        "Link": "https://example.com/ward-method"
    },
    {
        "ID": "ML-134",
        "Category": "3.機械学習の手法",
        "Question": "K-means法において、初期値の依存性を解消するために改良されたアルゴリズムはどれか？",
        "Opt1": "K-means++",
        "Opt2": "K-medoids",
        "Opt3": "ISODATA",
        "Opt4": "EMアルゴリズム",
        "Opt5": "決定木",
        "Opt6": "SVM",
        "Answer_Idx": 0,
        "Explanation": "初期の重心（セントロイド）を互いに離れた位置に選ぶことで、収束を安定させます。",
        "Link": "https://example.com/kmeans-plus-plus"
    },
    {
        "ID": "ML-135",
        "Category": "3.機械学習の手法",
        "Question": "密度に着目したクラスタリング手法で、ノイズ（外れ値）を自動的に検出できる手法は？",
        "Opt1": "K-means法",
        "Opt2": "主成分分析",
        "Opt3": "DBSCAN",
        "Opt4": "階層クラスタリング",
        "Opt5": "ロジスティック回帰",
        "Opt6": "単回帰分析",
        "Answer_Idx": 2,
        "Explanation": "半径ε以内に最小点数MinPtsが含まれるか否かでクラスタを形成します。形状が歪なクラスタも抽出可能です。",
        "Link": "https://example.com/dbscan"
    },
    {
        "ID": "ML-136",
        "Category": "3.機械学習の手法",
        "Question": "モデルの予測結果（出力）の確信度を調整し、実際の正解確率に近づける作業を何というか？",
        "Opt1": "正則化",
        "Opt2": "標準化",
        "Opt3": "キャリブレーション",
        "Opt4": "スケーリング",
        "Opt5": "二値化",
        "Opt6": "量子化",
        "Answer_Idx": 2,
        "Explanation": "例えば「AIが80%の確率で正解と予測したものは、実際に80%の頻度で当たっている」状態に調整します。",
        "Link": "https://example.com/calibration"
    },
    {
        "ID": "ML-137",
        "Category": "3.機械学習の手法",
        "Question": "教師あり学習の分類問題において、学習データ内のクラス比率を維持したまま交差検証を行う手法は？",
        "Opt1": "ホールドアウト法",
        "Opt2": "層化K-分割交差検証（Stratified K-fold）",
        "Opt3": "リーブワンアウト法",
        "Opt4": "ブートストラップ法",
        "Opt5": "ランダムサーチ",
        "Opt6": "アンサンブル学習",
        "Answer_Idx": 1,
        "Explanation": "各分割（Fold）内のラベル比率を偏らせないため、より安定した評価が可能です。",
        "Link": "https://example.com/stratified-kfold"
    },
    {
        "ID": "ML-138",
        "Category": "3.機械学習の手法",
        "Question": "線形回帰の予測において、平均的なズレを示す「バイアス」と、モデルの不安定さを示す「バリアンス」の合計を最小化する問題を何というか？",
        "Opt1": "過学習の回避",
        "Opt2": "バイアス・バリアンス分解",
        "Opt3": "正則化問題",
        "Opt4": "次元の呪い",
        "Opt5": "最適化問題",
        "Opt6": "二律背反",
        "Answer_Idx": 1,
        "Explanation": "バイアスを下げるとバリアンスが上がり、その逆も然りであるため、トータル誤差を最小化するバランスが必要です。",
        "Link": "https://example.com/bias-variance-decomp"
    },
    {
        "ID": "ML-139",
        "Category": "3.機械学習の手法",
        "Question": "高次元データを可視化する際、PCAよりも「局所的な位置関係」を保存することに優れた手法は？",
        "Opt1": "t-SNE",
        "Opt2": "Lasso",
        "Opt3": "Ridge",
        "Opt4": "K-means",
        "Opt5": "決定木",
        "Opt6": "線形判別分析",
        "Answer_Idx": 0,
        "Explanation": "非線形な構造を持つデータのクラスターを2次元で見やすく配置するのに適しています。",
        "Link": "https://example.com/tsne-viz"
    },
    {
        "ID": "ML-140",
        "Category": "3.機械学習の手法",
        "Question": "機械学習において、データの値を変換せずにそのまま使えるデータを「生データ」と呼ぶが、加工後のデータを何と呼ぶか？",
        "Opt1": "構造化データ",
        "Opt2": "特徴量（フィーチャー）",
        "Opt3": "メタデータ",
        "Opt4": "ラベル",
        "Opt5": "パラメータ",
        "Opt6": "バイアス",
        "Answer_Idx": 1,
        "Explanation": "アルゴリズムが学習しやすいように抽出・変換された変数のことです。",
        "Link": "https://example.com/feature-engineering-def"
    },
    {
        "ID": "ML-141",
        "Category": "3.機械学習の手法",
        "Question": "自然言語処理において、文章を最小単位である単語や品詞に分割する作業を何というか？",
        "Opt1": "構文解析",
        "Opt2": "形態素解析",
        "Opt3": "意味解析",
        "Opt4": "文脈解析",
        "Opt5": "固有表現抽出",
        "Opt6": "正規化",
        "Answer_Idx": 1,
        "Explanation": "日本語のような分かち書きのない言語では、機械学習の最初期ステップとして非常に重要です。",
        "Link": "https://example.com/morphological-analysis"
    },
    {
        "ID": "ML-142",
        "Category": "3.機械学習の手法",
        "Question": "文章中の単語を、その出現頻度に基づいたベクトルとして表現する手法を何というか？",
        "Opt1": "Word2Vec",
        "Opt2": "Bag-of-Words (BoW)",
        "Opt3": "TF-IDF",
        "Opt4": "N-gram",
        "Opt5": "BERT",
        "Opt6": "RNN",
        "Answer_Idx": 1,
        "Explanation": "単語の並び順を無視し、どの単語が何回出たかのみに注目する手法です。",
        "Link": "https://example.com/bag-of-words"
    },
    {
        "ID": "ML-143",
        "Category": "3.機械学習の手法",
        "Question": "テキストデータを「連続するN個の要素（単語や文字）」の単位で分割する手法を何というか？",
        "Opt1": "N-gram",
        "Opt2": "Skip-gram",
        "Opt3": "TF-IDF",
        "Opt4": "BoW",
        "Opt5": "Tokenization",
        "Opt6": "Stemming",
        "Answer_Idx": 0,
        "Explanation": "N=2なら「バイグラム」、N=3なら「トライグラム」と呼ばれます。",
        "Link": "https://example.com/n-gram"
    },
    {
        "ID": "ML-144",
        "Category": "3.機械学習の手法",
        "Question": "「ある状態から別の状態へ遷移する確率」に基づき、観測データから背後の状態を推定するモデルは？",
        "Opt1": "隠れマルコフモデル (HMM)",
        "Opt2": "決定木",
        "Opt3": "SVM",
        "Opt4": "K-means",
        "Opt5": "ロジスティック回帰",
        "Opt6": "主成分分析",
        "Answer_Idx": 0,
        "Explanation": "音声認識や形態素解析の古典的なアルゴリズムとして広く使われてきました。",
        "Link": "https://example.com/hmm"
    },
    {
        "ID": "ML-145",
        "Category": "3.機械学習の手法",
        "Question": "強化学習において、ある状態から行動を選択し、得られた「実際の報酬」を用いて価値を更新する手法は？",
        "Opt1": "TD学習",
        "Opt2": "モンテカルロ法",
        "Opt3": "ダイナミックプログラミング",
        "Opt4": "Q学習",
        "Opt5": "SARSA",
        "Opt6": "バンディット",
        "Answer_Idx": 0,
        "Explanation": "Temporal Difference（時間差）学習。エピソードの終了を待たずに学習を進められます。",
        "Link": "https://example.com/td-learning"
    },
    {
        "ID": "ML-146",
        "Category": "3.機械学習の手法",
        "Question": "強化学習において、エピソードが完全に終了した後に、一連の報酬を振り返って学習する手法は？",
        "Opt1": "TD学習",
        "Opt2": "モンテカルロ法",
        "Opt3": "Q学習",
        "Opt4": "SARSA",
        "Opt5": "貪欲法",
        "Opt6": "A*アルゴリズム",
        "Answer_Idx": 1,
        "Explanation": "一連の流れが完了してから平均報酬を計算するため、分散が大きくなりやすい特徴があります。",
        "Link": "https://example.com/monte-carlo-rl"
    },
    {
        "ID": "ML-147",
        "Category": "3.機械学習の手法",
        "Question": "強化学習アルゴリズム「SARSA」と「Q学習」の最大の違いはどれか？",
        "Opt1": "SARSAは教師なし学習である",
        "Opt2": "Q学習は方策オフ型でSARSAは方策オン型である",
        "Opt3": "SARSAは連続値しか扱えない",
        "Opt4": "Q学習は報酬を必要としない",
        "Opt5": "違いはない",
        "Opt6": "SARSAは深層学習である",
        "Answer_Idx": 1,
        "Explanation": "Q学習は「次に取る可能性のある最大価値」を使い、SARSAは「実際に次に取った行動」を使って更新します。",
        "Link": "https://example.com/q-vs-sarsa"
    },
    {
        "ID": "ML-148",
        "Category": "3.機械学習の手法",
        "Question": "テキストデータから不要な記号を除去したり、表記ゆれを修正したりする前処理を何というか？",
        "Opt1": "アノテーション",
        "Opt2": "テキスト正規化",
        "Opt3": "次元削減",
        "Opt4": "クラスタリング",
        "Opt5": "特徴量選択",
        "Opt6": "正則化",
        "Answer_Idx": 1,
        "Explanation": "「コンピュータ」と「コンピュータ」を同一視させるなどの処理です。",
        "Link": "https://example.com/text-normalization"
    },
    {
        "ID": "ML-149",
        "Category": "3.機械学習の手法",
        "Question": "「単語 A は単語 B に似ている」といった意味的・文法的な性質を低次元ベクトルで表現する技術は？",
        "Opt1": "単語の分散表現（埋め込み）",
        "Opt2": "TF-IDF",
        "Opt3": "N-gram",
        "Opt4": "形態素解析",
        "Opt5": "決定木",
        "Opt6": "SVM",
        "Answer_Idx": 0,
        "Explanation": "Word2Vecなどが代表例で、単語同士の演算（王様-男+女=女王）が可能になります。",
        "Link": "https://example.com/word-embedding"
    },
    {
        "ID": "ML-150",
        "Category": "3.機械学習の手法",
        "Question": "多数の文書から、それらがどのような「トピック」で構成されているかを推定する教師なし学習は？",
        "Opt1": "感情分析",
        "Opt2": "トピックモデル (LDA等)",
        "Opt3": "固有表現抽出",
        "Opt4": "機械翻訳",
        "Opt5": "要約",
        "Opt6": "文書分類",
        "Answer_Idx": 1,
        "Explanation": "潜在的ディレクトリ割当（LDA）などが、記事の自動分類などで使われます。",
        "Link": "https://example.com/topic-model"
    },
    {
        "ID": "ML-151",
        "Category": "3.機械学習の手法",
        "Question": "分類問題において、各クラスのデータの重要度を変えて学習させる手法を何というか？",
        "Opt1": "コスト考慮型学習",
        "Opt2": "強化学習",
        "Opt3": "転移学習",
        "Opt4": "能動学習",
        "Opt5": "半教師あり学習",
        "Opt6": "自己学習",
        "Answer_Idx": 0,
        "Explanation": "誤分類のコストがクラスごとに異なる場合（例：偽陰性が致命的な医療診断）に有効です。",
        "Link": "https://example.com/cost-sensitive"
    },
    {
        "ID": "ML-152",
        "Category": "3.機械学習の手法",
        "Question": "モデルのハイパーパラメータを、以前の試行結果（スコア）に基づいて確率的に絞り込む探索手法は？",
        "Opt1": "グリッドサーチ",
        "Opt2": "ランダムサーチ",
        "Opt3": "ベイズ最適化",
        "Opt4": "全探索",
        "Opt5": "二分探索",
        "Opt6": "遺伝的アルゴリズム",
        "Answer_Idx": 2,
        "Explanation": "効率的に最適な組み合わせを見つけるための手法。Optunaなどのライブラリが有名です。",
        "Link": "https://example.com/bayesian-optimization"
    },
    {
        "ID": "ML-153",
        "Category": "3.機械学習の手法",
        "Question": "バイアスを減らすために複雑なモデルを作るのではなく、単純なモデルを順次追加して誤差を修正する手法は？",
        "Opt1": "バギング",
        "Opt2": "ブースティング",
        "Opt3": "スタッキング",
        "Opt4": "正規化",
        "Opt5": "標準化",
        "Opt6": "正則化",
        "Answer_Idx": 1,
        "Explanation": "「弱学習器」を組み合わせて「強学習器」を作る考え方です。",
        "Link": "https://example.com/boosting-recap"
    },
    {
        "ID": "ML-154",
        "Category": "3.機械学習の手法",
        "Question": "SVMにおいて、高次元空間での計算を省略し、データ間の類似度（内積）だけで計算を完結させるのは？",
        "Opt1": "カーネルトリック",
        "Opt2": "プーリング",
        "Opt3": "畳み込み",
        "Opt4": "回帰分析",
        "Opt5": "決定木",
        "Opt6": "平均化",
        "Answer_Idx": 0,
        "Explanation": "第139問の復習を兼ねた重要トピックです。",
        "Link": "https://example.com/kernel-trick-recap"
    },
    {
        "ID": "ML-155",
        "Category": "3.機械学習の手法",
        "Question": "二つの変数の関係において、相関関係があるからといって必ずしも成立しない関係は？",
        "Opt1": "因果関係",
        "Opt2": "線形関係",
        "Opt3": "非線形関係",
        "Opt4": "逆相関",
        "Opt5": "共分散",
        "Opt6": "決定係数",
        "Answer_Idx": 0,
        "Explanation": "「AとBに相関がある」ことと「Aが原因でBが起きた」ことは別問題です。",
        "Link": "https://example.com/causality"
    },
    {
        "ID": "ML-156",
        "Category": "3.機械学習の手法",
        "Question": "画像認識において、色、形、テクスチャなどの情報を数値化したものを何というか？",
        "Opt1": "ラベル",
        "Opt2": "特徴量",
        "Opt3": "パラメータ",
        "Opt4": "バイアス",
        "Opt5": "重み",
        "Opt6": "損失",
        "Answer_Idx": 1,
        "Explanation": "ディープラーニング以前は、これらを人間が手動で定義（ハンドクラフト）していました。",
        "Link": "https://example.com/visual-features"
    },
    {
        "ID": "ML-157",
        "Category": "3.機械学習の手法",
        "Question": "回帰問題において、外れ値に引きずられないように「誤差の絶対値」を最小化する手法は？",
        "Opt1": "最小二乗法",
        "Opt2": "最小絶対値法 (LAD)",
        "Opt3": "ロジスティック回帰",
        "Opt4": "主成分分析",
        "Opt5": "SVM",
        "Opt6": "K-means",
        "Answer_Idx": 1,
        "Explanation": "MSE（二乗誤差）ではなくMAE（絶対誤差）を最小化する考え方です。",
        "Link": "https://example.com/lad-regression"
    },
    {
        "ID": "ML-158",
        "Category": "3.機械学習の手法",
        "Question": "「ある条件が整えば必ず結果が起きる」という論理的な推論ではなく、確率的な尤もらしさで判断する推論は？",
        "Opt1": "決定論的推論",
        "Opt2": "統計的推論",
        "Opt3": "演繹的推論",
        "Opt4": "三段論法",
        "Opt5": "帰納的推論",
        "Opt6": "アブダクション",
        "Answer_Idx": 1,
        "Explanation": "現在の機械学習のほとんどはこの統計的・確率的アプローチに基づいています。",
        "Link": "https://example.com/statistical-inference"
    },
    {
        "ID": "ML-159",
        "Category": "3.機械学習の手法",
        "Question": "複数の特徴量を組み合わせて一つの指標にまとめる際、各特徴量の「重要度」に応じて重みを変える平均は？",
        "Opt1": "算術平均",
        "Opt2": "加重平均 (重み付き平均)",
        "Opt3": "調和平均",
        "Opt4": "幾何平均",
        "Opt5": "移動平均",
        "Opt6": "トリム平均",
        "Answer_Idx": 1,
        "Explanation": "重要度の高いデータに大きな影響力を持たせる計算方法です。",
        "Link": "https://example.com/weighted-average"
    },
    {
        "ID": "ML-160",
        "Category": "3.機械学習の手法",
        "Question": "機械学習において、モデルの学習がうまく進まず、損失関数が最小値にたどり着かない現象を何というか？",
        "Opt1": "収束",
        "Opt2": "発散（未学習）",
        "Opt3": "過学習",
        "Opt4": "勾配消失",
        "Opt5": "次元の呪い",
        "Opt6": "シンギュラリティ",
        "Answer_Idx": 1,
        "Explanation": "学習率の設定が不適切だったり、データに規則性がない場合に起こります。",
        "Link": "https://example.com/non-convergence"
    },
    {
        "ID": "ML-161",
        "Category": "3.機械学習の手法",
        "Question": "決定木アルゴリズムにおいて、分類と回帰の両方に対応し、二分木を作成する代表的な手法は？",
        "Opt1": "ID3",
        "Opt2": "C4.5",
        "Opt3": "CART",
        "Opt4": "C5.0",
        "Opt5": "CHAID",
        "Opt6": "Random Forest",
        "Answer_Idx": 2,
        "Explanation": "Classification and Regression Trees (CART) は、ジニ係数や二乗誤差を用いて木を作成します。",
        "Link": "https://example.com/cart-algo"
    },
    {
        "ID": "ML-162",
        "Category": "3.機械学習の手法",
        "Question": "決定木のノード分割において、情報利得を計算する際に用いられる「乱雑さ」を示す指標は？",
        "Opt1": "相関係数",
        "Opt2": "エントロピー",
        "Opt3": "標準偏差",
        "Opt4": "決定係数",
        "Opt5": "累積寄与率",
        "Opt6": "リフト値",
        "Answer_Idx": 1,
        "Explanation": "エントロピーが減少するように分割を行うことで、データの純度を高めます。",
        "Link": "https://example.com/entropy-tree"
    },
    {
        "ID": "ML-163",
        "Category": "3.機械学習の手法",
        "Question": "不均衡データの評価において、正解率よりも信頼性が高い、全セル（TP/TN/FP/FN）を考慮した指標は？",
        "Opt1": "F1スコア",
        "Opt2": "マシューズ相関係数（MCC）",
        "Opt3": "適合率",
        "Opt4": "再現率",
        "Opt5": "特異度",
        "Opt6": "AUC",
        "Answer_Idx": 1,
        "Explanation": "MCCは-1から+1の値をとり、不均衡なデータセットでもモデルの質を正しく評価できます。",
        "Link": "https://example.com/mcc-score"
    },
    {
        "ID": "ML-164",
        "Category": "3.機械学習の手法",
        "Question": "モデルの評価指標において、負のクラスを正しく負と予測できた割合を何というか？",
        "Opt1": "適合率",
        "Opt2": "再現率",
        "Opt3": "特異度（Specificity）",
        "Opt4": "F値",
        "Opt5": "正解率",
        "Opt6": "AUC",
        "Answer_Idx": 2,
        "Explanation": "再現率が「正」に注目するのに対し、特異度は「負」の正解率に注目します。",
        "Link": "https://example.com/specificity"
    },
    {
        "ID": "ML-165",
        "Category": "3.機械学習の手法",
        "Question": "データセットの各特徴量を、最小値0、最大値1にする変換（Min-Max Scaling）が適切なケースは？",
        "Opt1": "外れ値が非常に多い場合",
        "Opt2": "データの分布が正規分布であることがわかっている場合",
        "Opt3": "データの範囲を一定に揃えたい場合",
        "Opt4": "パラメータが負の値を期待する場合",
        "Opt5": "クラスタリングを行わない場合",
        "Opt6": "標準化が使えない場合",
        "Answer_Idx": 2,
        "Explanation": "画像処理（画素値を0-1にする）などで多用されますが、外れ値に弱い欠点があります。",
        "Link": "https://example.com/min-max-scaling"
    },
    {
        "ID": "ML-166",
        "Category": "3.機械学習の手法",
        "Question": "サポートベクターマシンにおいて、線形分離できないデータを高次元空間へ写像する際に「計算コスト」を劇的に下げる工夫は？",
        "Opt1": "次元削減",
        "Opt2": "カーネルトリック",
        "Opt3": "勾配降下法",
        "Opt4": "主成分分析",
        "Opt5": "正則化",
        "Opt6": "標準化",
        "Answer_Idx": 1,
        "Explanation": "高次元のベクトル計算を直接行わず、元の空間の内積として計算する画期的な手法です。",
        "Link": "https://example.com/kernel-trick-detail"
    },
    {
        "ID": "ML-167",
        "Category": "3.機械学習の手法",
        "Question": "回帰分析において、説明変数の数が増えると「決定係数」が必ず増加してしまう問題を解決した指標は？",
        "Opt1": "自由度調整済み決定係数",
        "Opt2": "相関係数",
        "Opt3": "MSE",
        "Opt4": "RMSE",
        "Opt5": "MAE",
        "Opt6": "F値",
        "Answer_Idx": 0,
        "Explanation": "不必要な変数を追加しても値が上がりにくいようにペナルティを課した決定係数です。",
        "Link": "https://example.com/adj-r2"
    },
    {
        "ID": "ML-168",
        "Category": "3.機械学習の手法",
        "Question": "「直線一本で2つのクラスを完全に分けることができる」状態を何と呼ぶか？",
        "Opt1": "非線形",
        "Opt2": "線形分離可能",
        "Opt3": "次元圧縮済み",
        "Opt4": "定常性",
        "Opt5": "収束",
        "Opt6": "正規化",
        "Answer_Idx": 1,
        "Explanation": "単純なパーセプトロンで解ける問題の最低条件です。",
        "Link": "https://example.com/linearly-separable"
    },
    {
        "ID": "ML-169",
        "Category": "3.機械学習の手法",
        "Question": "「XOR問題」のように、直線一本では分けられない問題を解くために必要な構造は？",
        "Opt1": "単層パーセプトロン",
        "Opt2": "多層パーセプトロン",
        "Opt3": "線形回帰",
        "Opt4": "ロジスティック回帰",
        "Opt5": "ナイーブベイズ",
        "Opt6": "K近傍法",
        "Answer_Idx": 1,
        "Explanation": "隠れ層を追加することで、非線形な境界線を表現できるようになります。",
        "Link": "https://example.com/xor-problem"
    },
    {
        "ID": "ML-170",
        "Category": "3.機械学習の手法",
        "Question": "多数のモデルの平均をとるバギングが、単一のモデルよりも優れている数学的な理由は？",
        "Opt1": "バイアスの減少",
        "Opt2": "バリアンス（分散）の減少",
        "Opt3": "次元の呪いの回避",
        "Opt4": "計算時間の短縮",
        "Opt5": "ラベル付けの自動化",
        "Opt6": "欠損値の補完",
        "Answer_Idx": 1,
        "Explanation": "個々のモデルの誤差を打ち消し合い、モデルを安定させる効果があります。",
        "Link": "https://example.com/variance-reduction"
    },
    {
        "ID": "ML-171",
        "Category": "3.機械学習の手法",
        "Question": "ランダムフォレストにおいて、個々の決定木に「異なるデータのサブセット」と「異なる特徴量のセット」を与える理由は？",
        "Opt1": "計算を速くするため",
        "Opt2": "モデル間の相関を下げ、多様性を持たせるため",
        "Opt3": "メモリを節約するため",
        "Opt4": "過学習を促進するため",
        "Opt5": "解釈性を高めるため",
        "Opt6": "バイアスを増やすため",
        "Answer_Idx": 1,
        "Explanation": "似通った木ばかりになるとアンサンブルの効果が薄れるため、わざと多様性を作ります。",
        "Link": "https://example.com/rf-diversity"
    },
    {
        "ID": "ML-172",
        "Category": "3.機械学習の手法",
        "Question": "勾配ブースティング決定木（GBDT）の代表的な実装で、欠損値処理や並列計算に優れたライブラリは？",
        "Opt1": "scikit-learn",
        "Opt2": "TensorFlow",
        "Opt3": "XGBoost",
        "Opt4": "Keras",
        "Opt5": "PyTorch",
        "Opt6": "Pandas",
        "Answer_Idx": 2,
        "Explanation": "コンペティションや実務の表形式データ分析で極めて高い人気を誇ります。",
        "Link": "https://example.com/xgboost"
    },
    {
        "ID": "ML-173",
        "Category": "3.機械学習の手法",
        "Question": "強化学習において、エージェントが受け取る報酬の「合計」ではなく「時間的な期待値」を最大化する式は？",
        "Opt1": "ベルマン方程式",
        "Opt2": "オイラーの公式",
        "Opt3": "ピタゴラスの定理",
        "Opt4": "ニュートンの法則",
        "Opt5": "ケプラーの法則",
        "Opt6": "ラグランジュ方程式",
        "Answer_Idx": 0,
        "Explanation": "現在の状態の価値を、次の状態の価値と報酬で定義する強化学習の根幹となる方程式です。",
        "Link": "https://example.com/bellman-equation"
    },
    {
        "ID": "ML-174",
        "Category": "3.機械学習の手法",
        "Question": "強化学習の「Q学習」で、最も良い行動を選ぶ確率を高くしつつ、他の行動も試す戦略を何というか？",
        "Opt1": "ε-greedy法",
        "Opt2": "ソフトマックス法",
        "Opt3": "ルーレット選択",
        "Opt4": "トーナメント選択",
        "Opt5": "ランダムウォーク",
        "Opt6": "全探索",
        "Answer_Idx": 0,
        "Explanation": "一定の確率εでランダムな行動（探索）をし、1-εの確率で最善の行動（利用）をします。",
        "Link": "https://example.com/epsilon-greedy"
    },
    {
        "ID": "ML-175",
        "Category": "3.機械学習の手法",
        "Question": "特徴量のスケーリングにおいて、データの平均を0、分散を1にする「標準化」を行う最大のメリットは？",
        "Opt1": "外れ値の影響を完全に排除できる",
        "Opt2": "勾配降下法などの学習を安定・高速化できる",
        "Opt3": "データを0から1の範囲に収められる",
        "Opt4": "計算量を10分の1にできる",
        "Opt5": "カテゴリー変数を数値化できる",
        "Opt6": "ラベルの偏りを直せる",
        "Answer_Idx": 1,
        "Explanation": "単位が異なる特徴量（身長と年収など）を同じ尺度で扱えるようになります。",
        "Link": "https://example.com/standardization-pros"
    },
    {
        "ID": "ML-176",
        "Category": "3.機械学習の手法",
        "Question": "モデルの複雑さを「パラメータの数」と「尤度」のバランスで評価する指標（小さいほど良い）は？",
        "Opt1": "AIC（赤池情報量基準）",
        "Opt2": "F値",
        "Opt3": "決定係数",
        "Opt4": "相関係数",
        "Opt5": "ジニ係数",
        "Opt6": "AUC",
        "Answer_Idx": 0,
        "Explanation": "モデルの良さを、単純さと適合度の両面から評価する指標です。",
        "Link": "https://example.com/aic"
    },
    {
        "ID": "ML-177",
        "Category": "3.機械学習の手法",
        "Question": "2つの正規分布（クラスAとB）があるとき、その境界線を「最もよく分ける軸」を探す手法は？",
        "Opt1": "主成分分析 (PCA)",
        "Opt2": "線形判別分析 (LDA)",
        "Opt3": "K-means法",
        "Opt4": "ロジスティック回帰",
        "Opt5": "決定木",
        "Opt6": "SVM",
        "Answer_Idx": 1,
        "Explanation": "PCAが「全体の分散最大化」を目指すのに対し、LDAは「クラスの分離度最大化」を目指します。",
        "Link": "https://example.com/lda"
    },
    {
        "ID": "ML-178",
        "Category": "3.機械学習の手法",
        "Question": "分類問題において、予測が確率（0.8など）で出力される際、それを「正」か「負」に分ける境目となる値は？",
        "Opt1": "バイアス",
        "Opt2": "しきい値（閾値）",
        "Opt3": "マージン",
        "Opt4": "セントロイド",
        "Opt5": "学習率",
        "Opt6": "重み",
        "Answer_Idx": 1,
        "Explanation": "デフォルトは0.5が多いですが、目的（見逃しを防ぎたい等）に応じて変更します。",
        "Link": "https://example.com/threshold"
    },
    {
        "ID": "ML-179",
        "Category": "3.機械学習の手法",
        "Question": "欠損値の補完手法において、その変数の「平均値」や「中央値」を代入する手法を何というか？",
        "Opt1": "多重代入法",
        "Opt2": "単一代入法",
        "Opt3": "リストワイズ削除",
        "Opt4": "ペアワイズ削除",
        "Opt5": "回帰代入",
        "Opt6": "K近傍代入",
        "Answer_Idx": 1,
        "Explanation": "簡易的な手法ですが、データの分散が過小評価されるリスクがあります。",
        "Link": "https://example.com/imputation"
    },
    {
        "ID": "ML-180",
        "Category": "3.機械学習の手法",
        "Question": "「以前に学習した知識を忘れてしまう」現象で、特に強化学習やニューラルネットワークで問題となるのは？",
        "Opt1": "オーバーフィッティング",
        "Opt2": "勾配消失",
        "Opt3": "カタストロフィック忘却",
        "Opt4": "次元の呪い",
        "Opt5": "ハルシネーション",
        "Opt6": "モード崩壊",
        "Answer_Idx": 2,
        "Explanation": "新しいことを学ぶと古い知識が上書きされてしまう現象。継続学習の大きな課題です。",
        "Link": "https://example.com/catastrophic-forgetting"
    },
    {
        "ID": "ML-181",
        "Category": "3.機械学習の手法",
        "Question": "「TP=40",
        "Opt1": "FP=10",
        "Opt2": "FN=20",
        "Opt3": "TN=30」のとき、このモデルの適合率（Precision）はいくらか？",
        "Opt4": 0.4,
        "Opt5": 0.5,
        "Opt6": 0.6,
        "Answer_Idx": 0.7,
        "Explanation": 0.8,
        "Link": 0.9
    },
    {
        "ID": "ML-182",
        "Category": "3.機械学習の手法",
        "Question": "「TP=40",
        "Opt1": "FP=10",
        "Opt2": "FN=20",
        "Opt3": "TN=30」のとき、このモデルの再現率（Recall）はいくらか？",
        "Opt4": 0.33,
        "Opt5": 0.5,
        "Opt6": 0.6,
        "Answer_Idx": 0.67,
        "Explanation": 0.75,
        "Link": 0.8
    },
    {
        "ID": "ML-183",
        "Category": "3.機械学習の手法",
        "Question": "ROC曲線において、ランダムな予測しかできていないモデルのAUC（面積）の値は？",
        "Opt1": 0,
        "Opt2": 0.1,
        "Opt3": 0.5,
        "Opt4": 0.7,
        "Opt5": 0.9,
        "Opt6": 1,
        "Answer_Idx": 2,
        "Explanation": "ランダムな場合は対角線となり、面積は全体の半分の0.5になります。",
        "Link": "https://example.com/auc-random"
    },
    {
        "ID": "ML-184",
        "Category": "3.機械学習の手法",
        "Question": "学習データの中に、本来予測時には知り得ない「未来の情報」や「正解へのヒント」が混入することを何というか？",
        "Opt1": "オーバーフィッティング",
        "Opt2": "データリーク（漏洩）",
        "Opt3": "次元の呪い",
        "Opt4": "アンダーサンプリング",
        "Opt5": "正則化",
        "Opt6": "標準化",
        "Answer_Idx": 1,
        "Explanation": "データリークが起きると、学習中だけ異常に高い精度が出てしまい、実用時に失敗します。",
        "Link": "https://example.com/data-leakage"
    },
    {
        "ID": "ML-185",
        "Category": "3.機械学習の手法",
        "Question": "モデルのハイパーパラメータを調整する際、テストデータを使わず「検証用データ」を別途用意する理由は？",
        "Opt1": "計算速度を上げるため",
        "Opt2": "テストデータへの過学習（リーク）を防ぐため",
        "Opt3": "メモリを節約するため",
        "Opt4": "ラベルを増やすため",
        "Opt5": "次元を圧縮するため",
        "Opt6": "特徴量を選択するため",
        "Answer_Idx": 1,
        "Explanation": "テストデータは「最終確認」のためだけに使い、調整には検証用データを使うのが鉄則です。",
        "Link": "https://example.com/validation-data"
    },
    {
        "ID": "ML-186",
        "Category": "3.機械学習の手法",
        "Question": "SVMのカーネル関数の一つで、無限次元の写像に相当し、非線形分類に非常に強力なものは？",
        "Opt1": "線形カーネル",
        "Opt2": "多項式カーネル",
        "Opt3": "RBFカーネル（ガウスカーネル）",
        "Opt4": "シグモイドカーネル",
        "Opt5": "ステップカーネル",
        "Opt6": "ReLUカーネル",
        "Answer_Idx": 2,
        "Explanation": "Radial Basis Functionカーネルは、最も汎用的に使われる強力なカーネルです。",
        "Link": "https://example.com/rbf-kernel"
    },
    {
        "ID": "ML-187",
        "Category": "3.機械学習の手法",
        "Question": "回帰問題において、予測値と実測値の差を「実測値」で割り、誤差の割合（%）として評価する指標は？",
        "Opt1": "MSE",
        "Opt2": "RMSE",
        "Opt3": "MAE",
        "Opt4": "MAPE（平均絶対パーセント誤差）",
        "Opt5": "決定係数",
        "Opt6": "F値",
        "Answer_Idx": 3,
        "Explanation": "誤差を絶対的な数値ではなく、パーセンテージで把握したい場合に適しています。",
        "Link": "https://example.com/mape"
    },
    {
        "ID": "ML-188",
        "Category": "3.機械学習の手法",
        "Question": "多クラス分類の評価で、各クラスごとに指標を計算してから単純に平均をとる手法を何というか？",
        "Opt1": "マイクロ平均",
        "Opt2": "マクロ平均",
        "Opt3": "加重平均",
        "Opt4": "調和平均",
        "Opt5": "移動平均",
        "Opt6": "幾何平均",
        "Answer_Idx": 1,
        "Explanation": "マクロ平均は、各クラスのデータ数に関わらず、各クラスを平等に扱います。",
        "Link": "https://example.com/macro-average"
    },
    {
        "ID": "ML-189",
        "Category": "3.機械学習の手法",
        "Question": "多クラス分類の評価で、データ数が多いクラスの影響を強く受けるように重みを付けて平均をとる手法は？",
        "Opt1": "マイクロ平均",
        "Opt2": "マクロ平均",
        "Opt3": "単純平均",
        "Opt4": "期待値",
        "Opt5": "中央値",
        "Opt6": "最頻値",
        "Answer_Idx": 0,
        "Explanation": "全データを一括して集計してから指標を計算する手法で、データ数に比例した評価になります。",
        "Link": "https://example.com/micro-average"
    },
    {
        "ID": "ML-190",
        "Category": "3.機械学習の手法",
        "Question": "アンサンブル学習において、ブースティングがバギングに比べて「苦手」とする傾向があるのは？",
        "Opt1": "バイアスの削減",
        "Opt2": "未学習の解消",
        "Opt3": "外れ値やノイズへの耐性",
        "Opt4": "モデルの表現力向上",
        "Opt5": "計算の逐次性",
        "Opt6": "精度向上",
        "Answer_Idx": 2,
        "Explanation": "ブースティングは前のミスを追いかけるため、ノイズ（外れ値）に対しても過学習しやすい面があります。",
        "Link": "https://example.com/boosting-outlier"
    },
    {
        "ID": "ML-191",
        "Category": "3.機械学習の手法",
        "Question": "主成分分析（PCA）を行う前に、各特徴量に対して行うべき重要な前処理はどれか？",
        "Opt1": "二値化",
        "Opt2": "標準化（平均0、分散1）",
        "Opt3": "平滑化",
        "Opt4": "剪定",
        "Opt5": "データの水増し",
        "Opt6": "アノテーション",
        "Answer_Idx": 1,
        "Explanation": "単位が異なると、分散の大きい変数が主成分を支配してしまうため、標準化が必須です。",
        "Link": "https://example.com/pca-preprocessing"
    },
    {
        "ID": "ML-192",
        "Category": "3.機械学習の手法",
        "Question": "K-means法がうまく機能しにくい（苦手とする）データの分布形状はどのようなものか？",
        "Opt1": "球状のクラスタ",
        "Opt2": "三日月形やドーナツ形のような非線形な形状",
        "Opt3": "重なりが少ない形状",
        "Opt4": "データ数が少ない場合",
        "Opt5": "3次元データ",
        "Opt6": "ラベル付きデータ",
        "Answer_Idx": 1,
        "Explanation": "K-meansは重心からの距離（球状）を前提とするため、複雑な形状の分離にはDBSCANなどが向いています。",
        "Link": "https://example.com/kmeans-weakness"
    },
    {
        "ID": "ML-193",
        "Category": "3.機械学習の手法",
        "Question": "「ユーザーが過去に高く評価した映画」と「その映画のメタデータ」を利用して推薦を行う手法は？",
        "Opt1": "協調フィルタリング",
        "Opt2": "内容ベースフィルタリング",
        "Opt3": "ランダム推薦",
        "Opt4": "バンディットアルゴリズム",
        "Opt5": "強化学習",
        "Opt6": "次元圧縮",
        "Answer_Idx": 1,
        "Explanation": "アイテムの特徴（メタデータ）に依存するため、コールドスタート問題が起きにくいです。",
        "Link": "https://example.com/content-based-recap"
    },
    {
        "ID": "ML-194",
        "Category": "3.機械学習の手法",
        "Question": "特徴量エンジニアリングにおいて、カテゴリー変数（赤、青、緑など）を数値（1",
        "Opt1": 0,
        "Opt2": "0など）に変換する手法は？",
        "Opt3": "スケーリング",
        "Opt4": "正規化",
        "Opt5": "One-Hotエンコーディング",
        "Opt6": "ラベルエンコーディング",
        "Answer_Idx": "ビン化",
        "Explanation": "標準化",
        "Link": 2
    },
    {
        "ID": "ML-195",
        "Category": "3.機械学習の手法",
        "Question": "「あるモデルの予測値を、次のモデルの入力として使う」という多段構成のアンサンブル学習は？",
        "Opt1": "バギング",
        "Opt2": "ブースティング",
        "Opt3": "スタッキング",
        "Opt4": "ホールドアウト",
        "Opt5": "クロスバリデーション",
        "Opt6": "剪定",
        "Answer_Idx": 2,
        "Explanation": "Kaggleなどのコンペティションで最終的な精度を絞り出す際によく使われます。",
        "Link": "https://example.com/stacking-recap"
    },
    {
        "ID": "ML-196",
        "Category": "3.機械学習の手法",
        "Question": "モデルの学習が全く進まない（訓練誤差すら減らない）状態を何というか？",
        "Opt1": "過学習",
        "Opt2": "未学習（アンダーフィッティング）",
        "Opt3": "勾配消失",
        "Opt4": "収束",
        "Opt5": "汎化",
        "Opt6": "推論",
        "Answer_Idx": 1,
        "Explanation": "モデルの表現力が不足しているか、学習回数が少なすぎる場合に起こります。",
        "Link": "https://example.com/underfitting-recap"
    },
    {
        "ID": "ML-197",
        "Category": "3.機械学習の手法",
        "Question": "「モデルの複雑さ」と「汎化誤差」の関係において、複雑さを増すと汎化誤差が再び減少に転じる現象は？",
        "Opt1": "二重降下現象（Double Descent）",
        "Opt2": "次元の呪い",
        "Opt3": "ノー・フリーランチ",
        "Opt4": "オッカムの剃刀",
        "Opt5": "バイアス増大",
        "Opt6": "収束の遅れ",
        "Answer_Idx": 0,
        "Explanation": "ディープラーニングなどで見られる、過学習の領域を超えると再び精度が上がる現象です。",
        "Link": "https://example.com/double-descent"
    },
    {
        "ID": "ML-198",
        "Category": "3.機械学習の手法",
        "Question": "バイアスを最小限にしつつ、未知のデータへの対応能力（汎化性能）を最大化することが機械学習の目的であるが、その誤差の総和を何というか？",
        "Opt1": "経験損失",
        "Opt2": "期待損失（汎化誤差）",
        "Opt3": "トレーニング誤差",
        "Opt4": "正規化誤差",
        "Opt5": "残差",
        "Opt6": "偏差",
        "Answer_Idx": 1,
        "Explanation": "学習データではなく、見たことがない将来のデータに対する誤差を最小化することがゴールです。",
        "Link": "https://example.com/gen-error"
    },
    {
        "ID": "ML-199",
        "Category": "3.機械学習の手法",
        "Question": "機械学習において、データの傾向が時間の経過とともに変化してしまい、モデルの精度が落ちる現象は？",
        "Opt1": "データリーク",
        "Opt2": "コンセプトドリフト",
        "Opt3": "過学習",
        "Opt4": "ハルシネーション",
        "Opt5": "モード崩壊",
        "Opt6": "勾配消失",
        "Answer_Idx": 1,
        "Explanation": "ユーザーの好みや市場環境が変わることで、過去の学習結果が通用しなくなることです。",
        "Link": "https://example.com/concept-drift"
    },
    {
        "ID": "ML-200",
        "Category": "3.機械学習の手法",
        "Question": "「単純なモデルの方が、複雑なモデルよりも好ましい」という、科学における普遍的な指針を何というか？",
        "Opt1": "ムーアの法則",
        "Opt2": "オッカムの剃刀",
        "Opt3": "ジップの法則",
        "Opt4": "パレートの法則",
        "Opt5": "大数の法則",
        "Opt6": "ベルの法則",
        "Answer_Idx": 1,
        "Explanation": "不必要に複雑な説明（モデル）を避け、シンプルさを追求する考え方です。",
        "Link": "https://example.com/occams-razor"
    },
    {
        "ID": "DL-001",
        "Category": "4.ディープラーニングの概要",
        "Question": "複数の入力を受け取り、重み付けされた合計がある閾値を超えると「1」を出力する最も初期のモデルは？",
        "Opt1": "畳み込みニューラルネットワーク",
        "Opt2": "単純パーセプトロン",
        "Opt3": "リカレントニューラルネットワーク",
        "Opt4": "自己組織化マップ",
        "Opt5": "ボルツマンマシン",
        "Opt6": "サポートベクターマシン",
        "Answer_Idx": 1,
        "Explanation": "1958年にローゼンブラットが考案した、現代のニューラルネットワークの基礎となるモデルです。",
        "Link": "https://example.com/perceptron"
    },
    {
        "ID": "DL-002",
        "Category": "4.ディープラーニングの概要",
        "Question": "単純パーセプトロンを多層に重ねることで、どのような問題を解けるようになったか？",
        "Opt1": "線形分離可能な問題",
        "Opt2": "非線形な問題（XOR問題など）",
        "Opt3": "行列分解",
        "Opt4": "クラスタリング",
        "Opt5": "次元削減",
        "Opt6": "自己相関",
        "Answer_Idx": 1,
        "Explanation": "隠れ層（中間層）を追加した多層パーセプトロンにより、複雑な境界線を表現可能になりました。",
        "Link": "https://example.com/multi-layer"
    },
    {
        "ID": "DL-003",
        "Category": "4.ディープラーニングの概要",
        "Question": "ニューラルネットワークの各層で、入力の合計を次の層への出力に変換する関数を何というか？",
        "Opt1": "損失関数",
        "Opt2": "活性化関数",
        "Opt3": "目的関数",
        "Opt4": "正則化関数",
        "Opt5": "恒等関数",
        "Opt6": "正規化関数",
        "Answer_Idx": 1,
        "Explanation": "シグモイド関数やReLU関数などがあり、ネットワークに非線形性をもたらします。",
        "Link": "https://example.com/activation-fn"
    },
    {
        "ID": "DL-004",
        "Category": "4.ディープラーニングの概要",
        "Question": "深層学習で最も一般的に使われ、入力が0以下なら0、0より大きければそのまま出力する関数は？",
        "Opt1": "シグモイド関数",
        "Opt2": "tanh関数",
        "Opt3": "ReLU関数",
        "Opt4": "ステップ関数",
        "Opt5": "ソフトマックス関数",
        "Opt6": "サイン関数",
        "Answer_Idx": 2,
        "Explanation": "Rectified Linear Unit。勾配消失が起きにくく、計算が高速なため主流となっています。",
        "Link": "https://example.com/relu"
    },
    {
        "ID": "DL-005",
        "Category": "4.ディープラーニングの概要",
        "Question": "出力層で用いられ、各ノードの出力の合計が「1」になるように変換する関数は？",
        "Opt1": "ReLU関数",
        "Opt2": "シグモイド関数",
        "Opt3": "ソフトマックス関数",
        "Opt4": "tanh関数",
        "Opt5": "線形関数",
        "Opt6": "Leaky ReLU",
        "Answer_Idx": 2,
        "Explanation": "多クラス分類の出力層で、各クラスに属する「確率」を表現するために使われます。",
        "Link": "https://example.com/softmax"
    },
    {
        "ID": "DL-006",
        "Category": "4.ディープラーニングの概要",
        "Question": "誤差をネットワークの出力側から入力側へと逆方向に伝播させ、重みを更新する手法を何というか？",
        "Opt1": "順伝播（フォワードプロパゲーション）",
        "Opt2": "誤差逆伝播法（バックプロパゲーション）",
        "Opt3": "主成分分析",
        "Opt4": "モンテカルロ法",
        "Opt5": "Q学習",
        "Opt6": "最尤推定",
        "Answer_Idx": 1,
        "Explanation": "微分の連鎖律（チェインルール）を利用して、効率的に勾配を計算するアルゴリズムです。",
        "Link": "https://example.com/backprop"
    },
    {
        "ID": "DL-007",
        "Category": "4.ディープラーニングの概要",
        "Question": "誤差逆伝播法において、重みの更新量を決定するために使われる数学的な概念は？",
        "Opt1": "行列式",
        "Opt2": "勾配（微分）",
        "Opt3": "固有値",
        "Opt4": "分散",
        "Opt5": "標準偏差",
        "Opt6": "中央値",
        "Answer_Idx": 1,
        "Explanation": "各重みが誤差にどれだけ影響しているかを「勾配」として算出し、誤差を最小化する方向に更新します。",
        "Link": "https://example.com/gradient"
    },
    {
        "ID": "DL-008",
        "Category": "4.ディープラーニングの概要",
        "Question": "学習が進むにつれて勾配が極端に小さくなり、重みが更新されなくなる現象を何というか？",
        "Opt1": "勾配爆発",
        "Opt2": "勾配消失",
        "Opt3": "過学習",
        "Opt4": "未学習",
        "Opt5": "局所最適解",
        "Opt6": "サドルポイント",
        "Answer_Idx": 1,
        "Explanation": "シグモイド関数を多層で使うと、微分値が小さくなり、入力層に近い重みが学習されなくなります。",
        "Link": "https://example.com/vanishing-gradient"
    },
    {
        "ID": "DL-009",
        "Category": "4.ディープラーニングの概要",
        "Question": "勾配消失問題の解決策として、適切でないものはどれか？",
        "Opt1": "ReLU関数の使用",
        "Opt2": "バッチ正規化の導入",
        "Opt3": "重みの初期値の工夫（Heの初期値など）",
        "Opt4": "シグモイド関数の多用",
        "Opt5": "残差接続（Skip Connection）",
        "Opt6": "LSTMの採用",
        "Answer_Idx": 3,
        "Explanation": "シグモイド関数は微分の最大値が0.25と小さいため、多層化すると勾配消失を引き起こします。",
        "Link": "https://example.com/vanishing-sol"
    },
    {
        "ID": "DL-010",
        "Category": "4.ディープラーニングの概要",
        "Question": "重みの更新を行う際、1回の更新でどれだけ重みを動かすかを調整するパラメータは？",
        "Opt1": "エポック数",
        "Opt2": "バッチサイズ",
        "Opt3": "学習率",
        "Opt4": "モメンタム",
        "Opt5": "バイアス",
        "Opt6": "ドロップアウト率",
        "Answer_Idx": 2,
        "Explanation": "学習率が大きすぎると発散し、小さすぎると学習がいつまでも終わりません。",
        "Link": "https://example.com/learning-rate"
    },
    {
        "ID": "DL-011",
        "Category": "4.ディープラーニングの概要",
        "Question": "すべての学習データを1回使い切る学習単位のことを何というか？",
        "Opt1": "イテレーション",
        "Opt2": "エポック",
        "Opt3": "ステップ",
        "Opt4": "バッチ",
        "Opt5": "サンプル",
        "Opt6": "ウィンドウ",
        "Answer_Idx": 1,
        "Explanation": "例えば1000個のデータを10エポック学習させる場合、各データを10回学習に使います。",
        "Link": "https://example.com/epoch"
    },
    {
        "ID": "DL-012",
        "Category": "4.ディープラーニングの概要",
        "Question": "学習データをいくつかの塊に分け、その塊ごとに重みを更新する手法を何というか？",
        "Opt1": "オンライン学習",
        "Opt2": "バッチ学習",
        "Opt3": "ミニバッチ学習",
        "Opt4": "強化学習",
        "Opt5": "転移学習",
        "Opt6": "能動学習",
        "Answer_Idx": 2,
        "Explanation": "全データを使う「バッチ学習」と、1件ずつの「オンライン学習」の中間的な手法です。",
        "Link": "https://example.com/mini-batch"
    },
    {
        "ID": "DL-013",
        "Category": "4.ディープラーニングの概要",
        "Question": "ミニバッチ学習において、一度に処理するデータ件数のことを何というか？",
        "Opt1": "エポックサイズ",
        "Opt2": "バッチサイズ",
        "Opt3": "隠れ層の数",
        "Opt4": "ノード数",
        "Opt5": "イテレーション数",
        "Opt6": "学習率",
        "Answer_Idx": 1,
        "Explanation": "GPUのメモリ容量や学習の安定性を考慮して、32や64、128などがよく選ばれます。",
        "Link": "https://example.com/batch-size"
    },
    {
        "ID": "DL-014",
        "Category": "4.ディープラーニングの概要",
        "Question": "「学習率」のように、学習過程を通じて自動で更新されず、人間が設定する変数を何というか？",
        "Opt1": "パラメータ",
        "Opt2": "ハイパーパラメータ",
        "Opt3": "バイアス",
        "Opt4": "重み",
        "Opt5": "損失",
        "Opt6": "出力",
        "Answer_Idx": 1,
        "Explanation": "モデルの性能に大きく影響するため、グリッドサーチなどで最適化を行います。",
        "Link": "https://example.com/hyperparameter-dl"
    },
    {
        "ID": "DL-015",
        "Category": "4.ディープラーニングの概要",
        "Question": "最も基本的な勾配降下法で、現在の勾配に一定の学習率を掛けて重みを更新する手法は？",
        "Opt1": "SGD（確率的勾配降下法）",
        "Opt2": "Momentum",
        "Opt3": "AdaGrad",
        "Opt4": "RMSprop",
        "Opt5": "Adam",
        "Opt6": "AdaDelta",
        "Answer_Idx": 0,
        "Explanation": "シンプルですが、学習が停滞したり振動したりしやすい欠点があります。",
        "Link": "https://example.com/sgd"
    },
    {
        "ID": "DL-016",
        "Category": "4.ディープラーニングの概要",
        "Question": "過去の勾配を「慣性」として考慮し、重みの更新をスムーズにする手法を何というか？",
        "Opt1": "AdaGrad",
        "Opt2": "Momentum（慣性項）",
        "Opt3": "Dropout",
        "Opt4": "Batch Normalization",
        "Opt5": "L2正則化",
        "Opt6": "Early Stopping",
        "Answer_Idx": 1,
        "Explanation": "物理的な運動のように、これまでの移動方向を維持しようとする力を加えます。",
        "Link": "https://example.com/momentum"
    },
    {
        "ID": "DL-017",
        "Category": "4.ディープラーニングの概要",
        "Question": "学習率を、学習が進むにつれてパラメータごとに自動で調整する手法の代表例は？",
        "Opt1": "SGD",
        "Opt2": "Momentum",
        "Opt3": "AdaGrad",
        "Opt4": "Step Decay",
        "Opt5": "Random Forest",
        "Opt6": "SVM",
        "Answer_Idx": 2,
        "Explanation": "頻繁に出現する勾配に対しては学習率を下げ、稀な勾配には大きく学習させる手法です。",
        "Link": "https://example.com/adagrad"
    },
    {
        "ID": "DL-018",
        "Category": "4.ディープラーニングの概要",
        "Question": "MomentumとAdaGradの考え方を組み合わせた、現在最も広く使われている最適化手法は？",
        "Opt1": "RMSprop",
        "Opt2": "Nesterov Momentum",
        "Opt3": "Adam",
        "Opt4": "AdaDelta",
        "Opt5": "SGD",
        "Opt6": "RPROP",
        "Answer_Idx": 2,
        "Explanation": "Adaptive Moment Estimationの略。非常に安定しており、第一選択肢とされることが多いです。",
        "Link": "https://example.com/adam"
    },
    {
        "ID": "DL-019",
        "Category": "4.ディープラーニングの概要",
        "Question": "学習中にネットワークの一部のノードをランダムに無効化し、過学習を防ぐ手法は？",
        "Opt1": "バッチ正規化",
        "Opt2": "ドロップアウト",
        "Opt3": "L1正則化",
        "Opt4": "早期終了",
        "Opt5": "プーリング",
        "Opt6": "パディング",
        "Answer_Idx": 1,
        "Explanation": "特定のノードだけに頼らない（アンサンブル学習に近い）頑健なモデルになります。",
        "Link": "https://example.com/dropout"
    },
    {
        "ID": "DL-020",
        "Category": "4.ディープラーニングの概要",
        "Question": "各層の入力を、平均0、分散1になるように正規化し、学習を安定化・高速化する手法は？",
        "Opt1": "層化抽出",
        "Opt2": "バッチ正規化（Batch Normalization）",
        "Opt3": "重みの初期化",
        "Opt4": "データ拡張",
        "Opt5": "蒸留",
        "Opt6": "剪定",
        "Answer_Idx": 1,
        "Explanation": "勾配消失や勾配爆発を抑え、高い学習率を設定できるようになります。",
        "Link": "https://example.com/batch-norm"
    },
    {
        "ID": "DL-021",
        "Category": "4.ディープラーニングの概要",
        "Question": "画像認識で多用されるCNNにおいて、画像から特徴を抽出するために用いる小さな行列を何というか？",
        "Opt1": "フィルタ（カーネル）",
        "Opt2": "ストライド",
        "Opt3": "パディング",
        "Opt4": "プーリング",
        "Opt5": "バイアス",
        "Opt6": "全結合層",
        "Answer_Idx": 0,
        "Explanation": "フィルタを画像上で滑らせる「畳み込み演算」により、エッジなどの特徴を抽出します。",
        "Link": "https://example.com/cnn-filter"
    },
    {
        "ID": "DL-022",
        "Category": "4.ディープラーニングの概要",
        "Question": "CNNにおいて、フィルタを動かす間隔（ステップ数）のことを何と呼ぶか？",
        "Opt1": "パディング",
        "Opt2": "ストライド",
        "Opt3": "チャネル",
        "Opt4": "深度",
        "Opt5": "プーリング",
        "Opt6": "平坦化",
        "Answer_Idx": 1,
        "Explanation": "ストライドを大きくすると、出力される特徴マップのサイズは小さくなります。",
        "Link": "https://example.com/stride"
    },
    {
        "ID": "DL-023",
        "Category": "4.ディープラーニングの概要",
        "Question": "畳み込み演算後に出力サイズが小さくなるのを防ぐため、入力の周囲を0などで埋める操作は？",
        "Opt1": "ストライド",
        "Opt2": "プーリング",
        "Opt3": "パディング",
        "Opt4": "正規化",
        "Opt5": "標準化",
        "Opt6": "ドロップアウト",
        "Answer_Idx": 2,
        "Explanation": "パディングを行うことで、画像の端の情報も十分に活用できるようになります。",
        "Link": "https://example.com/padding"
    },
    {
        "ID": "DL-024",
        "Category": "4.ディープラーニングの概要",
        "Question": "特徴マップの重要な情報を残しつつ、サイズを縮小して計算量を減らす層を何というか？",
        "Opt1": "畳み込み層",
        "Opt2": "プーリング層",
        "Opt3": "全結合層",
        "Opt4": "出力層",
        "Opt5": "隠れ層",
        "Opt6": "回帰層",
        "Answer_Idx": 1,
        "Explanation": "最大値をとる「Maxプーリング」や、平均値をとる「Averageプーリング」があります。",
        "Link": "https://example.com/pooling"
    },
    {
        "ID": "DL-025",
        "Category": "4.ディープラーニングの概要",
        "Question": "プーリング層の主な特徴として、正しいものはどれか？",
        "Opt1": "学習すべきパラメータがない",
        "Opt2": "重みが誤差逆伝播で更新される",
        "Opt3": "常に活性化関数が必要である",
        "Opt4": "入力サイズを大きくする",
        "Opt5": "色の情報を増やす",
        "Opt6": "必ず全結合層の後ろに置く",
        "Answer_Idx": 0,
        "Explanation": "プーリングは一定のルール（最大値抽出など）に従うだけで、学習パラメータを持ちません。",
        "Link": "https://example.com/pooling-params"
    },
    {
        "ID": "DL-026",
        "Category": "4.ディープラーニングの概要",
        "Question": "CNNの最後に置かれ、抽出された特徴を1次元に並べて最終的な分類を行う層を何というか？",
        "Opt1": "畳み込み層",
        "Opt2": "プーリング層",
        "Opt3": "全結合層",
        "Opt4": "入力層",
        "Opt5": "平坦化層",
        "Opt6": "再帰層",
        "Answer_Idx": 2,
        "Explanation": "Fully Connected Layer。すべてのノードが次の層のすべてのノードと結合しています。",
        "Link": "https://example.com/fully-connected"
    },
    {
        "ID": "DL-027",
        "Category": "4.ディープラーニングの概要",
        "Question": "CNNにおいて、物体の位置が多少ずれても正しく認識できる性質を何というか？",
        "Opt1": "線形分離可能性",
        "Opt2": "不変性（位置不変性）",
        "Opt3": "局所性",
        "Opt4": "汎化性能",
        "Opt5": "正則化",
        "Opt6": "標準化",
        "Answer_Idx": 1,
        "Explanation": "畳み込みとプーリングの組み合わせにより、ズレや歪みに強い認識が可能になります。",
        "Link": "https://example.com/invariance"
    },
    {
        "ID": "DL-028",
        "Category": "4.ディープラーニングの概要",
        "Question": "系列データ（文章、音声、株価など）を扱うために、自身の出力を再び入力に利用するネットワークは？",
        "Opt1": "CNN",
        "Opt2": "RNN（再帰型ニューラルネットワーク）",
        "Opt3": "GAN",
        "Opt4": "SVM",
        "Opt5": "K-means",
        "Opt6": "決定木",
        "Answer_Idx": 1,
        "Explanation": "内部に状態を保持（記憶）することで、過去の情報を現在の処理に反映させます。",
        "Link": "https://example.com/rnn-intro"
    },
    {
        "ID": "DL-029",
        "Category": "4.ディープラーニングの概要",
        "Question": "RNNにおいて、長い系列を学習する際に勾配が消えてしまい、長期の依存関係を学べない問題を何というか？",
        "Opt1": "勾配爆発",
        "Opt2": "勾配消失問題",
        "Opt3": "過学習",
        "Opt4": "ハルシネーション",
        "Opt5": "モード崩壊",
        "Opt6": "デッドReLU",
        "Answer_Idx": 1,
        "Explanation": "古い情報の勾配が逆伝播中に消えてしまうため、長い文脈の理解が難しくなります。",
        "Link": "https://example.com/rnn-vanishing"
    },
    {
        "ID": "DL-030",
        "Category": "4.ディープラーニングの概要",
        "Question": "RNNの勾配消失問題を解決するために考案された、メモリセルやゲート構造を持つモデルは？",
        "Opt1": "CNN",
        "Opt2": "LSTM",
        "Opt3": "Perceptron",
        "Opt4": "LeNet",
        "Opt5": "AlexNet",
        "Opt6": "VGG",
        "Answer_Idx": 1,
        "Explanation": "Long Short-Term Memory。情報を「忘れる」「保存する」を選択するゲートを備えています。",
        "Link": "https://example.com/lstm"
    },
    {
        "ID": "DL-031",
        "Category": "4.ディープラーニングの概要",
        "Question": "LSTMを簡略化し、計算コストを抑えつつ同等の性能を目指したモデルを何というか？",
        "Opt1": "RNN",
        "Opt2": "GRU（Gated Recurrent Unit）",
        "Opt3": "CNN",
        "Opt4": "GAN",
        "Opt5": "Transformer",
        "Opt6": "BERT",
        "Answer_Idx": 1,
        "Explanation": "リセットゲートと更新ゲートの2つで構成され、LSTMよりパラメータが少ないです。",
        "Link": "https://example.com/gru"
    },
    {
        "ID": "DL-032",
        "Category": "4.ディープラーニングの概要",
        "Question": "畳み込み層において、同じフィルタの重みを画像全体で共有することを何というか？",
        "Opt1": "重み共有",
        "Opt2": "重み初期化",
        "Opt3": "重み減衰",
        "Opt4": "重み転送",
        "Opt5": "重み増幅",
        "Opt6": "重み制限",
        "Answer_Idx": 0,
        "Explanation": "これにより、パラメータ数を劇的に減らし、効率的な学習が可能になります。",
        "Link": "https://example.com/weight-sharing"
    },
    {
        "ID": "DL-033",
        "Category": "4.ディープラーニングの概要",
        "Question": "2012年のILSVRCで圧勝し、ディープラーニングブームの火付け役となったCNNモデルは？",
        "Opt1": "LeNet",
        "Opt2": "AlexNet",
        "Opt3": "VGG",
        "Opt4": "ResNet",
        "Opt5": "GoogLeNet",
        "Opt6": "MobileNet",
        "Answer_Idx": 1,
        "Explanation": "GPUの活用、ReLU、ドロップアウトなどを採用した革新的なモデルでした。",
        "Link": "https://example.com/alexnet"
    },
    {
        "ID": "DL-034",
        "Category": "4.ディープラーニングの概要",
        "Question": "層を非常に深くしても学習がうまく進むよう、入力を後の層に直接足し合わせる構造を持つモデルは？",
        "Opt1": "AlexNet",
        "Opt2": "VGG",
        "Opt3": "ResNet",
        "Opt4": "Inception",
        "Opt5": "LeNet",
        "Opt6": "RNN",
        "Answer_Idx": 2,
        "Explanation": "Residual Network（残差ネットワーク）。「スキップ接続」により100層を超える深層化を実現しました。",
        "Link": "https://example.com/resnet"
    },
    {
        "ID": "DL-035",
        "Category": "4.ディープラーニングの概要",
        "Question": "モデルの学習を、検証用データの誤差が上がり始めた時点で強制的に終了させる手法を何というか？",
        "Opt1": "ドロップアウト",
        "Opt2": "バッチ正規化",
        "Opt3": "早期終了（Early Stopping）",
        "Opt4": "正則化",
        "Opt5": "蒸留",
        "Opt6": "剪定",
        "Answer_Idx": 2,
        "Explanation": "過学習が始まる直前で止めることで、汎化性能を最大化します。",
        "Link": "https://example.com/early-stopping"
    },
    {
        "ID": "DL-036",
        "Category": "4.ディープラーニングの概要",
        "Question": "重みの2乗和を損失関数に加えることで、重みが大きくなりすぎるのを防ぐ正則化手法は？",
        "Opt1": "L1正則化",
        "Opt2": "L2正則化",
        "Opt3": "ドロップアウト",
        "Opt4": "バッチ正規化",
        "Opt5": "データ拡張",
        "Opt6": "早期終了",
        "Answer_Idx": 1,
        "Explanation": "「重み減衰（Weight Decay）」とも呼ばれ、モデルを滑らかにして過学習を抑えます。",
        "Link": "https://example.com/l2-reg"
    },
    {
        "ID": "DL-037",
        "Category": "4.ディープラーニングの概要",
        "Question": "重みの絶対値の和を損失関数に加え、不要な重みを0にする（スパースにする）正則化手法は？",
        "Opt1": "L1正則化",
        "Opt2": "L2正則化",
        "Opt3": "モーメンタム",
        "Opt4": "Adam",
        "Opt5": "AdaGrad",
        "Opt6": "Softmax",
        "Answer_Idx": 0,
        "Explanation": "特徴量選択のような効果が得られるのが特徴です。",
        "Link": "https://example.com/l1-reg"
    },
    {
        "ID": "DL-038",
        "Category": "4.ディープラーニングの概要",
        "Question": "画像全体を1次元のベクトルに変換する際、多次元配列を平らにする操作を何というか？",
        "Opt1": "畳み込み",
        "Opt2": "プーリング",
        "Opt3": "平坦化（Flatten）",
        "Opt4": "パディング",
        "Opt5": "ストライド",
        "Opt6": "正規化",
        "Answer_Idx": 2,
        "Explanation": "畳み込み層やプーリング層の出力を、全結合層に入力するために必要な処理です。",
        "Link": "https://example.com/flatten"
    },
    {
        "ID": "DL-039",
        "Category": "4.ディープラーニングの概要",
        "Question": "RNNを双方向に繋げ、過去の情報だけでなく未来の情報も加味して処理する手法は？",
        "Opt1": "Bi-RNN（双方向RNN）",
        "Opt2": "LSTM",
        "Opt3": "GRU",
        "Opt4": "CNN",
        "Opt5": "GAN",
        "Opt6": "Encoder-Decoder",
        "Answer_Idx": 0,
        "Explanation": "翻訳や音声認識など、文全体の文脈が必要なタスクで有効です。",
        "Link": "https://example.com/birnn"
    },
    {
        "ID": "DL-040",
        "Category": "4.ディープラーニングの概要",
        "Question": "入力を一度ベクトルに圧縮し、再び元の入力を復元するように学習する教師なし学習モデルは？",
        "Opt1": "CNN",
        "Opt2": "RNN",
        "Opt3": "自己符号化器（オートエンコーダ）",
        "Opt4": "GAN",
        "Opt5": "SVM",
        "Opt6": "決定木",
        "Answer_Idx": 2,
        "Explanation": "入力と同じものを出力するように学習することで、データの効率的な圧縮（特徴抽出）を学びます。",
        "Link": "https://example.com/autoencoder"
    },
    {
        "ID": "DL-041",
        "Category": "4.ディープラーニングの概要",
        "Question": "「生成器（Generator）」と「識別器（Discriminator）」を競わせることで精度の高いデータを生成するモデルは？",
        "Opt1": "RNN",
        "Opt2": "LSTM",
        "Opt3": "GAN（敵対的生成ネットワーク）",
        "Opt4": "オートエンコーダ",
        "Opt5": "CNN",
        "Opt6": "Transformer",
        "Answer_Idx": 2,
        "Explanation": "Generatorが偽物を作り、Discriminatorがそれを見破るというプロセスで学習が進みます。",
        "Link": "https://example.com/gan-concept"
    },
    {
        "ID": "DL-042",
        "Category": "4.ディープラーニングの概要",
        "Question": "画像内のどこに何があるかを「バウンディングボックス」で囲んで特定するタスクを何というか？",
        "Opt1": "画像分類",
        "Opt2": "物体検出",
        "Opt3": "セマンティックセグメンテーション",
        "Opt4": "インスタンスセグメンテーション",
        "Opt5": "異常検知",
        "Opt6": "超解像",
        "Answer_Idx": 1,
        "Explanation": "「何があるか（分類）」と「どこにあるか（回帰）」を同時に行うタスクです。",
        "Link": "https://example.com/object-detection"
    },
    {
        "ID": "DL-043",
        "Category": "4.ディープラーニングの概要",
        "Question": "物体検出アルゴリズムにおいて、画像全体を一度走査するだけで高速に検出を行う手法は？",
        "Opt1": "R-CNN",
        "Opt2": "Fast R-CNN",
        "Opt3": "Faster R-CNN",
        "Opt4": "YOLO",
        "Opt5": "Selective Search",
        "Opt6": "滑り窓法",
        "Answer_Idx": 3,
        "Explanation": "You Only Look Once（YOLO）は、リアルタイム性に優れた代表的なアルゴリズムです。",
        "Link": "https://example.com/yolo"
    },
    {
        "ID": "DL-044",
        "Category": "4.ディープラーニングの概要",
        "Question": "画像内のすべての画素（ピクセル）に対してクラスを割り当てるタスクを何というか？",
        "Opt1": "画像分類",
        "Opt2": "物体検出",
        "Opt3": "セマンティックセグメンテーション",
        "Opt4": "物体追跡",
        "Opt5": "次元削減",
        "Opt6": "正規化",
        "Answer_Idx": 2,
        "Explanation": "ピクセル単位で「道路」「空」「人」などを塗り分ける技術です。",
        "Link": "https://example.com/semantic-seg"
    },
    {
        "ID": "DL-045",
        "Category": "4.ディープラーニングの概要",
        "Question": "同じクラスの個別の物体（人A、人Bなど）を区別して塗り分けるセグメンテーション手法は？",
        "Opt1": "セマンティックセグメンテーション",
        "Opt2": "インスタンスセグメンテーション",
        "Opt3": "パノプティックセグメンテーション",
        "Opt4": "物体検出",
        "Opt5": "画像分類",
        "Opt6": "スタイル変換",
        "Answer_Idx": 1,
        "Explanation": "「人」というクラスだけでなく、個体まで識別して分離します。",
        "Link": "https://example.com/instance-seg"
    },
    {
        "ID": "DL-046",
        "Category": "4.ディープラーニングの概要",
        "Question": "翻訳モデルなどで、入力系列の特定の箇所に「注意」を向けて処理する仕組みを何というか？",
        "Opt1": "ドロップアウト",
        "Opt2": "プーリング",
        "Opt3": "Attention（注意機構）",
        "Opt4": "スキップ接続",
        "Opt5": "バッチ正規化",
        "Opt6": "活性化関数",
        "Answer_Idx": 2,
        "Explanation": "文中の重要な単語に高い重みを置くことで、長文の理解が飛躍的に向上しました。",
        "Link": "https://example.com/attention"
    },
    {
        "ID": "DL-047",
        "Category": "4.ディープラーニングの概要",
        "Question": "RNNを使わず、Attentionのみで構成された、現在のLLMの基礎となるアーキテクチャは？",
        "Opt1": "ResNet",
        "Opt2": "VGG",
        "Opt3": "Transformer",
        "Opt4": "AlexNet",
        "Opt5": "Inception",
        "Opt6": "LeNet",
        "Answer_Idx": 2,
        "Explanation": "2017年に発表され、並列計算が可能になったことで大規模な学習が可能になりました。",
        "Link": "https://example.com/transformer-arch"
    },
    {
        "ID": "DL-048",
        "Category": "4.ディープラーニングの概要",
        "Question": "Transformerにおいて、入力された単語の位置情報をネットワークに伝えるための仕組みは？",
        "Opt1": "ストライド",
        "Opt2": "パディング",
        "Opt3": "位置エンコーディング（Positional Encoding）",
        "Opt4": "ソフトマックス",
        "Opt5": "ReLU",
        "Opt6": "全結合層",
        "Answer_Idx": 2,
        "Explanation": "Transformer自体には順序を理解する構造がないため、この値を加算して順序を教えます。",
        "Link": "https://example.com/positional-encoding"
    },
    {
        "ID": "DL-049",
        "Category": "4.ディープラーニングの概要",
        "Question": "エンコーダとデコーダを繋ぎ、可変長の入力を可変長の出力に変換する構造を何というか？",
        "Opt1": "GAN",
        "Opt2": "Seq2Seq",
        "Opt3": "CNN",
        "Opt4": "SVM",
        "Opt5": "PCA",
        "Opt6": "K-means",
        "Answer_Idx": 1,
        "Explanation": "機械翻訳や対話システムなどの基盤となった、系列から系列への変換モデルです。",
        "Link": "https://example.com/seq2seq"
    },
    {
        "ID": "DL-050",
        "Category": "4.ディープラーニングの概要",
        "Question": "オートエンコーダの潜在変数に確率分布を導入し、新しいデータを生成可能にしたモデルは？",
        "Opt1": "Denoising Autoencoder",
        "Opt2": "VAE（変分自己符号化器）",
        "Opt3": "GAN",
        "Opt4": "CNN",
        "Opt5": "RNN",
        "Opt6": "LSTM",
        "Answer_Idx": 1,
        "Explanation": "Variational Autoencoder。潜在空間を整理し、連続的なデータの生成を可能にします。",
        "Link": "https://example.com/vae"
    },
    {
        "ID": "DL-051",
        "Category": "4.ディープラーニングの概要",
        "Question": "物体検出において、予測した枠と正解の枠の「重なり具合」を示す指標は？",
        "Opt1": "RMSE",
        "Opt2": "F値",
        "Opt3": "IoU (Intersection over Union)",
        "Opt4": "MAE",
        "Opt5": "決定係数",
        "Opt6": "ジニ係数",
        "Answer_Idx": 2,
        "Explanation": "重なり面積を和集合の面積で割った値。0から1の値をとり、1に近いほど正確です。",
        "Link": "https://example.com/iou"
    },
    {
        "ID": "DL-052",
        "Category": "4.ディープラーニングの概要",
        "Question": "学習済みのモデルの重みを初期値として、新しい別のタスクで追加学習させることを何というか？",
        "Opt1": "事前学習",
        "Opt2": "ファインチューニング",
        "Opt3": "蒸留",
        "Opt4": "剪定",
        "Opt5": "量子化",
        "Opt6": "データ拡張",
        "Answer_Idx": 1,
        "Explanation": "既存の知識を活かすことで、少ないデータでも高い精度を出すことができます。",
        "Link": "https://example.com/fine-tuning"
    },
    {
        "ID": "DL-053",
        "Category": "4.ディープラーニングの概要",
        "Question": "ニューラルネットワークのパラメータを、本来の精度を大きく落とさずにビット数を下げて表現する技術は？",
        "Opt1": "蒸留",
        "Opt2": "量子化",
        "Opt3": "剪定",
        "Opt4": "正則化",
        "Opt5": "標準化",
        "Opt6": "正規化",
        "Answer_Idx": 1,
        "Explanation": "浮動小数点（32bit）を整数（8bit）などに変換し、モデルの軽量化・高速化を図ります。",
        "Link": "https://example.com/quantization"
    },
    {
        "ID": "DL-054",
        "Category": "4.ディープラーニングの概要",
        "Question": "モデルの重みのうち、値がゼロに近い不要な接続を削除して軽量化する技術は？",
        "Opt1": "蒸留",
        "Opt2": "量子化",
        "Opt3": "剪定（プルーニング）",
        "Opt4": "ドロップアウト",
        "Opt5": "バッチ正規化",
        "Opt6": "平坦化",
        "Answer_Idx": 2,
        "Explanation": "脳の神経回路が整理される過程に似た、モデル圧縮技術の一つです。",
        "Link": "https://example.com/pruning-dl"
    },
    {
        "ID": "DL-055",
        "Category": "4.ディープラーニングの概要",
        "Question": "「Attention Is All You Need」という論文で提案された、注意機構を多重化した仕組みは？",
        "Opt1": "Single-head Attention",
        "Opt2": "Multi-head Attention",
        "Opt3": "Cross Attention",
        "Opt4": "Self Attention",
        "Opt5": "Hard Attention",
        "Opt6": "Soft Attention",
        "Answer_Idx": 1,
        "Explanation": "複数の視点で「注意」を並列に計算することで、多様な文脈を捉えられます。",
        "Link": "https://example.com/multi-head-attention"
    },
    {
        "ID": "DL-056",
        "Category": "4.ディープラーニングの概要",
        "Question": "GANの学習において、生成器と識別器のバランスが崩れ、同じような画像しか生成されなくなる現象は？",
        "Opt1": "勾配消失",
        "Opt2": "過学習",
        "Opt3": "モード崩壊（Mode Collapse）",
        "Opt4": "次元の呪い",
        "Opt5": "ハルシネーション",
        "Opt6": "欠損",
        "Answer_Idx": 2,
        "Explanation": "生成器が識別器を騙しやすい特定のパターンに固執してしまう問題です。",
        "Link": "https://example.com/mode-collapse"
    },
    {
        "ID": "DL-057",
        "Category": "4.ディープラーニングの概要",
        "Question": "低解像度の画像から高解像度の画像を生成するタスクを何というか？",
        "Opt1": "セグメンテーション",
        "Opt2": "物体検出",
        "Opt3": "超解像 (Super-Resolution)",
        "Opt4": "画像分類",
        "Opt5": "スタイル変換",
        "Opt6": "異常検知",
        "Answer_Idx": 2,
        "Explanation": "SRCNNやSRGANなどのモデルがこのタスクに用いられます。",
        "Link": "https://example.com/super-resolution"
    },
    {
        "ID": "DL-058",
        "Category": "4.ディープラーニングの概要",
        "Question": "RNNにおいて、現時刻の入力と1つ前の時刻の隠れ状態から次の隠れ状態を計算する関数を何というか？",
        "Opt1": "活性化関数",
        "Opt2": "遷移関数",
        "Opt3": "損失関数",
        "Opt4": "出力関数",
        "Opt5": "基底関数",
        "Opt6": "カーネル関数",
        "Answer_Idx": 1,
        "Explanation": "この再帰的な構造により、時系列情報の伝播が可能になります。",
        "Link": "https://example.com/rnn-transition"
    },
    {
        "ID": "DL-059",
        "Category": "4.ディープラーニングの概要",
        "Question": "画像の特徴マップのチャネルごとに重みを付け、重要なチャネルを強調する仕組み（SEBlock等）は？",
        "Opt1": "空間Attention",
        "Opt2": "チャネルAttention",
        "Opt3": "自己注意機構",
        "Opt4": "スキップ接続",
        "Opt5": "ドロップアウト",
        "Opt6": "バッチ正規化",
        "Answer_Idx": 1,
        "Explanation": "「何が映っているか（チャネル）」に注目して特徴を際立たせます。",
        "Link": "https://example.com/channel-attention"
    },
    {
        "ID": "DL-060",
        "Category": "4.ディープラーニングの概要",
        "Question": "セマンティックセグメンテーションでよく使われる、畳み込み層と逆畳み込み層を対称に配置したモデルは？",
        "Opt1": "VGG",
        "Opt2": "ResNet",
        "Opt3": "U-Net",
        "Opt4": "AlexNet",
        "Opt5": "YOLO",
        "Opt6": "RNN",
        "Answer_Idx": 2,
        "Explanation": "医療画像解析などで非常に高い実績を持つ、アルファベットの「U」字型の構造です。",
        "Link": "https://example.com/u-net"
    },
    {
        "ID": "DL-061",
        "Category": "4.ディープラーニングの概要",
        "Question": "シグモイド関数やtanh関数を活性化関数に用いる際、推奨される重みの初期化手法は？",
        "Opt1": "Heの初期値",
        "Opt2": "Xavier（ザビエル）の初期値",
        "Opt3": "ゼロ初期化",
        "Opt4": "定数初期化",
        "Opt5": "ランダム初期化",
        "Opt6": "正規分布初期化",
        "Answer_Idx": 1,
        "Explanation": "前層のノード数の平方根で割ることで、活性化後の値の分散を適度な範囲に保ちます。",
        "Link": "https://example.com/xavier-init"
    },
    {
        "ID": "DL-062",
        "Category": "4.ディープラーニングの概要",
        "Question": "ReLU関数を活性化関数に用いる際、勾配消失を防ぐために推奨される重みの初期化手法は？",
        "Opt1": "Xavierの初期値",
        "Opt2": "Heの初期値",
        "Opt3": "ゼロ初期化",
        "Opt4": "1固定初期化",
        "Opt5": "一様分布初期化",
        "Opt6": "バイアス初期化",
        "Answer_Idx": 1,
        "Explanation": "ReLUでは半分が0になるため、Xavierより大きな係数（$\\sqrt{2/n}$）を用いて分散を調整します。",
        "Link": "https://example.com/he-init"
    },
    {
        "ID": "DL-063",
        "Category": "4.ディープラーニングの概要",
        "Question": "バッチ正規化において、学習時ではなく「推論時」に使用される統計量はどれか？",
        "Opt1": "各ミニバッチの平均",
        "Opt2": "移動平均（学習時の平均の蓄積）",
        "Opt3": "常に0",
        "Opt4": "テストデータの平均",
        "Opt5": "直前のデータの平均",
        "Opt6": "ランダムな値",
        "Answer_Idx": 1,
        "Explanation": "推論時には単一データが来ることもあるため、学習時に計算しておいた移動平均（Running Mean）を使用します。",
        "Link": "https://example.com/batch-norm-inference"
    },
    {
        "ID": "DL-064",
        "Category": "4.ディープラーニングの概要",
        "Question": "ミニバッチのサイズに依存せず、各データ（サンプル）内の各層のノード間で正規化を行う手法は？",
        "Opt1": "Batch Normalization",
        "Opt2": "Layer Normalization",
        "Opt3": "Group Normalization",
        "Opt4": "Instance Normalization",
        "Opt5": "Weight Normalization",
        "Opt6": "Local Response Normalization",
        "Answer_Idx": 1,
        "Explanation": "Transformerなどの自然言語処理モデルでよく用いられます。",
        "Link": "https://example.com/layer-norm"
    },
    {
        "ID": "DL-065",
        "Category": "4.ディープラーニングの概要",
        "Question": "入力が負のとき、0ではなく非常に小さな値を返すことで「デッドReLU（更新停止）」を防ぐ関数は？",
        "Opt1": "Sigmoid",
        "Opt2": "tanh",
        "Opt3": "Leaky ReLU",
        "Opt4": "Softmax",
        "Opt5": "Step",
        "Opt6": "ReLU",
        "Answer_Idx": 2,
        "Explanation": "負の領域にわずかな傾き（0.01など）を持たせることで、学習が完全に止まるのを防ぎます。",
        "Link": "https://example.com/leaky-relu"
    },
    {
        "ID": "DL-066",
        "Category": "4.ディープラーニングの概要",
        "Question": "ReLUの滑らかな近似版で、自己正規化の性質を持つ活性化関数はどれか？",
        "Opt1": "Leaky ReLU",
        "Opt2": "ELU（Exponential Linear Unit）",
        "Opt3": "Swish",
        "Opt4": "Mish",
        "Opt5": "Step",
        "Opt6": "Linear",
        "Answer_Idx": 1,
        "Explanation": "負の値に対しても指数関数的に反応し、ノイズに対して頑健な性質を持ちます。",
        "Link": "https://example.com/elu"
    },
    {
        "ID": "DL-067",
        "Category": "4.ディープラーニングの概要",
        "Question": "Googleが考案した、$f(x) = x \\cdot \\text{sigmoid}(\\beta x)$ で表される、ReLUより高精度な活性化関数は？",
        "Opt1": "Mish",
        "Opt2": "Swish",
        "Opt3": "GELU",
        "Opt4": "Softsign",
        "Opt5": "Softplus",
        "Opt6": "LogSumExp",
        "Answer_Idx": 1,
        "Explanation": "シグモイド関数を組み合わせた複雑な形状をしており、勾配が滑らかになるのが特徴です。",
        "Link": "https://example.com/swish"
    },
    {
        "ID": "DL-068",
        "Category": "4.ディープラーニングの概要",
        "Question": "BERTなどの大規模言語モデルで一般的に採用されている、ガウス分布に基づいた活性化関数は？",
        "Opt1": "GELU（Gaussian Error Linear Unit）",
        "Opt2": "ReLU",
        "Opt3": "Leaky ReLU",
        "Opt4": "tanh",
        "Opt5": "Sigmoid",
        "Opt6": "Maxout",
        "Answer_Idx": 0,
        "Explanation": "確率的な性質を取り入れた活性化関数で、現在のLLMの標準となっています。",
        "Link": "https://example.com/gelu"
    },
    {
        "ID": "DL-069",
        "Category": "4.ディープラーニングの概要",
        "Question": "ニューラルネットワークにおいて、すべての重みを「0」で初期化するとどうなるか？",
        "Opt1": "学習が非常に速くなる",
        "Opt2": "すべてのノードが同じ更新をしてしまい、学習が実質不可能になる",
        "Opt3": "過学習が完全に防げる",
        "Opt4": "勾配爆発が起きる",
        "Opt5": "次元圧縮が自動で行われる",
        "Opt6": "正則化と同じ効果がある",
        "Answer_Idx": 1,
        "Explanation": "対称性を壊すことができないため、多層の意味がなくなってしまいます。",
        "Link": "https://example.com/zero-init-problem"
    },
    {
        "ID": "DL-070",
        "Category": "4.ディープラーニングの概要",
        "Question": "学習率を一定期間ごとに半分にしたり、特定の倍率で減衰させたりする手法を何というか？",
        "Opt1": "重み減衰",
        "Opt2": "学習率スケジューリング",
        "Opt3": "早期終了",
        "Opt4": "データ拡張",
        "Opt5": "正則化",
        "Opt6": "標準化",
        "Answer_Idx": 1,
        "Explanation": "学習の初期は大きく動き、終盤は細かく調整するために学習率を減衰（Decay）させます。",
        "Link": "https://example.com/lr-scheduling"
    },
    {
        "ID": "DL-071",
        "Category": "4.ディープラーニングの概要",
        "Question": "勾配が非常に大きくなり、学習が不安定になるのを防ぐために、勾配に上限を設ける手法は？",
        "Opt1": "Gradient Clipping",
        "Opt2": "Weight Decay",
        "Opt3": "Dropout",
        "Opt4": "Early Stopping",
        "Opt5": "Batch Norm",
        "Opt6": "Layer Norm",
        "Answer_Idx": 0,
        "Explanation": "RNNなどの勾配が蓄積しやすいモデルで、勾配爆発を抑えるために必須のテクニックです。",
        "Link": "https://example.com/gradient-clipping"
    },
    {
        "ID": "DL-072",
        "Category": "4.ディープラーニングの概要",
        "Question": "学習データとテストデータの分布が異なることにより、学習済みモデルの精度が低下することを何というか？",
        "Opt1": "過学習",
        "Opt2": "バイアスエラー",
        "Opt3": "共変量シフト（Covariate Shift）",
        "Opt4": "次元の呪い",
        "Opt5": "ハルシネーション",
        "Opt6": "モード崩壊",
        "Answer_Idx": 2,
        "Explanation": "入力データの統計的な性質が、運用時（テスト時）に変化してしまう問題です。",
        "Link": "https://example.com/covariate-shift"
    },
    {
        "ID": "DL-073",
        "Category": "4.ディープラーニングの概要",
        "Question": "ディープラーニングにおいて、データ1件ごとに学習を繰り返す手法はどれか？",
        "Opt1": "ミニバッチ学習",
        "Opt2": "バッチ学習",
        "Opt3": "オンライン学習（逐次学習）",
        "Opt4": "転移学習",
        "Opt5": "能動学習",
        "Opt6": "教師なし学習",
        "Answer_Idx": 2,
        "Explanation": "メモリ消費は少ないですが、ノイズに影響を受けやすく学習が不安定になることがあります。",
        "Link": "https://example.com/online-learning"
    },
    {
        "ID": "DL-074",
        "Category": "4.ディープラーニングの概要",
        "Question": "CNNで特徴抽出後、複数のチャネルをまたいで画素ごとに畳み込みを行い、チャネル数を削減する手法は？",
        "Opt1": "Depthwise畳み込み",
        "Opt2": "1x1畳み込み（Pointwise畳み込み）",
        "Opt3": "アトラス畳み込み",
        "Opt4": "逆畳み込み",
        "Opt5": "プーリング",
        "Opt6": "パディング",
        "Answer_Idx": 1,
        "Explanation": "パラメータ数を抑えつつ、特徴の非線形な組み合わせを可能にします。",
        "Link": "https://example.com/one-by-one-conv"
    },
    {
        "ID": "DL-075",
        "Category": "4.ディープラーニングの概要",
        "Question": "畳み込み演算を「空間方向」と「チャネル方向」に分解して計算量を劇的に削減する手法は？",
        "Opt1": "転置畳み込み",
        "Opt2": "Depthwise Separable Convolution",
        "Opt3": "Dilated Convolution",
        "Opt4": "フル畳み込み",
        "Opt5": "ResNet",
        "Opt6": "Inception",
        "Answer_Idx": 1,
        "Explanation": "MobileNetなどで採用されており、エッジデバイスでの実行に欠かせない技術です。",
        "Link": "https://example.com/dw-sep-conv"
    },
    {
        "ID": "DL-076",
        "Category": "4.ディープラーニングの概要",
        "Question": "通常の畳み込みに隙間を空けて適用し、パラメータ数を増やさずに受容野（見える範囲）を広げる手法は？",
        "Opt1": "Dilated Convolution",
        "Opt2": "Deconvolution",
        "Opt3": "Unpooling",
        "Opt4": "DenseBlock",
        "Opt5": "BottleNeck",
        "Opt6": "Global Average Pooling",
        "Answer_Idx": 0,
        "Explanation": "空洞畳み込みとも呼ばれ、セグメンテーションなどで広い範囲の文脈を捉えるのに有効です。",
        "Link": "https://example.com/dilated-conv"
    },
    {
        "ID": "DL-077",
        "Category": "4.ディープラーニングの概要",
        "Question": "CNNの最後で全結合層を使わず、各チャネルの平均値をとることでパラメータを大幅に削減する層は？",
        "Opt1": "Max Pooling",
        "Opt2": "Average Pooling",
        "Opt3": "Global Average Pooling (GAP)",
        "Opt4": "Flatten",
        "Opt5": "Softmax",
        "Opt6": "Batch Norm",
        "Answer_Idx": 2,
        "Explanation": "過学習を抑え、入力画像のサイズ変化にも柔軟に対応できるようになります。",
        "Link": "https://example.com/gap"
    },
    {
        "ID": "DL-078",
        "Category": "4.ディープラーニングの概要",
        "Question": "ニューラルネットワークのパラメータの一部を、学習中に定数として固定し、更新しないことを何というか？",
        "Opt1": "ドロップアウト",
        "Opt2": "プルーニング",
        "Opt3": "フリーズ（重みの凍結）",
        "Opt4": "転移学習",
        "Opt5": "量子化",
        "Opt6": "白色化",
        "Answer_Idx": 2,
        "Explanation": "転移学習において、既存の知識を壊さないために下位層の学習を止める際によく行われます。",
        "Link": "https://example.com/freezing"
    },
    {
        "ID": "DL-079",
        "Category": "4.ディープラーニングの概要",
        "Question": "ネットワークの出力を入力と同じにするのではなく、出力と入力の「差分（残差）」を学習させる構造は？",
        "Opt1": "スキップ接続（Shortcut Connection）",
        "Opt2": "プーリング接続",
        "Opt3": "全結合接続",
        "Opt4": "回帰接続",
        "Opt5": "カプセルネットワーク",
        "Opt6": "Transformer",
        "Answer_Idx": 0,
        "Explanation": "ResNetの核となる技術で、勾配を直接手前の層に流すことができます。",
        "Link": "https://example.com/skip-connection"
    },
    {
        "ID": "DL-080",
        "Category": "4.ディープラーニングの概要",
        "Question": "誤差逆伝播法において、損失関数を入力まで辿る際に用いられる、微分の合成関数の性質は？",
        "Opt1": "ガウスの法則",
        "Opt2": "チェインルール（連鎖律）",
        "Opt3": "テーラー展開",
        "Opt4": "ピタゴラスの定理",
        "Opt5": "大数の法則",
        "Opt6": "ベイズの定理",
        "Answer_Idx": 1,
        "Explanation": "各層の局所的な導関数を掛け合わせていくことで、ネットワーク全体の勾配を求めます。",
        "Link": "https://example.com/chain-rule"
    },
    {
        "ID": "DL-081",
        "Category": "4.ディープラーニングの概要",
        "Question": "物体検出において、候補領域の抽出とクラス分類を2段階に分けて行う手法の総称は？",
        "Opt1": "1段階検出（One-stage）",
        "Opt2": "2段階検出（Two-stage）",
        "Opt3": "アンサンブル法",
        "Opt4": "回帰型検出",
        "Opt5": "セグメンテーション",
        "Opt6": "自己符号化",
        "Answer_Idx": 1,
        "Explanation": "R-CNNシリーズが代表的です。精度は高いですが、計算速度は1段階（YOLO等）に劣ります。",
        "Link": "https://example.com/two-stage-det"
    },
    {
        "ID": "DL-082",
        "Category": "4.ディープラーニングの概要",
        "Question": "物体検出で、あらかじめ設定された様々なサイズ・比率の枠（領域候補）を何というか？",
        "Opt1": "アンカーボックス",
        "Opt2": "バウンディングボックス",
        "Opt3": "フィルタ",
        "Opt4": "カーネル",
        "Opt5": "セントロイド",
        "Opt6": "スロット",
        "Answer_Idx": 0,
        "Explanation": "Faster R-CNNやSSDなどで、効率的に物体を捉えるための基準枠として使われます。",
        "Link": "https://example.com/anchor-box"
    },
    {
        "ID": "DL-083",
        "Category": "4.ディープラーニングの概要",
        "Question": "物体検出において、重複して検出された枠の中から、最もスコアが高いものだけを残す処理は？",
        "Opt1": "NMS（非最大値抑制）",
        "Opt2": "RoIプーリング",
        "Opt3": "パディング",
        "Opt4": "正規化",
        "Opt5": "データ拡張",
        "Opt6": "蒸留",
        "Answer_Idx": 0,
        "Explanation": "Non-Maximum Suppression。1つの物体に対して複数の枠が出るのを防ぎます。",
        "Link": "https://example.com/nms"
    },
    {
        "ID": "DL-084",
        "Category": "4.ディープラーニングの概要",
        "Question": "異なる解像度の特徴マップを組み合わせることで、小さい物体の検出精度を高める構造は？",
        "Opt1": "FPN（Feature Pyramid Network）",
        "Opt2": "VGG",
        "Opt3": "AlexNet",
        "Opt4": "RNN",
        "Opt5": "LSTM",
        "Opt6": "GRU",
        "Answer_Idx": 0,
        "Explanation": "低解像度の強い特徴と、高解像度の細かな情報を融合させる技術です。",
        "Link": "https://example.com/fpn"
    },
    {
        "ID": "DL-085",
        "Category": "4.ディープラーニングの概要",
        "Question": "画像内の個々の物体を検出し、さらにピクセル単位で塗り分ける（セグメンテーションする）モデルは？",
        "Opt1": "YOLO",
        "Opt2": "SSD",
        "Opt3": "Mask R-CNN",
        "Opt4": "U-Net",
        "Opt5": "DeepLab",
        "Opt6": "ResNet",
        "Answer_Idx": 2,
        "Explanation": "Faster R-CNNにセグメンテーション用の枝（マスクブランチ）を追加したモデルです。",
        "Link": "https://example.com/mask-rcnn"
    },
    {
        "ID": "DL-086",
        "Category": "4.ディープラーニングの概要",
        "Question": "TransformerのSelf-Attentionにおいて、単語自身の情報を表すベクトルを何というか？",
        "Opt1": "Query（クエリ）",
        "Opt2": "Key（キー）",
        "Opt3": "Value（バリュー）",
        "Opt4": "Context（コンテキスト）",
        "Opt5": "Bias（バイアス）",
        "Opt6": "Weight（重み）",
        "Answer_Idx": 0,
        "Explanation": "Queryが「何を探しているか」、Keyが「どんな情報を持っているか」に対応します。",
        "Link": "https://example.com/qkv-attention"
    },
    {
        "ID": "DL-087",
        "Category": "4.ディープラーニングの概要",
        "Question": "Self-Attentionにおいて、QueryとKeyの関連度（内積）に基づいて、実際に取り出される情報は？",
        "Opt1": "Query",
        "Opt2": "Key",
        "Opt3": "Value",
        "Opt4": "Gradient",
        "Opt5": "Loss",
        "Opt6": "Entropy",
        "Answer_Idx": 2,
        "Explanation": "QueryとKeyの類似度が高いほど、対応するValue（値）が強く出力に反映されます。",
        "Link": "https://example.com/qkv-mechanism"
    },
    {
        "ID": "DL-088",
        "Category": "4.ディープラーニングの概要",
        "Question": "Transformerのデコーダにおいて、まだ生成していない未来の単語を見ないようにする仕組みは？",
        "Opt1": "パディング",
        "Opt2": "ストライド",
        "Opt3": "マスキング（Masked Attention）",
        "Opt4": "ドロップアウト",
        "Opt5": "プーリング",
        "Opt6": "正則化",
        "Answer_Idx": 2,
        "Explanation": "自分より後ろの単語への注意を0にすることで、カンニングを防ぎます。",
        "Link": "https://example.com/masked-attention"
    },
    {
        "ID": "DL-089",
        "Category": "4.ディープラーニングの概要",
        "Question": "自然言語処理だけでなく、画像をパッチに分割してTransformerを適用するモデルを何というか？",
        "Opt1": "ViT (Vision Transformer)",
        "Opt2": "CNN",
        "Opt3": "GAN",
        "Opt4": "Autoencoder",
        "Opt5": "U-Net",
        "Opt6": "RNN",
        "Answer_Idx": 0,
        "Explanation": "画像も系列データとして扱うことで、CNNを凌駕する性能を示すようになりました。",
        "Link": "https://example.com/vit"
    },
    {
        "ID": "DL-090",
        "Category": "4.ディープラーニングの概要",
        "Question": "GANにおいて、学習が安定しない原因の一つである、損失関数の勾配が消失する問題を解決した手法は？",
        "Opt1": "WGAN (Wasserstein GAN)",
        "Opt2": "CycleGAN",
        "Opt3": "StyleGAN",
        "Opt4": "Pix2Pix",
        "Opt5": "StackGAN",
        "Opt6": "DCGAN",
        "Answer_Idx": 0,
        "Explanation": "距離の指標を工夫することで、生成器に適切な勾配が伝わりやすくしています。",
        "Link": "https://example.com/wgan"
    },
    {
        "ID": "DL-091",
        "Category": "4.ディープラーニングの概要",
        "Question": "対になっている画像がなくても（例：馬をシマウマに）、ドメイン間の変換を学習できるGANは？",
        "Opt1": "DCGAN",
        "Opt2": "CycleGAN",
        "Opt3": "Pix2Pix",
        "Opt4": "BigGAN",
        "Opt5": "ProGAN",
        "Opt6": "StyleGAN",
        "Answer_Idx": 1,
        "Explanation": "Cycle Consistency Lossを用いることで、非対データからの学習を可能にしました。",
        "Link": "https://example.com/cycle-gan"
    },
    {
        "ID": "DL-092",
        "Category": "4.ディープラーニングの概要",
        "Question": "データにノイズを少しずつ加えていき、その逆工程（除去）を学ぶことで生成を行うモデルは？",
        "Opt1": "GAN",
        "Opt2": "VAE",
        "Opt3": "拡散モデル (Diffusion Model)",
        "Opt4": "RNN",
        "Opt5": "LSTM",
        "Opt6": "CNN",
        "Answer_Idx": 2,
        "Explanation": "Stable Diffusionなどの画像生成AIの基盤となっている非常に強力な生成モデルです。",
        "Link": "https://example.com/diffusion-model"
    },
    {
        "ID": "DL-093",
        "Category": "4.ディープラーニングの概要",
        "Question": "Transformerで採用されている、層を飛び越えて情報を伝える「スキップ接続」の後に必ず行われる処理は？",
        "Opt1": "Layer Normalization",
        "Opt2": "Batch Normalization",
        "Opt3": "Dropout",
        "Opt4": "Softmax",
        "Opt5": "ReLU",
        "Opt6": "Sigmoid",
        "Answer_Idx": 0,
        "Explanation": "Add & Normという構造で、学習の安定化を図ります。",
        "Link": "https://example.com/transformer-norm"
    },
    {
        "ID": "DL-094",
        "Category": "4.ディープラーニングの概要",
        "Question": "大規模モデルのパラメータ数を減らさず、一部の小さなモジュール（Adapter等）だけを学習させる手法は？",
        "Opt1": "フルチューニング",
        "Opt2": "PEFT（効率的パラメータ微調整）",
        "Opt3": "蒸留",
        "Opt4": "量子化",
        "Opt5": "剪定",
        "Opt6": "正則化",
        "Answer_Idx": 1,
        "Explanation": "LoRA（Low-Rank Adaptation）などが代表例で、少ない資源で微調整が可能です。",
        "Link": "https://example.com/peft"
    },
    {
        "ID": "DL-095",
        "Category": "4.ディープラーニングの概要",
        "Question": "学習データにはない「未知のクラス」を、数個のサンプル（あるいは0個）だけで認識する手法は？",
        "Opt1": "Few-shot Learning / Zero-shot Learning",
        "Opt2": "転移学習",
        "Opt3": "能動学習",
        "Opt4": "半教師あり学習",
        "Opt5": "強化学習",
        "Opt6": "次元圧縮",
        "Answer_Idx": 0,
        "Explanation": "CLIPなどのマルチモーダルモデルにより、この能力が飛躍的に向上しました。",
        "Link": "https://example.com/few-shot"
    },
    {
        "ID": "DL-096",
        "Category": "4.ディープラーニングの概要",
        "Question": "画像とテキストを同じベクトル空間に写像し、両者の関連性を学習するモデルの代表例は？",
        "Opt1": "BERT",
        "Opt2": "ResNet",
        "Opt3": "CLIP",
        "Opt4": "VGG",
        "Opt5": "YOLO",
        "Opt6": "U-Net",
        "Answer_Idx": 2,
        "Explanation": "OpenAIが開発したモデルで、画像生成AIのプロンプト理解などにも使われています。",
        "Link": "https://example.com/clip"
    },
    {
        "ID": "DL-097",
        "Category": "4.ディープラーニングの概要",
        "Question": "CNNにおいて、特定のチャネル（色や特定の模様）が画像のどこに強く反応しているかを示す図は？",
        "Opt1": "散布図",
        "Opt2": "ヒストグラム",
        "Opt3": "特徴マップ（Feature Map）",
        "Opt4": "デンドログラム",
        "Opt5": "ROC曲線",
        "Opt6": "損失曲線",
        "Answer_Idx": 2,
        "Explanation": "層が深くなるほど、エッジなどの単純な特徴から、顔や物体などの複雑な特徴へ変化します。",
        "Link": "https://example.com/feature-map"
    },
    {
        "ID": "DL-098",
        "Category": "4.ディープラーニングの概要",
        "Question": "ニューラルネットワークのパラメータ更新を、勾配の「移動平均」を用いて適応的に行うOptimizerは？",
        "Opt1": "SGD",
        "Opt2": "Momentum",
        "Opt3": "RMSprop",
        "Opt4": "AdaGrad",
        "Opt5": "ステップ関数",
        "Opt6": "ラグランジュ",
        "Answer_Idx": 2,
        "Explanation": "AdaGradで学習率が下がりすぎる問題を解決した手法です。",
        "Link": "https://example.com/rmsprop"
    },
    {
        "ID": "DL-099",
        "Category": "4.ディープラーニングの概要",
        "Question": "モデルの予測結果の「偏り」を評価するために、クラスごとの正解率の平均をとる指標は？",
        "Opt1": "Overall Accuracy",
        "Opt2": "Balanced Accuracy",
        "Opt3": "MSE",
        "Opt4": "RMSE",
        "Opt5": "L1ノルム",
        "Opt6": "L2ノルム",
        "Answer_Idx": 1,
        "Explanation": "不均衡データにおいて、多数派クラスに引きずられない評価が可能です。",
        "Link": "https://example.com/balanced-accuracy"
    },
    {
        "ID": "DL-100",
        "Category": "4.ディープラーニングの概要",
        "Question": "深層学習において、特定の入力に対して意図的に誤認を誘発するように加工されたデータを何というか？",
        "Opt1": "ビッグデータ",
        "Opt2": "ノイズデータ",
        "Opt3": "敵対的サンプル (Adversarial Examples)",
        "Opt4": "教師データ",
        "Opt5": "欠損データ",
        "Opt6": "構造化データ",
        "Answer_Idx": 2,
        "Explanation": "人間に見えない程度の微小なノイズで、AIを騙す攻撃（敵対的攻撃）に使われます。",
        "Link": "https://example.com/adversarial-examples"
    },
    {
        "ID": "DL-101",
        "Category": "4.ディープラーニングの概要",
        "Question": "画像認識モデルが「画像のどこを見て判断したか」を、勾配情報を用いてヒートマップで可視化する手法は？",
        "Opt1": "LIME",
        "Opt2": "SHAP",
        "Opt3": "Grad-CAM",
        "Opt4": "t-SNE",
        "Opt5": "PCA",
        "Opt6": "Attention",
        "Answer_Idx": 2,
        "Explanation": "畳み込み層の最終出力の勾配を利用して、判断根拠となった領域を特定します。",
        "Link": "https://example.com/grad-cam"
    },
    {
        "ID": "DL-102",
        "Category": "4.ディープラーニングの概要",
        "Question": "強化学習のQ学習にディープラーニングを組み合わせ、ゲーム画面（画像）から直接行動を学習可能にしたモデルは？",
        "Opt1": "RNN",
        "Opt2": "CNN",
        "Opt3": "DQN（Deep Q-Network）",
        "Opt4": "GAN",
        "Opt5": "Transformer",
        "Opt6": "VGG",
        "Answer_Idx": 2,
        "Explanation": "DeepMindが発表し、インベーダーゲーム等で人間超えのスコアを出して話題になりました。",
        "Link": "https://example.com/dqn-intro"
    },
    {
        "ID": "DL-103",
        "Category": "4.ディープラーニングの概要",
        "Question": "DQNにおいて、過去の経験（状態・行動・報酬）を保存し、ランダムに取り出して学習に使う手法を何というか？",
        "Opt1": "Experience Replay",
        "Opt2": "Target Network",
        "Opt3": "Fixed Q-Target",
        "Opt4": "Self-Attention",
        "Opt5": "Dropout",
        "Opt6": "Data Augmentation",
        "Answer_Idx": 0,
        "Explanation": "データの相関を断ち切り、学習を安定させるための非常に重要な技術です。",
        "Link": "https://example.com/experience-replay"
    },
    {
        "ID": "DL-104",
        "Category": "4.ディープラーニングの概要",
        "Question": "DQNの学習を安定させるため、重みを更新するネットワークとは別に、価値を計算するための一時的なネットワークを用いる手法は？",
        "Opt1": "Experience Replay",
        "Opt2": "Target Network",
        "Opt3": "Dropout",
        "Opt4": "Batch Norm",
        "Opt5": "ResNet",
        "Opt6": "RNN",
        "Answer_Idx": 1,
        "Explanation": "学習中の目標値が頻繁に動くのを防ぎ、収束を早める効果があります。",
        "Link": "https://example.com/target-network"
    },
    {
        "ID": "DL-105",
        "Category": "4.ディープラーニングの概要",
        "Question": "「画像」と「テキスト」など、種類の異なるデータを組み合わせて同時に学習する手法を何というか？",
        "Opt1": "マルチタスク学習",
        "Opt2": "マルチモーダル学習",
        "Opt3": "アンサンブル学習",
        "Opt4": "転移学習",
        "Opt5": "蒸留",
        "Opt6": "量子化",
        "Answer_Idx": 1,
        "Explanation": "異なる種類のデータ（モダリティ）を統合して、より高度な判断や生成を行います。",
        "Link": "https://example.com/multi-modal"
    },
    {
        "ID": "DL-106",
        "Category": "4.ディープラーニングの概要",
        "Question": "Googleが開発した、Transformerのエンコーダのみを使用し、文脈を双方向に考慮して事前学習する言語モデルは？",
        "Opt1": "GPT",
        "Opt2": "BERT",
        "Opt3": "RoBERTa",
        "Opt4": "T5",
        "Opt5": "Llama",
        "Opt6": "Claude",
        "Answer_Idx": 1,
        "Explanation": "Bidirectional Encoder Representations from Transformersの略。穴埋め問題形式で学習します。",
        "Link": "https://example.com/bert"
    },
    {
        "ID": "DL-107",
        "Category": "4.ディープラーニングの概要",
        "Question": "OpenAIが開発した、Transformerのデコーダのみを使用し、次の単語を予測するように学習する言語モデルのシリーズは？",
        "Opt1": "BERT",
        "Opt2": "ResNet",
        "Opt3": "GPT (Generative Pre-trained Transformer)",
        "Opt4": "VGG",
        "Opt5": "YOLO",
        "Opt6": "U-Net",
        "Answer_Idx": 2,
        "Explanation": "文章生成に特化しており、ChatGPTなどの基盤となっています。",
        "Link": "https://example.com/gpt"
    },
    {
        "ID": "DL-108",
        "Category": "4.ディープラーニングの概要",
        "Question": "言語モデルにおいて、膨大なパラメータ数を持つモデルが特定のサイズを超えると、急激に知能（能力）が向上する現象を何というか？",
        "Opt1": "ダブルディセント",
        "Opt2": "能力の創発 (Emergent Abilities)",
        "Opt3": "モード崩壊",
        "Opt4": "勾配消失",
        "Opt5": "次元の呪い",
        "Opt6": "フレーム問題",
        "Answer_Idx": 1,
        "Explanation": "小規模モデルにはなかった推論能力や計算能力が、大規模化により突如現れることを指します。",
        "Link": "https://example.com/emergence"
    },
    {
        "ID": "DL-109",
        "Category": "4.ディープラーニングの概要",
        "Question": "モデルの重みなどの「パラメータ数」、学習に使う「データ量」、計算に使う「計算資源」の3つのバランスで精度が決まる法則は？",
        "Opt1": "ムーアの法則",
        "Opt2": "スケーリング則（べき乗則）",
        "Opt3": "ジップの法則",
        "Opt4": "パレートの法則",
        "Opt5": "アムダールの法則",
        "Opt6": "大数の法則",
        "Answer_Idx": 1,
        "Explanation": "性能はこれら3要素のべき乗に比例して向上するという法則で、LLM巨大化の根拠となりました。",
        "Link": "https://example.com/scaling-law"
    },
    {
        "ID": "DL-110",
        "Category": "4.ディープラーニングの概要",
        "Question": "LLMにおいて、モデルの重みを変更せずに、プロンプトに数個の例示を含めるだけで回答精度を高める手法は？",
        "Opt1": "ファインチューニング",
        "Opt2": "In-Context Learning (Few-shot prompting)",
        "Opt3": "蒸留",
        "Opt4": "量子化",
        "Opt5": "継続学習",
        "Opt6": "能動学習",
        "Answer_Idx": 1,
        "Explanation": "文脈の中で学習（推論）させる手法で、モデルの更新を必要としません。",
        "Link": "https://example.com/icl"
    },
    {
        "ID": "DL-111",
        "Category": "4.ディープラーニングの概要",
        "Question": "生成AIにおいて、学習データにない情報をあたかも真実のように生成してしまう現象を何というか？",
        "Opt1": "モード崩壊",
        "Opt2": "ハルシネーション（幻覚）",
        "Opt3": "オーバーフィッティング",
        "Opt4": "勾配消失",
        "Opt5": "正則化",
        "Opt6": "スムージング",
        "Answer_Idx": 1,
        "Explanation": "もっともらしい嘘をつく現象で、正確性が求められる用途での大きな課題です。",
        "Link": "https://example.com/hallucination"
    },
    {
        "ID": "DL-112",
        "Category": "4.ディープラーニングの概要",
        "Question": "言語モデルが人間にとって望ましい（安全で役立つ）回答をするように、人間のフィードバックを用いて微調整する手法は？",
        "Opt1": "RLHF（人間のフィードバックによる強化学習）",
        "Opt2": "自己教師あり学習",
        "Opt3": "蒸留",
        "Opt4": "剪定",
        "Opt5": "データ拡張",
        "Opt6": "標準化",
        "Answer_Idx": 0,
        "Explanation": "Reinforcement Learning from Human Feedback。ChatGPTの調整に不可欠な技術です。",
        "Link": "https://example.com/rlhf"
    },
    {
        "ID": "DL-113",
        "Category": "4.ディープラーニングの概要",
        "Question": "入力されたプロンプトを外部の知識ベース（検索エンジン等）と連携させ、最新情報や専門情報を回答に含める手法は？",
        "Opt1": "RLHF",
        "Opt2": "RAG（検索拡張生成）",
        "Opt3": "PEFT",
        "Opt4": "LoRA",
        "Opt5": "Transformer",
        "Opt6": "BERT",
        "Answer_Idx": 1,
        "Explanation": "Retrieval-Augmented Generation。モデル自体が知らない最新の事実を外部から補います。",
        "Link": "https://example.com/rag"
    },
    {
        "ID": "DL-114",
        "Category": "4.ディープラーニングの概要",
        "Question": "Transformerにおいて、単語同士の関係性を計算する際に「内積」を用いるが、計算結果を安定させるために行う処理は？",
        "Opt1": "スケーリング（平方根で割る）",
        "Opt2": "パディング",
        "Opt3": "ストライド",
        "Opt4": "量子化",
        "Opt5": "剪定",
        "Opt6": "正則化",
        "Answer_Idx": 0,
        "Explanation": "Scaled Dot-Product Attention。内積が大きくなりすぎて勾配が小さくなるのを防ぎます。",
        "Link": "https://example.com/scaled-dot-product"
    },
    {
        "ID": "DL-115",
        "Category": "4.ディープラーニングの概要",
        "Question": "モデルの予測結果の確実性を、確率ではなく「距離（マージン）」によって表現する分類器は？",
        "Opt1": "ロジスティック回帰",
        "Opt2": "ソフトマックス関数",
        "Opt3": "SVM（サポートベクターマシン）",
        "Opt4": "K-means",
        "Opt5": "決定木",
        "Opt6": "ナイーブベイズ",
        "Answer_Idx": 2,
        "Explanation": "SVMは「いかに境界から離れているか」というマージン最大化を目的とします。",
        "Link": "https://example.com/svm-margin"
    },
    {
        "ID": "DL-116",
        "Category": "4.ディープラーニングの概要",
        "Question": "深層学習のライブラリにおいて、計算グラフを構築してからデータを流す方式（TensorFlow1.x等）を何というか？",
        "Opt1": "Define-by-Run",
        "Opt2": "Define-and-Run",
        "Opt3": "Online Learning",
        "Opt4": "Transfer Learning",
        "Opt5": "Active Learning",
        "Opt6": "Zero-shot",
        "Answer_Idx": 1,
        "Explanation": "静的な計算グラフを構築するため最適化しやすいですが、デバッグが難しい側面があります。",
        "Link": "https://example.com/define-and-run"
    },
    {
        "ID": "DL-117",
        "Category": "4.ディープラーニングの概要",
        "Question": "データの入力を受けるたびに動的に計算グラフを構築する方式（PyTorch",
        "Opt1": "Chainer等）を何というか？",
        "Opt2": "Define-and-Run",
        "Opt3": "Define-by-Run",
        "Opt4": "Static Graph",
        "Opt5": "Batch Learning",
        "Opt6": "Batch Norm",
        "Answer_Idx": "Layer Norm",
        "Explanation": 1,
        "Link": "プログラムの実行順にグラフが作られるため、デバッグが容易で柔軟な実装が可能です。"
    },
    {
        "ID": "DL-118",
        "Category": "4.ディープラーニングの概要",
        "Question": "画像から「文章（キャプション）」を生成したり、逆に「テキスト」から「画像」を生成したりするタスクを横断的に何というか？",
        "Opt1": "マルチタスク",
        "Opt2": "マルチモーダル",
        "Opt3": "画像分類",
        "Opt4": "異常検知",
        "Opt5": "自然言語処理",
        "Opt6": "音声認識",
        "Answer_Idx": 1,
        "Explanation": "異なるデータの形式（モダリティ）をまたぐ処理です。",
        "Link": "https://example.com/cross-modal"
    },
    {
        "ID": "DL-119",
        "Category": "4.ディープラーニングの概要",
        "Question": "学習済みの大規模モデルを教師として、より小さなモデルを学習させることで、精度を保ちつつ軽量化する手法は？",
        "Opt1": "量子化",
        "Opt2": "蒸留 (Knowledge Distillation)",
        "Opt3": "剪定",
        "Opt4": "転移学習",
        "Opt5": "ファインチューニング",
        "Opt6": "データ拡張",
        "Answer_Idx": 1,
        "Explanation": "教師モデルの出力分布（ソフトターゲット）を模倣するように学習させます。",
        "Link": "https://example.com/distillation-detail"
    },
    {
        "ID": "DL-120",
        "Category": "4.ディープラーニングの概要",
        "Question": "オートエンコーダの潜在空間（隠れ層）において、データを意図的に「スパース（疎）」にする手法を何というか？",
        "Opt1": "積層自己符号化器",
        "Opt2": "スパースオートエンコーダ",
        "Opt3": "VAE",
        "Opt4": "Denoising Autoencoder",
        "Opt5": "CNN",
        "Opt6": "RNN",
        "Answer_Idx": 1,
        "Explanation": "入力データの少数の重要な特徴のみで情報を表現するように強制する手法です。",
        "Link": "https://example.com/sparse-ae"
    },
    {
        "ID": "DL-121",
        "Category": "4.ディープラーニングの概要",
        "Question": "Googleが開発した、層の「深さ」「幅」「解像度」を効率的にスケールアップさせたCNNモデルは？",
        "Opt1": "VGG16",
        "Opt2": "ResNet",
        "Opt3": "EfficientNet",
        "Opt4": "AlexNet",
        "Opt5": "MobileNet",
        "Opt6": "Inception-v3",
        "Answer_Idx": 2,
        "Explanation": "複合係数を用いてモデルのサイズを最適化し、高い精度と効率を両立させました。",
        "Link": "https://example.com/efficientnet"
    },
    {
        "ID": "DL-122",
        "Category": "4.ディープラーニングの概要",
        "Question": "音声データをディープラーニングで扱う際、周波数成分を時間軸に沿って可視化したものを何というか？",
        "Opt1": "ヒストグラム",
        "Opt2": "スペクトログラム",
        "Opt3": "散布図",
        "Opt4": "箱ひげ図",
        "Opt5": "ROC曲線",
        "Opt6": "デンドログラム",
        "Answer_Idx": 1,
        "Explanation": "音声波形を短時間フーリエ変換（STFT）することで得られる画像状のデータです。",
        "Link": "https://example.com/spectrogram"
    },
    {
        "ID": "DL-123",
        "Category": "4.ディープラーニングの概要",
        "Question": "音声認識において、人間が聞き取りやすい周波数帯域を重視するように加工された尺度は？",
        "Opt1": "ヘルツ（Hz）",
        "Opt2": "メル尺度（Mel Scale）",
        "Opt3": "デシベル（dB）",
        "Opt4": "ビット（bit）",
        "Opt5": "バイト",
        "Opt6": "ケルビン",
        "Answer_Idx": 1,
        "Explanation": "この尺度を用いた「メルスペクトログラム」が音声処理の標準的な入力特徴量です。",
        "Link": "https://example.com/mel-scale"
    },
    {
        "ID": "DL-124",
        "Category": "4.ディープラーニングの概要",
        "Question": "時系列データの「局所的なパターン」を捉えるために、1次元方向にフィルタを滑らせる手法は？",
        "Opt1": "1D-CNN",
        "Opt2": "2D-CNN",
        "Opt3": "3D-CNN",
        "Opt4": "RNN",
        "Opt5": "LSTM",
        "Opt6": "GRU",
        "Answer_Idx": 0,
        "Explanation": "音声信号や心電図データ、株価などの時系列データの解析に有効です。",
        "Link": "https://example.com/1d-cnn"
    },
    {
        "ID": "DL-125",
        "Category": "4.ディープラーニングの概要",
        "Question": "SNSの交友関係や化合物の分子構造など、非欧幾里得的な構造を持つデータを扱うネットワークは？",
        "Opt1": "CNN",
        "Opt2": "RNN",
        "Opt3": "GNN（グラフニューラルネットワーク）",
        "Opt4": "GAN",
        "Opt5": "Transformer",
        "Opt6": "MLP",
        "Answer_Idx": 2,
        "Explanation": "ノード（点）とエッジ（線）の関係性を直接学習できるモデルです。",
        "Link": "https://example.com/gnn"
    },
    {
        "ID": "DL-126",
        "Category": "4.ディープラーニングの概要",
        "Question": "学習の初期段階で学習率を非常に小さく設定し、数エポックかけて徐々に上げていく手法を何というか？",
        "Opt1": "学習率減衰",
        "Opt2": "ウォームアップ（Warmup）",
        "Opt3": "勾配クリッピング",
        "Opt4": "ドロップアウト",
        "Opt5": "正則化",
        "Opt6": "標準化",
        "Answer_Idx": 1,
        "Explanation": "学習初期の不安定な勾配による重みの破壊を防ぎ、学習を安定させます。",
        "Link": "https://example.com/lr-warmup"
    },
    {
        "ID": "DL-127",
        "Category": "4.ディープラーニングの概要",
        "Question": "Transformerのデコーダにおいて、エンコーダからの出力（Key/Value）とデコーダ内のQueryを照合する仕組みは？",
        "Opt1": "Self-Attention",
        "Opt2": "Cross-Attention",
        "Opt3": "Hard Attention",
        "Opt4": "Soft Attention",
        "Opt5": "Multi-head Attention",
        "Opt6": "Causal Attention",
        "Answer_Idx": 1,
        "Explanation": "これにより、入力文のどの単語に注目して翻訳（出力）すべきかを決定します。",
        "Link": "https://example.com/cross-attention"
    },
    {
        "ID": "DL-128",
        "Category": "4.ディープラーニングの概要",
        "Question": "「2つの入力データが同じものか（あるいは似ているか）」を判定するために、重みを共有した2つのNNを用いる構造は？",
        "Opt1": "GAN",
        "Opt2": "シャムネットワーク（Siamese Network）",
        "Opt3": "Autoencoder",
        "Opt4": "RNN",
        "Opt5": "U-Net",
        "Opt6": "ResNet",
        "Answer_Idx": 1,
        "Explanation": "顔認証や署名照合など、2つのデータの類似度を測るタスクで使われます。",
        "Link": "https://example.com/siamese-net"
    },
    {
        "ID": "DL-129",
        "Category": "4.ディープラーニングの概要",
        "Question": "CNNにおいて、チャネル方向の情報を圧縮し、空間方向（縦横）の解像度を保持したまま情報を整理する手法は？",
        "Opt1": "Max Pooling",
        "Opt2": "Global Average Pooling",
        "Opt3": "1x1畳み込み",
        "Opt4": "Flatten",
        "Opt5": "Dropout",
        "Opt6": "Batch Norm",
        "Answer_Idx": 2,
        "Explanation": "チャネル間の相関を学習させつつ、計算量を削減する重要なテクニックです。",
        "Link": "https://example.com/pointwise-conv"
    },
    {
        "ID": "DL-130",
        "Category": "4.ディープラーニングの概要",
        "Question": "活性化関数の出力が特定の範囲（0〜1など）に収まり、入力が極端な値のときに勾配が0に近づく現象は？",
        "Opt1": "勾配爆発",
        "Opt2": "飽和（Saturation）",
        "Opt3": "発散",
        "Opt4": "収束",
        "Opt5": "共線性",
        "Opt6": "正規化",
        "Answer_Idx": 1,
        "Explanation": "シグモイド関数やtanh関数で顕著であり、これが勾配消失の直接的な原因となります。",
        "Link": "https://example.com/saturation-dl"
    },
    {
        "ID": "DL-131",
        "Category": "4.ディープラーニングの概要",
        "Question": "出力層のノードが1つで、活性化関数にシグモイド関数を用いる場合に最適な損失関数は？",
        "Opt1": "平均二乗誤差 (MSE)",
        "Opt2": "二値交差エントロピー (Binary Cross Entropy)",
        "Opt3": "カテゴリカル交差エントロピー",
        "Opt4": "ヒンジ損失",
        "Opt5": "ジニ係数",
        "Opt6": "KLダイバージェンス",
        "Answer_Idx": 1,
        "Explanation": "2値分類（Yes/No）のタスクで最も一般的に使用される損失関数です。",
        "Link": "https://example.com/bce-loss"
    },
    {
        "ID": "DL-132",
        "Category": "4.ディープラーニングの概要",
        "Question": "学習が進むにつれて、最適な解（最小値）の周辺で重みが振動してしまい、収束しない場合に下げるべき値は？",
        "Opt1": "バッチサイズ",
        "Opt2": "エポック数",
        "Opt3": "学習率",
        "Opt4": "モーメンタム係数",
        "Opt5": "隠れ層の数",
        "Opt6": "ノード数",
        "Answer_Idx": 2,
        "Explanation": "学習率を段階的に下げる（スケジューリング）ことで、解を安定して収束させます。",
        "Link": "https://example.com/lr-annealing"
    },
    {
        "ID": "DL-133",
        "Category": "4.ディープラーニングの概要",
        "Question": "CNNで「縦横の解像度を半分」にするのと同等の効果を持つ畳み込み設定は？",
        "Opt1": "ストライド=1",
        "Opt2": "ストライド=2",
        "Opt3": "パディング=1",
        "Opt4": "パディング=0",
        "Opt5": "カーネルサイズ=1",
        "Opt6": "フィルタ数=2",
        "Answer_Idx": 1,
        "Explanation": "ストライドを2に設定すると、1画素飛ばしで走査するため出力サイズが縦横半分になります。",
        "Link": "https://example.com/stride-calculation"
    },
    {
        "ID": "DL-134",
        "Category": "4.ディープラーニングの概要",
        "Question": "訓練データに対しては損失が減り続けるが、検証データに対する損失が増え始めた状態を何というか？",
        "Opt1": "アンダーフィッティング",
        "Opt2": "オーバーフィッティング（過学習）",
        "Opt3": "収束",
        "Opt4": "デッドロック",
        "Opt5": "モード崩壊",
        "Opt6": "ハルシネーション",
        "Answer_Idx": 1,
        "Explanation": "モデルが訓練データのノイズまで学習してしまい、汎用性を失った状態です。",
        "Link": "https://example.com/overfitting-loss"
    },
    {
        "ID": "DL-135",
        "Category": "4.ディープラーニングの概要",
        "Question": "画像の各画素の値を255で割り、0.0〜1.0の範囲に収める処理をディープラーニングの文脈で何というか？",
        "Opt1": "標準化",
        "Opt2": "正規化（入力の正規化）",
        "Opt3": "正則化",
        "Opt4": "量子化",
        "Opt5": "平滑化",
        "Opt6": "白色化",
        "Answer_Idx": 1,
        "Explanation": "ニューラルネットワークへの入力を小さな値の範囲に揃えることで、学習を効率化します。",
        "Link": "https://example.com/pixel-norm"
    },
    {
        "ID": "DL-136",
        "Category": "4.ディープラーニングの概要",
        "Question": "RNNにおいて、出力層だけでなく隠れ層（状態）を多段に重ねた構造を何というか？",
        "Opt1": "双方向RNN",
        "Opt2": "深層RNN (Stacked RNN)",
        "Opt3": "LSTM",
        "Opt4": "GRU",
        "Opt5": "CNN",
        "Opt6": "GAN",
        "Answer_Idx": 1,
        "Explanation": "より複雑な時系列のパターンや概念の階層を学習することが可能になります。",
        "Link": "https://example.com/stacked-rnn"
    },
    {
        "ID": "DL-137",
        "Category": "4.ディープラーニングの概要",
        "Question": "特定のクラスのデータだけを集中的に学習させ、それ以外のデータ（異常）を検知させる手法は？",
        "Opt1": "多クラス分類",
        "Opt2": "1クラス分類（One-class Classification）",
        "Opt3": "クラスタリング",
        "Opt4": "次元削減",
        "Opt5": "回帰分析",
        "Opt6": "強化学習",
        "Answer_Idx": 1,
        "Explanation": "正常データのみで学習し、そこから外れたものを異常とみなす異常検知の基本手法です。",
        "Link": "https://example.com/one-class"
    },
    {
        "ID": "DL-138",
        "Category": "4.ディープラーニングの概要",
        "Question": "ある分布 P と別の分布 Q の「近さ（類似度）」を測るために用いられる統計的な指標は？",
        "Opt1": "平均二乗誤差",
        "Opt2": "KLダイバージェンス",
        "Opt3": "相関係数",
        "Opt4": "標準偏差",
        "Opt5": "決定係数",
        "Opt6": "ジニ不純度",
        "Answer_Idx": 1,
        "Explanation": "VAE（変分自己符号化器）の損失関数などで、潜在変数の分布を制御するために使われます。",
        "Link": "https://example.com/kl-divergence"
    },
    {
        "ID": "DL-139",
        "Category": "4.ディープラーニングの概要",
        "Question": "モデルの重みをL2正則化（Weight Decay）によって抑制することで、決定境界はどうなるか？",
        "Opt1": "より複雑でギザギザになる",
        "Opt2": "より滑らかで単純になる",
        "Opt3": "垂直または水平になる",
        "Opt4": "変化しない",
        "Opt5": "完全に消滅する",
        "Opt6": "2つに分裂する",
        "Answer_Idx": 1,
        "Explanation": "重みが小さく抑えられることで、急激な変化（過学習）が抑制され、境界が滑らかになります。",
        "Link": "https://example.com/regularization-boundary"
    },
    {
        "ID": "DL-140",
        "Category": "4.ディープラーニングの概要",
        "Question": "大規模な事前学習済みモデル（Foundation Model）を基盤に、少数のデータで特定用途へ適応させる一連の流れを何というか？",
        "Opt1": "スクラッチ学習",
        "Opt2": "アダプテーション（適応学習）",
        "Opt3": "蒸留",
        "Opt4": "量子化",
        "Opt5": "剪定",
        "Opt6": "正則化",
        "Answer_Idx": 1,
        "Explanation": "現在の生成AI開発の主流である「大規模学習＋微調整」のアプローチを指します。",
        "Link": "https://example.com/model-adaptation"
    },
    {
        "ID": "DL-141",
        "Category": "4.ディープラーニングの概要",
        "Question": "限られた計算資源で動作させるため、パラメータ数を極限まで減らした「Fireモジュール」を採用している軽量モデルは？",
        "Opt1": "ResNet",
        "Opt2": "SqueezeNet",
        "Opt3": "VGG16",
        "Opt4": "Inception",
        "Opt5": "BERT",
        "Opt6": "GAN",
        "Answer_Idx": 1,
        "Explanation": "「精度を維持しつつモデルサイズを50倍小さくする」ことを目標に開発された軽量化の先駆けです。",
        "Link": "https://example.com/squeezenet"
    },
    {
        "ID": "DL-142",
        "Category": "4.ディープラーニングの概要",
        "Question": "ニューラルネットワークの構造自体を、AI（強化学習など）を用いて自動で設計・最適化する技術を何というか？",
        "Opt1": "転移学習",
        "Opt2": "NAS（神経構造探索）",
        "Opt3": "蒸留",
        "Opt4": "量子化",
        "Opt5": "アンサンブル学習",
        "Opt6": "特徴量選択",
        "Answer_Idx": 1,
        "Explanation": "Neural Architecture Search。人間が設計するよりも高性能なモデル（EfficientNet等）を発見しました。",
        "Link": "https://example.com/nas-intro"
    },
    {
        "ID": "DL-143",
        "Category": "4.ディープラーニングの概要",
        "Question": "学習率を周期的に上下させることで、局所解（ローカルミニマ）から脱出しやすくする手法を何というか？",
        "Opt1": "学習率減衰",
        "Opt2": "サイクリック学習率 (CLR)",
        "Opt3": "バッチ正規化",
        "Opt4": "ドロップアウト",
        "Opt5": "早期終了",
        "Opt6": "L1正則化",
        "Answer_Idx": 1,
        "Explanation": "学習率を一定にせず、リズミカルに変化させることで、より広域的な最適解を探します。",
        "Link": "https://example.com/cyclic-lr"
    },
    {
        "ID": "DL-144",
        "Category": "4.ディープラーニングの概要",
        "Question": "モデルが特定の入力に対して「なぜその予測をしたか」を人間が理解できるようにする技術の総称は？",
        "Opt1": "XAI（説明可能なAI）",
        "Opt2": "AGI（汎用人工知能）",
        "Opt3": "NLP（自然言語処理）",
        "Opt4": "Edge AI",
        "Opt5": "Federated Learning",
        "Opt6": "AutoML",
        "Answer_Idx": 0,
        "Explanation": "eXplainable AI。ブラックボックス化した深層学習の判断根拠（Grad-CAM等）を明らかにします。",
        "Link": "https://example.com/xai-def"
    },
    {
        "ID": "DL-145",
        "Category": "4.ディープラーニングの概要",
        "Question": "学習データに含まれる偏りによって、AIが特定の属性（人種・性別等）に対し不当な判断を下すことを何というか？",
        "Opt1": "オーバーフィッティング",
        "Opt2": "アルゴリズム・バイアス",
        "Opt3": "コンセプトドリフト",
        "Opt4": "勾配消失",
        "Opt5": "ハルシネーション",
        "Opt6": "モード崩壊",
        "Answer_Idx": 1,
        "Explanation": "公平なAIを実現するために、データの選定や評価指標の工夫が求められています。",
        "Link": "https://example.com/algo-bias"
    },
    {
        "ID": "DL-146",
        "Category": "4.ディープラーニングの概要",
        "Question": "データを一箇所に集めず、各端末（スマホ等）で学習を行い、重みの更新情報だけを統合する学習手法は？",
        "Opt1": "蒸留",
        "Opt2": "連合学習（フェデレーション学習）",
        "Opt3": "能動学習",
        "Opt4": "半教師あり学習",
        "Opt5": "多タスク学習",
        "Opt6": "転移学習",
        "Answer_Idx": 1,
        "Explanation": "プライバシーを保護しながら、多くのユーザーデータから学習できるメリットがあります。",
        "Link": "https://example.com/federated-learning"
    },
    {
        "ID": "DL-147",
        "Category": "4.ディープラーニングの概要",
        "Question": "CNNにおいて、入力画像の一部を矩形にマスク（ゼロにする）して学習させる強力な正則化手法は？",
        "Opt1": "ドロップアウト",
        "Opt2": "Cutout",
        "Opt3": "Mixup",
        "Opt4": "ラベルスムージング",
        "Opt5": "バッチ正規化",
        "Opt6": "パディング",
        "Answer_Idx": 1,
        "Explanation": "画像の一部を隠すことで、特定の部位だけに頼らない頑健な特徴抽出を促します。",
        "Link": "https://example.com/cutout"
    },
    {
        "ID": "DL-148",
        "Category": "4.ディープラーニングの概要",
        "Question": "2つの異なる学習データを一定の割合で混ぜ合わせ（合成し）、ラベルもその割合で混ぜて学習する手法は？",
        "Opt1": "Cutout",
        "Opt2": "Mixup",
        "Opt3": "データオーギュメンテーション",
        "Opt4": "正則化",
        "Opt5": "標準化",
        "Opt6": "蒸留",
        "Answer_Idx": 1,
        "Explanation": "データの境界を滑らかに学習させることで、モデルの汎化性能を大幅に向上させます。",
        "Link": "https://example.com/mixup"
    },
    {
        "ID": "DL-149",
        "Category": "4.ディープラーニングの概要",
        "Question": "学習中のモデルの複数の時点（エポック）の重みを平均化することで、安定した精度を得る手法は？",
        "Opt1": "SWA（Stochastic Weight Averaging）",
        "Opt2": "モーメンタム",
        "Opt3": "Adam",
        "Opt4": "ドロップアウト",
        "Opt5": "早期終了",
        "Opt6": "量子化",
        "Answer_Idx": 0,
        "Explanation": "パラメータ空間における「平坦な解」を見つけやすくなり、汎化性能が向上します。",
        "Link": "https://example.com/swa"
    },
    {
        "ID": "DL-150",
        "Category": "4.ディープラーニングの概要",
        "Question": "学習済みの重みを一切変えず、入力側の「プロンプトベクトル」のみを学習させるPEFTの一種は？",
        "Opt1": "LoRA",
        "Opt2": "Prompt Tuning",
        "Opt3": "アダプター",
        "Opt4": "フルチューニング",
        "Opt5": "量子化",
        "Opt6": "蒸留",
        "Answer_Idx": 1,
        "Explanation": "巨大なLLMの重みを固定したまま、タスク固有の適応を非常に低コストで行えます。",
        "Link": "https://example.com/prompt-tuning"
    },
    {
        "ID": "DL-151",
        "Category": "4.ディープラーニングの概要",
        "Question": "物体検出モデル「SSD」において、複数の異なる解像度の層から予測を行うことで得られるメリットは？",
        "Opt1": "計算速度の低下",
        "Opt2": "多スケール（大小様々）な物体の検出",
        "Opt3": "色の識別の向上",
        "Opt4": "メモリ消費の削減",
        "Opt5": "欠損値の補完",
        "Opt6": "過学習の促進",
        "Answer_Idx": 1,
        "Explanation": "Single Shot MultiBox Detectorは、浅い層で小さい、深い層で大きい物体を捉えます。",
        "Link": "https://example.com/ssd-multiscale"
    },
    {
        "ID": "DL-152",
        "Category": "4.ディープラーニングの概要",
        "Question": "ディープラーニングにおける「勾配消失」を数学的に説明する際、最も関連が深い計算ルールは？",
        "Opt1": "ピタゴラスの定理",
        "Opt2": "連鎖律（チェインルール）による積の計算",
        "Opt3": "ベイズの定理",
        "Opt4": "ガウス分布の積分",
        "Opt5": "行列の行列式",
        "Opt6": "ラグランジュの未定乗数法",
        "Answer_Idx": 1,
        "Explanation": "1未満の微分の値を何度も掛け合わせる（連鎖させる）ことで、値が極小化します。",
        "Link": "https://example.com/chain-rule-multiplication"
    },
    {
        "ID": "DL-153",
        "Category": "4.ディープラーニングの概要",
        "Question": "学習をこれ以上続けても誤差が減らない「停滞した状態（高原）」のことを何というか？",
        "Opt1": "サドルポイント",
        "Opt2": "プラトー (Plateau)",
        "Opt3": "ローカルミニマ",
        "Opt4": "グローバルミニマ",
        "Opt5": "バイアス",
        "Opt6": "バリアンス",
        "Answer_Idx": 1,
        "Explanation": "学習率スケジューラなどで、プラトーに達した時に学習率を下げる制御が行われます。",
        "Link": "https://example.com/plateau"
    },
    {
        "ID": "DL-154",
        "Category": "4.ディープラーニングの概要",
        "Question": "あるタスクで学習した重みを、そのまま別のタスクの初期値として使う行為を何というか？",
        "Opt1": "知識の転送（Weight Transfer）",
        "Opt2": "蒸留",
        "Opt3": "量子化",
        "Opt4": "剪定",
        "Opt5": "正則化",
        "Opt6": "標準化",
        "Answer_Idx": 0,
        "Explanation": "「転移学習」の具体的な一工程であり、ゼロからの学習（Scratch）を避ける手法です。",
        "Link": "https://example.com/weight-transfer"
    },
    {
        "ID": "DL-155",
        "Category": "4.ディープラーニングの概要",
        "Question": "計算グラフ上の演算を並列化・最適化し、GPUなどのハードウェア性能を引き出すライブラリの核となる部分は？",
        "Opt1": "GUI",
        "Opt2": "JITコンパイラ（エンジン）",
        "Opt3": "ファイルシステム",
        "Opt4": "データベース",
        "Opt5": "ネットワークプロトコル",
        "Opt6": "シェル",
        "Answer_Idx": 1,
        "Explanation": "PyTorchやTensorFlowは、内部で計算グラフを高速に処理するエンジンを持っています。",
        "Link": "https://example.com/dl-engine"
    },
    {
        "ID": "DL-156",
        "Category": "4.ディープラーニングの概要",
        "Question": "「1つのモデルで100種類以上の言語を同時に翻訳する」ような、複数タスクを並行して学ぶ学習は？",
        "Opt1": "マルチタスク学習",
        "Opt2": "マルチモーダル学習",
        "Opt3": "転移学習",
        "Opt4": "自己学習",
        "Opt5": "強化学習",
        "Opt6": "次元削減",
        "Answer_Idx": 0,
        "Explanation": "共通の特徴表現を学ぶことで、データが少ないタスクの精度も底上げされる効果があります。",
        "Link": "https://example.com/multi-task"
    },
    {
        "ID": "DL-157",
        "Category": "4.ディープラーニングの概要",
        "Question": "入力データに微小なノイズを加え、モデルにあえて「誤答」させることを目的とした研究分野は？",
        "Opt1": "説明可能なAI",
        "Opt2": "敵対的学習（Adversarial Machine Learning）",
        "Opt3": "能動学習",
        "Opt4": "連合学習",
        "Opt5": "転移学習",
        "Opt6": "蒸留",
        "Answer_Idx": 1,
        "Explanation": "AIの脆弱性を理解し、より頑健なモデル（敵対的訓練済みモデル）を作るために重要です。",
        "Link": "https://example.com/adversarial-ml"
    },
    {
        "ID": "DL-158",
        "Category": "4.ディープラーニングの概要",
        "Question": "Transformerの自己注意機構の計算量は、入力列の長さ（L）に対してどのように増大するか？",
        "Opt1": "Lの1乗（線形）",
        "Opt2": "Lの2乗（二乗）",
        "Opt3": "Lの対数",
        "Opt4": "一定（定数）",
        "Opt5": "Lの3乗",
        "Opt6": "指数関数的",
        "Answer_Idx": 1,
        "Explanation": "計算量が二乗で増えるため、非常に長い文章（ロングコンテキスト）の処理には工夫が必要です。",
        "Link": "https://example.com/attention-complexity"
    },
    {
        "ID": "DL-159",
        "Category": "4.ディープラーニングの概要",
        "Question": "深層学習の「汎化性能」を高めるための最も確実で基本的な対策は？",
        "Opt1": "モデルを極限まで大きくする",
        "Opt2": "高品質な学習データを増やす",
        "Opt3": "学習率を最大にする",
        "Opt4": "エポック数を100万回にする",
        "Opt5": "活性化関数をすべて外す",
        "Opt6": "正則化をすべて外す",
        "Answer_Idx": 1,
        "Explanation": "データ量と質は、アルゴリズムの工夫以上にモデルの汎化性能を決定づけます。",
        "Link": "https://example.com/generalization-data"
    },
    {
        "ID": "DL-160",
        "Category": "4.ディープラーニングの概要",
        "Question": "ニューラルネットワークが「普遍的近似定理」を持つために、活性化関数に必要な性質は？",
        "Opt1": "線形であること",
        "Opt2": "非線形であること",
        "Opt3": "常に0以上であること",
        "Opt4": "微分が常に1であること",
        "Opt5": "周期関数であること",
        "Opt6": "負の値をとらないこと",
        "Answer_Idx": 1,
        "Explanation": "非線形な活性化関数があることで、理論上、どんな複雑な関数も近似できることが証明されています。",
        "Link": "https://example.com/universal-approximation"
    },
    {
        "ID": "DLA-001",
        "Category": "5.手法（応用）",
        "Question": "自然言語処理において、単語の意味を考慮せず、単に単語の出現頻度のみをカウントする手法は？",
        "Opt1": "Word2Vec",
        "Opt2": "RNN",
        "Opt3": "Bag-of-Words (BoW)",
        "Opt4": "BERT",
        "Opt5": "Attention",
        "Opt6": "LSTM",
        "Answer_Idx": 2,
        "Explanation": "文章を単語の「カバン（Bag）」と見なし、順序を無視して頻度をベクトル化します。",
        "Link": "https://example.com/bow-details"
    },
    {
        "ID": "DLA-002",
        "Category": "5.手法（応用）",
        "Question": "「単語の出現頻度」と「その単語がいくつの文書に出現するか」を組み合わせて、重要度を計算する手法は？",
        "Opt1": "BoW",
        "Opt2": "TF-IDF",
        "Opt3": "N-gram",
        "Opt4": "LDA",
        "Opt5": "One-Hot Encoding",
        "Opt6": "Word2Vec",
        "Answer_Idx": 1,
        "Explanation": "多くの文書に出現する「の」や「です」などの価値を下げ、特徴的な単語を抽出します。",
        "Link": "https://example.com/tf-idf-details"
    },
    {
        "ID": "DLA-003",
        "Category": "5.手法（応用）",
        "Question": "Word2Vecにおいて、周囲の単語から中心の単語を予測するモデルを何というか？",
        "Opt1": "Skip-gram",
        "Opt2": "CBOW (Continuous Bag-of-Words)",
        "Opt3": "RNN",
        "Opt4": "CNN",
        "Opt5": "Seq2Seq",
        "Opt6": "Attention",
        "Answer_Idx": 1,
        "Explanation": "「周囲から中心」がCBOW、「中心から周囲」がSkip-gramです。",
        "Link": "https://example.com/cbow-vs-skipgram"
    },
    {
        "ID": "DLA-004",
        "Category": "5.手法（応用）",
        "Question": "Word2Vecにおいて、ある単語から周囲の単語（コンテキスト）を予測するモデルを何というか？",
        "Opt1": "CBOW",
        "Opt2": "Skip-gram",
        "Opt3": "N-gram",
        "Opt4": "TF-IDF",
        "Opt5": "LDA",
        "Opt6": "FastText",
        "Answer_Idx": 1,
        "Explanation": "計算量は多いですが、低頻度の単語でも高い精度でベクトル化できる傾向があります。",
        "Link": "https://example.com/skip-gram-details"
    },
    {
        "ID": "DLA-005",
        "Category": "5.手法（応用）",
        "Question": "単語だけでなく、文字単位（サブワード）の情報を考慮することで未知語に強くなったWord2Vecの改良版は？",
        "Opt1": "GloVe",
        "Opt2": "FastText",
        "Opt3": "ELMo",
        "Opt4": "BERT",
        "Opt5": "GPT",
        "Opt6": "RoBERTa",
        "Answer_Idx": 1,
        "Explanation": "Facebook（現Meta）が開発。語形変化が激しい言語などで特に有効です。",
        "Link": "https://example.com/fasttext-details"
    },
    {
        "ID": "DLA-006",
        "Category": "5.手法（応用）",
        "Question": "RNNを用いたSeq2Seqにおいて、入力系列の情報を一つの固定長ベクトルに圧縮する部分を何というか？",
        "Opt1": "デコーダ",
        "Opt2": "エンコーダ",
        "Opt3": "アテンション",
        "Opt4": "プーリング",
        "Opt5": "フィルタ",
        "Opt6": "ゲート",
        "Answer_Idx": 1,
        "Explanation": "Encoderで文脈を凝縮し、Decoderでそれを展開して出力します。",
        "Link": "https://example.com/encoder-decoder-concept"
    },
    {
        "ID": "DLA-007",
        "Category": "5.手法（応用）",
        "Question": "Transformerにおいて、計算を高速化するために並列化を阻害していたどの構造を排除したか？",
        "Opt1": "畳み込み構造",
        "Opt2": "再帰構造（RNN）",
        "Opt3": "全結合構造",
        "Opt4": "アテンション構造",
        "Opt5": "活性化関数",
        "Opt6": "バイアス",
        "Answer_Idx": 1,
        "Explanation": "RNNを排除しAttentionのみにすることで、大規模データの高速な並列学習が可能になりました。",
        "Link": "https://example.com/transformer-rnn-less"
    },
    {
        "ID": "DLA-008",
        "Category": "5.手法（応用）",
        "Question": "BERTの事前学習タスクの一つで、文中の一部の単語を隠してそれを当てさせる手法を何というか？",
        "Opt1": "Next Sentence Prediction",
        "Opt2": "Masked Language Model (MLM)",
        "Opt3": "Self-Attention",
        "Opt4": "Fine-tuning",
        "Opt5": "Distillation",
        "Opt6": "Data Augmentation",
        "Answer_Idx": 1,
        "Explanation": "文章の双方向の文脈を理解するための主要な学習タスクです。",
        "Link": "https://example.com/masked-lm"
    },
    {
        "ID": "DLA-009",
        "Category": "5.手法（応用）",
        "Question": "BERTの事前学習において、2つの文章が連続しているかどうかを判定するタスクを何というか？",
        "Opt1": "Masked Language Model",
        "Opt2": "Next Sentence Prediction (NSP)",
        "Opt3": "Sentiment Analysis",
        "Opt4": "Question Answering",
        "Opt5": "POS Tagging",
        "Opt6": "Summarization",
        "Answer_Idx": 1,
        "Explanation": "文と文の関係性を理解し、文書全体の構造を把握するために使われます。",
        "Link": "https://example.com/nsp-details"
    },
    {
        "ID": "DLA-010",
        "Category": "5.手法（応用）",
        "Question": "画像認識において、1枚の画像に複数の物体がある場合、それら全てのカテゴリと位置を特定するタスクは？",
        "Opt1": "画像分類",
        "Opt2": "物体検出（Object Detection）",
        "Opt3": "セマンティックセグメンテーション",
        "Opt4": "異常検知",
        "Opt5": "超解像",
        "Opt6": "画像生成",
        "Answer_Idx": 1,
        "Explanation": "バウンディングボックスと呼ばれる矩形枠で物体の位置を示します。",
        "Link": "https://example.com/obj-detection-task"
    },
    {
        "ID": "DLA-011",
        "Category": "5.手法（応用）",
        "Question": "物体検出において、領域候補（Region Proposal）を抽出してから分類を行う手法はどれか？",
        "Opt1": "YOLO",
        "Opt2": "SSD",
        "Opt3": "R-CNN",
        "Opt4": "RetinaNet",
        "Opt5": "CornerNet",
        "Opt6": "DETR",
        "Answer_Idx": 2,
        "Explanation": "「どこに物があるか」を探してから「それが何か」を判別する2段階の手法です。",
        "Link": "https://example.com/rcnn-family"
    },
    {
        "ID": "DLA-012",
        "Category": "5.手法（応用）",
        "Question": "YOLOやSSDがR-CNN系よりも優れている（実用上重宝される）最大の点は何か？",
        "Opt1": "検出精度が極めて高い",
        "Opt2": "計算速度が速くリアルタイム処理が可能",
        "Opt3": "小さな物体の検出に強い",
        "Opt4": "学習データが少なくて済む",
        "Opt5": "モデルの層が深い",
        "Opt6": "ハイパーパラメータがない",
        "Answer_Idx": 1,
        "Explanation": "1回のスキャンで検出を完結させるため、自動運転や監視カメラ等に向いています。",
        "Link": "https://example.com/yolo-merit"
    },
    {
        "ID": "DLA-013",
        "Category": "5.手法（応用）",
        "Question": "セマンティックセグメンテーションにおいて、低レイヤーの細かい情報を高レイヤーに伝えるために使われる構造は？",
        "Opt1": "スキップ接続 (Skip Connection)",
        "Opt2": "プーリング層",
        "Opt3": "全結合層",
        "Opt4": "ドロップアウト層",
        "Opt5": "ストライド",
        "Opt6": "パディング",
        "Answer_Idx": 0,
        "Explanation": "U-Netなどで、位置情報を正確に復元するために不可欠な技術です。",
        "Link": "https://example.com/unet-skip"
    },
    {
        "ID": "DLA-014",
        "Category": "5.手法（応用）",
        "Question": "画像生成において、テキストでの指示（プロンプト）を元に画像を生成するモデルの総称は？",
        "Opt1": "Image-to-Image",
        "Opt2": "Text-to-Image",
        "Opt3": "Text-to-Speech",
        "Opt4": "Video-to-Text",
        "Opt5": "NLP",
        "Opt6": "CNN",
        "Answer_Idx": 1,
        "Explanation": "Stable DiffusionやMidjourneyなどがこれに該当します。",
        "Link": "https://example.com/text-to-image-ai"
    },
    {
        "ID": "DLA-015",
        "Category": "5.手法（応用）",
        "Question": "GANにおいて、生成器（Generator）が最小化しようとするものはどれか？",
        "Opt1": "生成画像が本物と判定される確率",
        "Opt2": "生成画像が偽物と判定される確率",
        "Opt3": "識別器の精度",
        "Opt4": "データの次元数",
        "Opt5": "学習率",
        "Opt6": "重みの数",
        "Answer_Idx": 1,
        "Explanation": "識別器を「騙す」ことが目的であるため、偽物だとバレる確率を最小化します。",
        "Link": "https://example.com/gan-objective"
    },
    {
        "ID": "DLA-016",
        "Category": "5.手法（応用）",
        "Question": "強化学習のDQNにおいて、方策（Action）を決定する際に「現在の最大Q値」のみに固執しないための手法は？",
        "Opt1": "Experience Replay",
        "Opt2": "ε-greedy法",
        "Opt3": "Target Network",
        "Opt4": "Batch Norm",
        "Opt5": "Softmax",
        "Opt6": "ReLU",
        "Answer_Idx": 1,
        "Explanation": "一定の確率でランダムな行動を選択することで、より良い戦略の「探索」を行います。",
        "Link": "https://example.com/exploration-exploitation"
    },
    {
        "ID": "DLA-017",
        "Category": "5.手法（応用）",
        "Question": "AlphaGoが囲碁で人間に勝利した際、有望な次の一手を効率的に探索するために用いたアルゴリズムは？",
        "Opt1": "Q学習",
        "Opt2": "モンテカルロ木探索",
        "Opt3": "A*アルゴリズム",
        "Opt4": "ダイクストラ法",
        "Opt5": "深さ優先探索",
        "Opt6": "幅優先探索",
        "Answer_Idx": 1,
        "Explanation": "膨大な選択肢の中から、シミュレーションを通じて筋の良い手を選び出します。",
        "Link": "https://example.com/mcts-alphago"
    },
    {
        "ID": "DLA-018",
        "Category": "5.手法（応用）",
        "Question": "音声認識の分野で、入力された音声の波形を「音素」や「文字」の並びへと直接変換するモデルを何というか？",
        "Opt1": "End-to-Endモデル",
        "Opt2": "カスケードモデル",
        "Opt3": "ルールベースモデル",
        "Opt4": "形態素解析モデル",
        "Opt5": "音響モデル",
        "Opt6": "言語モデル",
        "Answer_Idx": 0,
        "Explanation": "音響モデルや言語モデルを個別に作らず、一つの巨大なネットワークで完結させます。",
        "Link": "https://example.com/e2e-speech"
    },
    {
        "ID": "DLA-019",
        "Category": "5.手法（応用）",
        "Question": "Transformerにおいて、同じ入力文の中での単語同士の関連性を計算するメカニズムは？",
        "Opt1": "Cross-Attention",
        "Opt2": "Self-Attention",
        "Opt3": "Hard-Attention",
        "Opt4": "Soft-Attention",
        "Opt5": "Global-Attention",
        "Opt6": "Local-Attention",
        "Answer_Idx": 1,
        "Explanation": "文中の単語が、同じ文の中の他のどの単語と強く結びついているかを計算します。",
        "Link": "https://example.com/self-attention-mechanism"
    },
    {
        "ID": "DLA-020",
        "Category": "5.手法（応用）",
        "Question": "BERTをさらに改良し、学習時間を長くし、より大きなデータセットとバッチサイズで学習させたモデルは？",
        "Opt1": "GPT-2",
        "Opt2": "RoBERTa",
        "Opt3": "ALBERT",
        "Opt4": "DistilBERT",
        "Opt5": "XLNet",
        "Opt6": "T5",
        "Answer_Idx": 1,
        "Explanation": "Robustly Optimized BERT Approach。NSPタスクを削除するなどの工夫もされています。",
        "Link": "https://example.com/roberta-details"
    },
    {
        "ID": "DLA-021",
        "Category": "5.手法（応用）",
        "Question": "画像を固定サイズのパッチに分割し、Transformerのエンコーダに入力して画像分類を行うモデルは？",
        "Opt1": "CNN",
        "Opt2": "RNN",
        "Opt3": "ViT (Vision Transformer)",
        "Opt4": "GAN",
        "Opt5": "U-Net",
        "Opt6": "DeepLab",
        "Answer_Idx": 2,
        "Explanation": "画像を「単語の並び」のように扱うことで、CNNを凌駕する性能を達成しました。",
        "Link": "https://example.com/vit-details"
    },
    {
        "ID": "DLA-022",
        "Category": "5.手法（応用）",
        "Question": "ViTの計算コスト（解像度の2乗に比例）を抑えるため、パッチを段階的に統合して階層的な特徴を得るモデルは？",
        "Opt1": "AlexNet",
        "Opt2": "VGG",
        "Opt3": "Swin Transformer",
        "Opt4": "ResNet",
        "Opt5": "Inception",
        "Opt6": "MobileNet",
        "Answer_Idx": 2,
        "Explanation": "「Windows」という局所的な範囲でAttentionを計算し、効率化を図っています。",
        "Link": "https://example.com/swin-transformer"
    },
    {
        "ID": "DLA-023",
        "Category": "5.手法（応用）",
        "Question": "物体検出において、アンカーボックスやNMS（非最大値抑制）を不要にした、Transformerベースのモデルは？",
        "Opt1": "YOLOv5",
        "Opt2": "SSD",
        "Opt3": "DETR (DEtection TRansformer)",
        "Opt4": "Faster R-CNN",
        "Opt5": "R-FCN",
        "Opt6": "RetinaNet",
        "Answer_Idx": 2,
        "Explanation": "物体検出を「集合予測」の問題として捉え、エンドツーエンドでの学習を可能にしました。",
        "Link": "https://example.com/detr-details"
    },
    {
        "ID": "DLA-024",
        "Category": "5.手法（応用）",
        "Question": "セマンティックセグメンテーションにおいて、ピクセルごとの分類ではなく、物体（マスク）の集合を直接予測する手法は？",
        "Opt1": "FCN",
        "Opt2": "Mask2Former",
        "Opt3": "U-Net",
        "Opt4": "SegNet",
        "Opt5": "PSPNet",
        "Opt6": "DeepLabv3",
        "Answer_Idx": 1,
        "Explanation": "最新のセグメンテーション手法で、Transformerを用いて高品質なマスクを生成します。",
        "Link": "https://example.com/mask2former"
    },
    {
        "ID": "DLA-025",
        "Category": "5.手法（応用）",
        "Question": "拡散モデルにおいて、画像にノイズを加えていく過程を何というか？",
        "Opt1": "逆拡散過程",
        "Opt2": "順拡散過程",
        "Opt3": "サンプリング",
        "Opt4": "量子化",
        "Opt5": "蒸留",
        "Opt6": "剪定",
        "Answer_Idx": 1,
        "Explanation": "Forward Diffusion Process。最終的に画像を完全なガウスノイズに変換します。",
        "Link": "https://example.com/forward-diffusion"
    },
    {
        "ID": "DLA-026",
        "Category": "5.手法（応用）",
        "Question": "拡散モデルにおいて、ノイズから元の画像を段階的に復元していく過程を何というか？",
        "Opt1": "順拡散過程",
        "Opt2": "逆拡散過程",
        "Opt3": "エンコーディング",
        "Opt4": "プーリング",
        "Opt5": "パディング",
        "Opt6": "ストライド",
        "Answer_Idx": 1,
        "Explanation": "Reverse Diffusion Process。学習したモデルを用いて、ノイズを除去（デノイジング）します。",
        "Link": "https://example.com/reverse-diffusion"
    },
    {
        "ID": "DLA-027",
        "Category": "5.手法（応用）",
        "Question": "Stable Diffusionなどで、テキストによる指示（プロンプト）を拡散モデルに反映させるために使われる仕組みは？",
        "Opt1": "Self-Attention",
        "Opt2": "Cross-Attention",
        "Opt3": "Skip Connection",
        "Opt4": "Residual Block",
        "Opt5": "Batch Norm",
        "Opt6": "Dropout",
        "Answer_Idx": 1,
        "Explanation": "テキストのベクトルと画像の特徴をCross-Attentionで照合し、生成を制御します。",
        "Link": "https://example.com/sd-cross-attention"
    },
    {
        "ID": "DLA-028",
        "Category": "5.手法（応用）",
        "Question": "画像とテキストを共通のベクトル空間に写像し、その類似度を最大化するように学習したモデルは？",
        "Opt1": "BERT",
        "Opt2": "GPT",
        "Opt3": "CLIP",
        "Opt4": "ResNet",
        "Opt5": "VGG",
        "Opt6": "YOLO",
        "Answer_Idx": 2,
        "Explanation": "Contrastive Language-Image Pre-training。ゼロショットでの画像分類などを可能にしました。",
        "Link": "https://example.com/clip-details"
    },
    {
        "ID": "DLA-029",
        "Category": "5.手法（応用）",
        "Question": "CLIPのように、複数のモダリティ（画像・テキスト等）の相関を学習する手法を総称して何というか？",
        "Opt1": "自己教師あり学習",
        "Opt2": "コントラスティブ学習（対照学習）",
        "Opt3": "強化学習",
        "Opt4": "能動学習",
        "Opt5": "半教師あり学習",
        "Opt6": "アンサンブル学習",
        "Answer_Idx": 1,
        "Explanation": "似ているペアを近づけ、似ていないペアを遠ざけるように学習します。",
        "Link": "https://example.com/contrastive-learning"
    },
    {
        "ID": "DLA-030",
        "Category": "5.手法（応用）",
        "Question": "動画解析において、時間方向と空間方向の両方の特徴を抽出するために用いられるCNNは？",
        "Opt1": "1D-CNN",
        "Opt2": "2D-CNN",
        "Opt3": "3D-CNN",
        "Opt4": "RNN",
        "Opt5": "LSTM",
        "Opt6": "GRU",
        "Answer_Idx": 2,
        "Explanation": "フィルタを3次元（縦・横・時間）に動かすことで、動きの情報を捉えます。",
        "Link": "https://example.com/3d-cnn"
    },
    {
        "ID": "DLA-031",
        "Category": "5.手法（応用）",
        "Question": "「犬」というラベルがない状態で、「これは動物で、耳が垂れていて…」という属性情報から未知のクラスを識別する手法は？",
        "Opt1": "Few-shot Learning",
        "Opt2": "Zero-shot Learning",
        "Opt3": "転移学習",
        "Opt4": "能動学習",
        "Opt5": "マルチタスク学習",
        "Opt6": "蒸留",
        "Answer_Idx": 1,
        "Explanation": "一度も学習したことがないクラスを、知識（属性や説明文）から推論します。",
        "Link": "https://example.com/zero-shot"
    },
    {
        "ID": "DLA-032",
        "Category": "5.手法（応用）",
        "Question": "強化学習において、エージェントが過去の成功体験に固執せず、未知の行動を試すことを何というか？",
        "Opt1": "活用（Exploitation）",
        "Opt2": "探索（Exploration）",
        "Opt3": "収束",
        "Opt4": "発散",
        "Opt5": "初期化",
        "Opt6": "正規化",
        "Answer_Idx": 1,
        "Explanation": "「探索」と「活用」のトレードオフをどう管理するかが強化学習の鍵です。",
        "Link": "https://example.com/exploration"
    },
    {
        "ID": "DLA-033",
        "Category": "5.手法（応用）",
        "Question": "囲碁AIのAlphaZeroにおいて、勝敗を予測する「バリューネットワーク」と共に使われる、次の一手の確率を出すネットワークは？",
        "Opt1": "Qネットワーク",
        "Opt2": "ポリシーネットワーク（方策ネットワーク）",
        "Opt3": "ターゲットネットワーク",
        "Opt4": "残差ネットワーク",
        "Opt5": "符号化ネットワーク",
        "Opt6": "復号化ネットワーク",
        "Answer_Idx": 1,
        "Explanation": "有望な手の候補を絞り込む役割を果たします。",
        "Link": "https://example.com/policy-network"
    },
    {
        "ID": "DLA-034",
        "Category": "5.手法（応用）",
        "Question": "音声合成において、テキストから直接、人間の声に近い波形を生成するニューラルネットワークモデルの代表例は？",
        "Opt1": "WaveNet",
        "Opt2": "BERT",
        "Opt3": "ResNet",
        "Opt4": "YOLO",
        "Opt5": "SSD",
        "Opt6": "Transformer",
        "Answer_Idx": 0,
        "Explanation": "DeepMindが開発。ピクセル（サンプル）単位で波形を予測し、極めて自然な音声を生成します。",
        "Link": "https://example.com/wavenet"
    },
    {
        "ID": "DLA-035",
        "Category": "5.手法（応用）",
        "Question": "機械翻訳などで、入力文全体を一度に処理せず、出力される単語ごとに逐次処理を行う性質を何というか？",
        "Opt1": "並列性",
        "Opt2": "自己再帰性（Autoregressive）",
        "Opt3": "不変性",
        "Opt4": "局所性",
        "Opt5": "線形性",
        "Opt6": "直交性",
        "Answer_Idx": 1,
        "Explanation": "一つ前に出力した単語を次の入力として使う、GPTなどの言語モデルの基本性質です。",
        "Link": "https://example.com/autoregressive"
    },
    {
        "ID": "DLA-036",
        "Category": "5.手法（応用）",
        "Question": "画像認識の精度向上において、画像を回転させたり、左右反転させたりして、見かけ上のデータ量を増やす手法は？",
        "Opt1": "データ正規化",
        "Opt2": "データ拡張（Data Augmentation）",
        "Opt3": "データ圧縮",
        "Opt4": "データ蒸留",
        "Opt5": "データ量子化",
        "Opt6": "データ剪定",
        "Answer_Idx": 1,
        "Explanation": "過学習を抑え、モデルの頑健性を高めるために必須の前処理です。",
        "Link": "https://example.com/data-aug"
    },
    {
        "ID": "DLA-037",
        "Category": "5.手法（応用）",
        "Question": "Transformerにおいて、単語の出現順序を考慮するために、単語ベクトルに加算される位置情報は？",
        "Opt1": "バイアスベクトル",
        "Opt2": "Positional Encoding",
        "Opt3": "重みベクトル",
        "Opt4": "勾配ベクトル",
        "Opt5": "潜在ベクトル",
        "Opt6": "コンテキストベクトル",
        "Answer_Idx": 1,
        "Explanation": "Attention自体には位置の概念がないため、正弦波などを用いて位置を教えます。",
        "Link": "https://example.com/positional-encoding-dla"
    },
    {
        "ID": "DLA-038",
        "Category": "5.手法（応用）",
        "Question": "LLM（大規模言語モデル）の推論能力を向上させるため、プロンプトで「ステップバイステップで考えて」と指示する手法は？",
        "Opt1": "Few-shot",
        "Opt2": "Chain-of-Thought (CoT)",
        "Opt3": "Zero-shot",
        "Opt4": "Self-Correction",
        "Opt5": "RAG",
        "Opt6": "RLHF",
        "Answer_Idx": 1,
        "Explanation": "思考過程を書き出させることで、複雑な推論問題の正解率が向上します。",
        "Link": "https://example.com/cot-prompting"
    },
    {
        "ID": "DLA-039",
        "Category": "5.手法（応用）",
        "Question": "画像から特徴を抽出するエンコーダ部分に、CNNではなくTransformerを用いた物体検出モデルは？",
        "Opt1": "Faster R-CNN",
        "Opt2": "SSD",
        "Opt3": "Swin-L (Swin Transformer)",
        "Opt4": "YOLOv3",
        "Opt5": "VGG19",
        "Opt6": "MobileNet",
        "Answer_Idx": 2,
        "Explanation": "バックボーンにSwin Transformerなどを用いることで、より広域な文脈を捉えた検出が可能になります。",
        "Link": "https://example.com/backbone-transformer"
    },
    {
        "ID": "DLA-040",
        "Category": "5.手法（応用）",
        "Question": "学習データの一部のみに正解ラベルが付いている場合に、未ラベルデータも活用して学習する手法は？",
        "Opt1": "教師あり学習",
        "Opt2": "教師なし学習",
        "Opt3": "半教師あり学習 (Semi-supervised)",
        "Opt4": "強化学習",
        "Opt5": "転移学習",
        "Opt6": "多タスク学習",
        "Answer_Idx": 2,
        "Explanation": "コストのかかるラベル付けを最小限にしつつ、大量のデータから知識を得る手法です。",
        "Link": "https://example.com/semi-supervised"
    },
    {
        "ID": "DLA-041",
        "Category": "5.手法（応用）",
        "Question": "Googleが開発した、翻訳や要約などあらゆるタスクを「Text-to-Text」の形式で解くTransformerモデルは？",
        "Opt1": "BERT",
        "Opt2": "GPT",
        "Opt3": "T5 (Text-to-Text Transfer Transformer)",
        "Opt4": "VGG",
        "Opt5": "ResNet",
        "Opt6": "YOLO",
        "Answer_Idx": 2,
        "Explanation": "入力も出力もテキストとして扱うことで、同一モデルで多様なタスクを柔軟に処理します。",
        "Link": "https://example.com/t5-model"
    },
    {
        "ID": "DLA-042",
        "Category": "5.手法（応用）",
        "Question": "Metaが開発し、オープンソースに近い形で公開された、高い効率性を持つ大規模言語モデルのシリーズは？",
        "Opt1": "GPT-4",
        "Opt2": "PaLM",
        "Opt3": "Llama",
        "Opt4": "Claude",
        "Opt5": "Gemini",
        "Opt6": "Mistral",
        "Answer_Idx": 2,
        "Explanation": "効率的なパラメータ構成により、比較的小規模なモデルでも高い性能を発揮し、研究開発を加速させました。",
        "Link": "https://example.com/llama-details"
    },
    {
        "ID": "DLA-043",
        "Category": "5.手法（応用）",
        "Question": "LLMにおいて、入力されたプロンプトに対して「次のトークン（単語）」を確率的に選択するプロセスを何というか？",
        "Opt1": "エンコーディング",
        "Opt2": "デコーディング（サンプリング）",
        "Opt3": "正規化",
        "Opt4": "蒸留",
        "Opt5": "量子化",
        "Opt6": "剪定",
        "Answer_Idx": 1,
        "Explanation": "Greedy SearchやTop-pサンプリングなどの手法で、生成の多様性を制御します。",
        "Link": "https://example.com/decoding-strategy"
    },
    {
        "ID": "DLA-044",
        "Category": "5.手法（応用）",
        "Question": "LLMの学習において、特定のタスクに特化させるのではなく、指示に従う能力を高めるための微調整を何というか？",
        "Opt1": "事前学習",
        "Opt2": "インストラクション・チューニング",
        "Opt3": "量子化",
        "Opt4": "継続学習",
        "Opt5": "次元削減",
        "Opt6": "クラスタリング",
        "Answer_Idx": 1,
        "Explanation": "「〜を要約して」といった命令形式のデータで学習させ、対話性能を向上させます。",
        "Link": "https://example.com/instruction-tuning"
    },
    {
        "ID": "DLA-045",
        "Category": "5.手法（応用）",
        "Question": "強化学習において、エージェントが方策を更新する際、急激な変化を抑えて学習を安定させる代表的なアルゴリズムは？",
        "Opt1": "Q学習",
        "Opt2": "DQN",
        "Opt3": "PPO (Proximal Policy Optimization)",
        "Opt4": "モンテカルロ法",
        "Opt5": "TD学習",
        "Opt6": "SARSA",
        "Answer_Idx": 2,
        "Explanation": "OpenAIが開発したアルゴリズムで、現在の深層強化学習の標準的な手法の一つです。",
        "Link": "https://example.com/ppo-details"
    },
    {
        "ID": "DLA-046",
        "Category": "5.手法（応用）",
        "Question": "ChatGPTなどの対話型AIにおいて、回答の「有用性」や「安全性」を人間に評価させてモデルを最適化する手法は？",
        "Opt1": "RAG",
        "Opt2": "RLHF",
        "Opt3": "Few-shot",
        "Opt4": "Chain-of-Thought",
        "Opt5": "Fine-tuning",
        "Opt6": "Distillation",
        "Answer_Idx": 1,
        "Explanation": "Reinforcement Learning from Human Feedback。人間の好みにモデルを「整列（アライメント）」させます。",
        "Link": "https://example.com/rlhf-details"
    },
    {
        "ID": "DLA-047",
        "Category": "5.手法（応用）",
        "Question": "LLMが学習データにない最新ニュースや社内文書を回答に反映させるために、外部知識を検索して参照する技術は？",
        "Opt1": "Fine-tuning",
        "Opt2": "RAG (Retrieval-Augmented Generation)",
        "Opt3": "Prompt Engineering",
        "Opt4": "Zero-shot",
        "Opt5": "蒸留",
        "Opt6": "剪定",
        "Answer_Idx": 1,
        "Explanation": "検索エンジンやデータベースと連携し、ハルシネーション（嘘）を抑制する効果もあります。",
        "Link": "https://example.com/rag-details"
    },
    {
        "ID": "DLA-048",
        "Category": "5.手法（応用）",
        "Question": "大規模なモデルの全パラメータを更新する代わりに、一部の小さな行列のみを学習させる効率的な微調整手法は？",
        "Opt1": "Full Fine-tuning",
        "Opt2": "LoRA (Low-Rank Adaptation)",
        "Opt3": "Dropout",
        "Opt4": "Batch Norm",
        "Opt5": "Early Stopping",
        "Opt6": "Regularization",
        "Answer_Idx": 1,
        "Explanation": "少数のパラメータのみを学習させるため、GPUメモリやストレージを大幅に節約できます。",
        "Link": "https://example.com/lora-details"
    },
    {
        "ID": "DLA-049",
        "Category": "5.手法（応用）",
        "Question": "LLMのプロンプトにおいて、解答の例を一つも提示せずに質問に答えさせることを何というか？",
        "Opt1": "Zero-shot Prompting",
        "Opt2": "One-shot Prompting",
        "Opt3": "Few-shot Prompting",
        "Opt4": "Chain-of-Thought",
        "Opt5": "RAG",
        "Opt6": "RLHF",
        "Answer_Idx": 0,
        "Explanation": "モデルが本来持っている汎用的な知識のみを用いて回答を生成させます。",
        "Link": "https://example.com/zero-shot-prompting"
    },
    {
        "ID": "DLA-050",
        "Category": "5.手法（応用）",
        "Question": "自動運転などで、画像から「周辺車両の位置」と「自車の将来の軌道」を同時に予測するような手法は？",
        "Opt1": "シングルタスク学習",
        "Opt2": "マルチタスク学習",
        "Opt3": "教師なし学習",
        "Opt4": "強化学習",
        "Opt5": "次元圧縮",
        "Opt6": "標準化",
        "Answer_Idx": 1,
        "Explanation": "複数の関連するタスクを同時に学習することで、特徴抽出の精度を高めます。",
        "Link": "https://example.com/multi-task-learning"
    },
    {
        "ID": "DLA-051",
        "Category": "5.手法（応用）",
        "Question": "物体検出モデル「Faster R-CNN」において、物体が存在しそうな領域の候補（Region Proposal）を出すネットワークは？",
        "Opt1": "CNN",
        "Opt2": "RNN",
        "Opt3": "RPN (Region Proposal Network)",
        "Opt4": "GAN",
        "Opt5": "LSTM",
        "Opt6": "Attention",
        "Answer_Idx": 2,
        "Explanation": "スライディングウィンドウを用いず、ネットワーク自体が候補領域を効率的に提案します。",
        "Link": "https://example.com/rpn-details"
    },
    {
        "ID": "DLA-052",
        "Category": "5.手法（応用）",
        "Question": "TransformerのAttention計算において、特定のトークンが他のトークンから受ける影響を決定する「重要度の重み」の算出に使われる関数は？",
        "Opt1": "ReLU",
        "Opt2": "Sigmoid",
        "Opt3": "Softmax",
        "Opt4": "tanh",
        "Opt5": "Step",
        "Opt6": "Linear",
        "Answer_Idx": 2,
        "Explanation": "スコアの合計が1になるように正規化し、どこにどれだけ注力すべきかを確率的に表現します。",
        "Link": "https://example.com/attention-softmax"
    },
    {
        "ID": "DLA-053",
        "Category": "5.手法（応用）",
        "Question": "音声認識の精度を高めるために、単語の並びとしての自然さを評価し、候補を絞り込むモデルを何というか？",
        "Opt1": "音響モデル",
        "Opt2": "言語モデル",
        "Opt3": "発音モデル",
        "Opt4": "画像モデル",
        "Opt5": "回帰モデル",
        "Opt6": "分類モデル",
        "Answer_Idx": 1,
        "Explanation": "「今日は（晴れ/貼れ）」のように、文脈的に正しい単語の組み合わせを選択します。",
        "Link": "https://example.com/language-model-speech"
    },
    {
        "ID": "DLA-054",
        "Category": "5.手法（応用）",
        "Question": "エージェントが環境の中で試行錯誤し、累積報酬を最大化するように行動を学習する枠組みは？",
        "Opt1": "教師あり学習",
        "Opt2": "教師なし学習",
        "Opt3": "強化学習",
        "Opt4": "半教師あり学習",
        "Opt5": "転移学習",
        "Opt6": "能動学習",
        "Answer_Idx": 2,
        "Explanation": "正解ラベルの代わりに、行動の結果として得られる「報酬」を頼りに学習します。",
        "Link": "https://example.com/reinforcement-learning"
    },
    {
        "ID": "DLA-055",
        "Category": "5.手法（応用）",
        "Question": "強化学習において、現在の状態 $s$ で行動 $a$ をとったときの「将来得られる報酬の期待値」を表す関数は？",
        "Opt1": "損失関数",
        "Opt2": "活性化関数",
        "Opt3": "Q関数（行動価値関数）",
        "Opt4": "ソフトマックス関数",
        "Opt5": "目的関数",
        "Opt6": "シグモイド関数",
        "Answer_Idx": 2,
        "Explanation": "このQ値を正確に推定することが、Q学習やDQNの目的です。",
        "Link": "https://example.com/q-function"
    },
    {
        "ID": "DLA-056",
        "Category": "5.手法（応用）",
        "Question": "LLMの推論において、回答のランダム性を制御し、数値が高いほど多様で創造的な（低いほど決定論的な）回答にするパラメータは？",
        "Opt1": "学習率",
        "Opt2": "バッチサイズ",
        "Opt3": "温度（Temperature）",
        "Opt4": "エポック数",
        "Opt5": "重み",
        "Opt6": "バイアス",
        "Answer_Idx": 2,
        "Explanation": "確率分布を平滑化（高い値）または先鋭化（低い値）させることで、出力を調整します。",
        "Link": "https://example.com/llm-temperature"
    },
    {
        "ID": "DLA-057",
        "Category": "5.手法（応用）",
        "Question": "画像生成AIにおいて、初期のランダムなノイズから徐々に目的の画像へと近づけるためのステップを繰り返すアルゴリズムは？",
        "Opt1": "勾配降下法",
        "Opt2": "サンプリング（デノイジング）",
        "Opt3": "正規化",
        "Opt4": "正則化",
        "Opt5": "プーリング",
        "Opt6": "パディング",
        "Answer_Idx": 1,
        "Explanation": "学習した逆拡散過程を何度も適用し、高品質な画像を「彫り出す」ように生成します。",
        "Link": "https://example.com/diffusion-sampling"
    },
    {
        "ID": "DLA-058",
        "Category": "5.手法（応用）",
        "Question": "AlphaGoが人間の棋譜を学習した後に、自分自身と対局を繰り返して能力を高めた手法を何というか？",
        "Opt1": "教師あり学習",
        "Opt2": "自己対局（Self-play）",
        "Opt3": "蒸留",
        "Opt4": "量子化",
        "Opt5": "剪定",
        "Opt6": "能動学習",
        "Answer_Idx": 1,
        "Explanation": "自分自身と戦うことで、人間が到達していない領域の戦略を自律的に獲得しました。",
        "Link": "https://example.com/self-play-alphago"
    },
    {
        "ID": "DLA-059",
        "Category": "5.手法（応用）",
        "Question": "モデルをサーバーに送らず、スマートフォンなどのデバイス上で直接推論を実行する技術を何というか？",
        "Opt1": "クラウドAI",
        "Opt2": "エッジAI",
        "Opt3": "フェデレーション学習",
        "Opt4": "量子コンピュータ",
        "Opt5": "ブロックチェーン",
        "Opt6": "メタバース",
        "Answer_Idx": 1,
        "Explanation": "プライバシー保護、低遅延、通信コスト削減などのメリットがあります。",
        "Link": "https://example.com/edge-ai"
    },
    {
        "ID": "DLA-060",
        "Category": "5.手法（応用）",
        "Question": "深層学習を用いた自動翻訳において、単語の並び（系列）から別の単語の並び（系列）を生成するモデルの総称は？",
        "Opt1": "CNN",
        "Opt2": "GAN",
        "Opt3": "Encoder-Decoder（Seq2Seq）",
        "Opt4": "Autoencoder",
        "Opt5": "SVM",
        "Opt6": "K-means",
        "Answer_Idx": 2,
        "Explanation": "入力文をベクトルに変換し、それを元に出力文を生成する、翻訳や要約の基本構造です。",
        "Link": "https://example.com/seq2seq-details"
    },
    {
        "ID": "DLA-061",
        "Category": "5.手法（応用）",
        "Question": "テキスト・画像・音声を一つのモデルで統合的に処理し、リアルタイムな対話を可能にした「オムニ」モデルは？",
        "Opt1": "GPT-4o",
        "Opt2": "BERT",
        "Opt3": "ResNet",
        "Opt4": "YOLOv8",
        "Opt5": "VGG",
        "Opt6": "Stable Diffusion",
        "Answer_Idx": 0,
        "Explanation": "OpenAIが発表したGPT-4o（omni）は、複数のモダリティを跨いで直接学習・推論を行います。",
        "Link": "https://example.com/gpt-4o-details"
    },
    {
        "ID": "DLA-062",
        "Category": "5.手法（応用）",
        "Question": "オートエンコーダを用いた異常検知において、異常と判定する基準として一般的に使われる値は？",
        "Opt1": "学習率",
        "Opt2": "再構成誤差 (Reconstruction Error)",
        "Opt3": "バッチサイズ",
        "Opt4": "重みの初期値",
        "Opt5": "活性化関数の出力値",
        "Opt6": "エントロピー",
        "Answer_Idx": 1,
        "Explanation": "正常データのみで学習したモデルが、未知のデータをうまく復元できない（誤差が大きい）場合に異常とみなします。",
        "Link": "https://example.com/ae-anomaly"
    },
    {
        "ID": "DLA-063",
        "Category": "5.手法（応用）",
        "Question": "BERTを軽量化するために、モデルの「蒸留（Distillation）」を用いて構築されたモデルは？",
        "Opt1": "RoBERTa",
        "Opt2": "ALBERT",
        "Opt3": "DistilBERT",
        "Opt4": "T5",
        "Opt5": "GPT-3",
        "Opt6": "Llama",
        "Answer_Idx": 2,
        "Explanation": "教師モデルの知識を小さな生徒モデルに継承させることで、精度を維持しつつ高速化しました。",
        "Link": "https://example.com/distilbert"
    },
    {
        "ID": "DLA-064",
        "Category": "5.手法（応用）",
        "Question": "文中の特定の単語に注目するのではなく、文章全体の意味（特徴）を一つのベクトルとして抽出することを何というか？",
        "Opt1": "Sentence Embedding（文埋め込み）",
        "Opt2": "Tokenization",
        "Opt3": "One-Hot Encoding",
        "Opt4": "N-gram",
        "Opt5": "Stemming",
        "Opt6": "Lemmatization",
        "Answer_Idx": 0,
        "Explanation": "Sentence-BERTなどを用いて、文同士の類似度計算や検索に利用されます。",
        "Link": "https://example.com/sentence-embedding"
    },
    {
        "ID": "DLA-065",
        "Category": "5.手法（応用）",
        "Question": "GANを応用し、低解像度の画像から高解像度の画像を生成する「超解像」モデルの代表例は？",
        "Opt1": "DCGAN",
        "Opt2": "CycleGAN",
        "Opt3": "SRGAN",
        "Opt4": "Pix2Pix",
        "Opt5": "StackGAN",
        "Opt6": "StyleGAN",
        "Answer_Idx": 2,
        "Explanation": "Super-Resolution GAN。識別器が「本物の高解像度か」を判定することで、細部が鮮明な画像を生成します。",
        "Link": "https://example.com/srgan-details"
    },
    {
        "ID": "DLA-066",
        "Category": "5.手法（応用）",
        "Question": "LLMにおいて、思考過程を段階的に出力させることで数学などの論理的課題を解く能力を高めたOpenAIのモデルは？",
        "Opt1": "o1 (OpenAI o1)",
        "Opt2": "GPT-2",
        "Opt3": "BERT",
        "Opt4": "ResNet",
        "Opt5": "MobileNet",
        "Opt6": "VGG",
        "Answer_Idx": 0,
        "Explanation": "強化学習による思考の連鎖（CoT）をモデル内部で実行し、高度な推論を可能にしました。",
        "Link": "https://example.com/openai-o1"
    },
    {
        "ID": "DLA-067",
        "Category": "5.手法（応用）",
        "Question": "畳み込みニューラルネットワークにおいて、全結合層を使わずに「クラスごとの特徴マップの平均」をとる手法は？",
        "Opt1": "Max Pooling",
        "Opt2": "Global Average Pooling (GAP)",
        "Opt3": "Dropout",
        "Opt4": "Flatten",
        "Opt5": "Batch Norm",
        "Opt6": "Softmax",
        "Answer_Idx": 1,
        "Explanation": "パラメータ数を大幅に削減し、過学習を抑制するとともに、任意の入力サイズに対応可能にします。",
        "Link": "https://example.com/gap-details"
    },
    {
        "ID": "DLA-068",
        "Category": "5.手法（応用）",
        "Question": "音声認識のタスクにおいて、入力の音声特徴量と出力の文字の長さが異なる場合に、その対応付けを学習する手法は？",
        "Opt1": "CTC (Connectionist Temporal Classification)",
        "Opt2": "ReLU",
        "Opt3": "Softmax",
        "Opt4": "PCA",
        "Opt5": "LIME",
        "Opt6": "SHAP",
        "Answer_Idx": 0,
        "Explanation": "空白（blank）記号を導入し、時系列データのアライメントを自動で行うアルゴリズムです。",
        "Link": "https://example.com/ctc-details"
    },
    {
        "ID": "DLA-069",
        "Category": "5.手法（応用）",
        "Question": "深層学習モデルの予測結果に対して、個々の入力データがどの程度寄与したかを算出する手法（説明可能なAI）は？",
        "Opt1": "Grad-CAM",
        "Opt2": "SHAP",
        "Opt3": "LIME",
        "Opt4": "t-SNE",
        "Opt5": "PCA",
        "Opt6": "ROC",
        "Answer_Idx": 1,
        "Explanation": "ゲーム理論の「シャープレイ値」を応用し、公平で数学的な根拠に基づく寄与度を算出します。",
        "Link": "https://example.com/shap-details"
    },
    {
        "ID": "DLA-070",
        "Category": "5.手法（応用）",
        "Question": "特定のドメイン（医療、法律等）に特化したLLMを作る際、少量の専門データを用いて重みを微調整することを何というか？",
        "Opt1": "事前学習",
        "Opt2": "ドメイン適応（ファインチューニング）",
        "Opt3": "量子化",
        "Opt4": "蒸留",
        "Opt5": "剪定",
        "Opt6": "正則化",
        "Answer_Idx": 1,
        "Explanation": "汎用的なモデルに対し、特定の分野の知識や口調を学習させます。",
        "Link": "https://example.com/domain-adaptation"
    },
    {
        "ID": "DLA-071",
        "Category": "5.手法（応用）",
        "Question": "画像生成AIにおいて、特定の人物やキャラクターの画風を追加で学習させるために使われる軽量な学習手法は？",
        "Opt1": "LoRA",
        "Opt2": "Full Fine-tuning",
        "Opt3": "Distillation",
        "Opt4": "Pruning",
        "Opt5": "Quantization",
        "Opt6": "Zero-shot",
        "Answer_Idx": 0,
        "Explanation": "モデル全体の重みを変えず、小さな追加層のみを学習させることで特定のスタイルを再現します。",
        "Link": "https://example.com/lora-gen-ai"
    },
    {
        "ID": "DLA-072",
        "Category": "5.手法（応用）",
        "Question": "モデルの予測結果の信頼性を評価するために、出力される「確率」が実際の「正解率」と一致しているかを検証することを何というか？",
        "Opt1": "正規化",
        "Opt2": "標準化",
        "Opt3": "較正（キャリブレーション）",
        "Opt4": "正則化",
        "Opt5": "蒸留",
        "Opt6": "剪定",
        "Answer_Idx": 2,
        "Explanation": "「確率80%」と予測したものが、実際に80%の確率で正解しているかを評価します。",
        "Link": "https://example.com/calibration"
    },
    {
        "ID": "DLA-073",
        "Category": "5.手法（応用）",
        "Question": "時系列データの予測において、過去のデータだけでなく、曜日やイベント情報などの外部変数を組み込む手法は？",
        "Opt1": "共変量（外生変数）の導入",
        "Opt2": "単回帰分析",
        "Opt3": "主成分分析",
        "Opt4": "クラスタリング",
        "Opt5": "次元削減",
        "Opt6": "正規化",
        "Answer_Idx": 0,
        "Explanation": "より精度の高い需要予測や株価予測を行うために必須の構成要素です。",
        "Link": "https://example.com/exogenous-variables"
    },
    {
        "ID": "DLA-074",
        "Category": "5.手法（応用）",
        "Question": "LLMのコンテキストウィンドウ（一度に読み込める情報量）を広げるために使われる、位置エンコーディングの拡張手法は？",
        "Opt1": "RoPE (Rotary Positional Embedding)",
        "Opt2": "Absolute Positional Encoding",
        "Opt3": "One-Hot",
        "Opt4": "Bag-of-Words",
        "Opt5": "TF-IDF",
        "Opt6": "N-gram",
        "Answer_Idx": 0,
        "Explanation": "回転行列を用いて位置情報を表現することで、より長い文章の相対的な位置関係を維持します。",
        "Link": "https://example.com/rope-details"
    },
    {
        "ID": "DLA-075",
        "Category": "5.手法（応用）",
        "Question": "画像から抽出した特徴ベクトルを元に、データベースから似た画像を高速に検索する技術を何というか？",
        "Opt1": "画像分類",
        "Opt2": "物体検出",
        "Opt3": "画像検索（イメージリカバリ）",
        "Opt4": "セグメンテーション",
        "Opt5": "超解像",
        "Opt6": "異常検知",
        "Answer_Idx": 2,
        "Explanation": "ベクトル検索エンジンなどを用いて、数億件のデータから瞬時に類似画像を特定します。",
        "Link": "https://example.com/image-retrieval"
    },
    {
        "ID": "DLA-076",
        "Category": "5.手法（応用）",
        "Question": "モデルの推論を高速化するために、不要な演算をスキップしたり、ハードウェアに最適化した実行形式に変換するツールは？",
        "Opt1": "TensorRT",
        "Opt2": "PyTorch",
        "Opt3": "TensorFlow",
        "Opt4": "Scikit-learn",
        "Opt5": "Pandas",
        "Opt6": "NumPy",
        "Answer_Idx": 0,
        "Explanation": "NVIDIAが提供する推論最適化ライブラリで、エッジデバイス等での実行を加速します。",
        "Link": "https://example.com/tensorrt"
    },
    {
        "ID": "DLA-077",
        "Category": "5.手法（応用）",
        "Question": "GANにおいて、特定のラベル（「猫」「犬」など）を指定して画像を生成する手法を何というか？",
        "Opt1": "DCGAN",
        "Opt2": "Conditional GAN (cGAN)",
        "Opt3": "CycleGAN",
        "Opt4": "WGAN",
        "Opt5": "StyleGAN",
        "Opt6": "VAE",
        "Answer_Idx": 1,
        "Explanation": "GeneratorとDiscriminatorの両方に条件（ラベル）を与えることで、生成結果を制御します。",
        "Link": "https://example.com/cgan-details"
    },
    {
        "ID": "DLA-078",
        "Category": "5.手法（応用）",
        "Question": "推薦システムにおいて、ユーザーの行動履歴だけでなく、商品画像や説明文の特徴も組み合わせて推薦する手法は？",
        "Opt1": "協調フィルタリング",
        "Opt2": "コンテンツベース・フィルタリング",
        "Opt3": "ハイブリッド推薦",
        "Opt4": "ランダム推薦",
        "Opt5": "バンディット",
        "Opt6": "回帰",
        "Answer_Idx": 2,
        "Explanation": "複数の情報源（モダリティ）を統合して、より精度の高いレコメンドを行います。",
        "Link": "https://example.com/hybrid-recommendation"
    },
    {
        "ID": "DLA-079",
        "Category": "5.手法（応用）",
        "Question": "LLMが回答を生成する際、直前の単語の繰り返しを抑制したり、特定の単語の出現率を制御したりする設定は？",
        "Opt1": "Repetition Penalty",
        "Opt2": "Learning Rate",
        "Opt3": "Batch Size",
        "Opt4": "Epoch",
        "Opt5": "Dropout",
        "Opt6": "Momentum",
        "Answer_Idx": 0,
        "Explanation": "同じ言葉をループしてしまう現象を防ぎ、文章の品質を高めるために使われます。",
        "Link": "https://example.com/repetition-penalty"
    },
    {
        "ID": "DLA-080",
        "Category": "5.手法（応用）",
        "Question": "自動運転の安全性評価などで使われる、AIが判断を誤る「最悪のケース」を意図的に作り出してテストすることを何というか？",
        "Opt1": "ABテスト",
        "Opt2": "敵対的テスト (Adversarial Testing)",
        "Opt3": "ユニットテスト",
        "Opt4": "回帰テスト",
        "Opt5": "結合テスト",
        "Opt6": "受入テスト",
        "Answer_Idx": 1,
        "Explanation": "エッジケースにおけるモデルの挙動を確認し、システムの信頼性を高めます。",
        "Link": "https://example.com/adversarial-testing-ai"
    },
    {
        "ID": "DLA-081",
        "Category": "5.手法（応用）",
        "Question": "モデル内の全パラメータを動かさず、入力に応じて一部の「エキスパート」層のみを選択的に活性化させる手法は？",
        "Opt1": "蒸留",
        "Opt2": "MoE (Mixture of Experts)",
        "Opt3": "量子化",
        "Opt4": "剪定",
        "Opt5": "ドロップアウト",
        "Opt6": "バッチ正規化",
        "Answer_Idx": 1,
        "Explanation": "Mixtral 8x7Bなどで採用されており、巨大なパラメータ数を持ちつつ計算コストを抑える技術です。",
        "Link": "https://example.com/moe-details"
    },
    {
        "ID": "DLA-082",
        "Category": "5.手法（応用）",
        "Question": "MoEにおいて、入力データ（トークン）をどのエキスパートに割り振るかを決定する仕組みを何というか？",
        "Opt1": "アテンション",
        "Opt2": "ゲーティング・ネットワーク（ルーター）",
        "Opt3": "エンコーダ",
        "Opt4": "デコーダ",
        "Opt5": "活性化関数",
        "Opt6": "損失関数",
        "Answer_Idx": 1,
        "Explanation": "Gating Networkが入力に最適な「専門家」を選別し、効率的な推論を実現します。",
        "Link": "https://example.com/moe-gating"
    },
    {
        "ID": "DLA-083",
        "Category": "5.手法（応用）",
        "Question": "拡散モデルにおいて、計算コストのかかるピクセル空間ではなく、圧縮された「潜在空間」でノイズ除去を行う手法は？",
        "Opt1": "GAN",
        "Opt2": "VAE",
        "Opt3": "潜在拡散モデル (LDM / Stable Diffusion)",
        "Opt4": "RNN",
        "Opt5": "LSTM",
        "Opt6": "CNN",
        "Answer_Idx": 2,
        "Explanation": "画像を一度低次元の潜在変数に圧縮してから拡散過程を行うことで、高速かつ高品質な生成を可能にします。",
        "Link": "https://example.com/latent-diffusion"
    },
    {
        "ID": "DLA-084",
        "Category": "5.手法（応用）",
        "Question": "強化学習において、行動の結果がすぐに出ず、将来の報酬を現在の価値に換算するために使われる係数は？",
        "Opt1": "学習率",
        "Opt2": "割引率 (γ)",
        "Opt3": "エピソード数",
        "Opt4": "ステップ数",
        "Opt5": "バッチサイズ",
        "Opt6": "モーメンタム",
        "Answer_Idx": 1,
        "Explanation": "将来の報酬をどれだけ重視するかを決定します。0に近いほど目先の報酬を優先します。",
        "Link": "https://example.com/discount-factor"
    },
    {
        "ID": "DLA-085",
        "Category": "5.手法（応用）",
        "Question": "画像の一部を指定して、その部分だけをAIで描き直したり修正したりする技術を何というか？",
        "Opt1": "アウトペインティング",
        "Opt2": "インペインティング (Inpainting)",
        "Opt3": "超解像",
        "Opt4": "セグメンテーション",
        "Opt5": "物体検出",
        "Opt6": "量子化",
        "Answer_Idx": 1,
        "Explanation": "写真から不要なものを消したり、欠損した部分を補完したりする際に使われます。",
        "Link": "https://example.com/inpainting-details"
    },
    {
        "ID": "DLA-086",
        "Category": "5.手法（応用）",
        "Question": "画像の外側を推論して描き足し、元の画像よりも広い範囲の風景を生成する技術を何というか？",
        "Opt1": "インペインティング",
        "Opt2": "アウトペインティング (Outpainting)",
        "Opt3": "スタイル変換",
        "Opt4": "データ拡張",
        "Opt5": "蒸留",
        "Opt6": "剪定",
        "Answer_Idx": 1,
        "Explanation": "元の画像の文脈を維持したまま、キャンバスを広げる生成技術です。",
        "Link": "https://example.com/outpainting-details"
    },
    {
        "ID": "DLA-087",
        "Category": "5.手法（応用）",
        "Question": "強化学習において、環境のモデル（遷移確率など）をあらかじめ学習し、それを用いてシミュレーションを行う手法は？",
        "Opt1": "モデルフリー強化学習",
        "Opt2": "モデルベース強化学習",
        "Opt3": "教師あり学習",
        "Opt4": "教師なし学習",
        "Opt5": "半教師あり学習",
        "Opt6": "能動学習",
        "Answer_Idx": 1,
        "Explanation": "現実の試行錯誤を減らすため、脳内シミュレーションのように環境を予測しながら学習します。",
        "Link": "https://example.com/model-based-rl"
    },
    {
        "ID": "DLA-088",
        "Category": "5.手法（応用）",
        "Question": "LLMが特定の形式（JSONやコードなど）で回答するように、出力の構造を固定する技術やライブラリを総称して何というか？",
        "Opt1": "プロンプト注入",
        "Opt2": "構造化出力 (Structured Outputs)",
        "Opt3": "ファインチューニング",
        "Opt4": "蒸留",
        "Opt5": "量子化",
        "Opt6": "剪定",
        "Answer_Idx": 1,
        "Explanation": "API連携などで、AIの回答をプログラムで扱いやすくするために重要です。",
        "Link": "https://example.com/structured-outputs"
    },
    {
        "ID": "DLA-089",
        "Category": "5.手法（応用）",
        "Question": "複数のモデルを組み合わせ、それぞれの出力を多数決や平均などで統合して精度を高める手法は？",
        "Opt1": "シングルモデル",
        "Opt2": "アンサンブル学習",
        "Opt3": "転移学習",
        "Opt4": "能動学習",
        "Opt5": "オンライン学習",
        "Opt6": "強化学習",
        "Answer_Idx": 1,
        "Explanation": "個々のモデルの弱点を補い合い、単一モデルよりも高い汎化性能を得ることができます。",
        "Link": "https://example.com/ensemble-dl"
    },
    {
        "ID": "DLA-090",
        "Category": "5.手法（応用）",
        "Question": "AIの信頼性を保証するため、意図しない挙動（差別、バイアス、有害情報）を抑制する「防護柵」のような仕組みは？",
        "Opt1": "バックドア",
        "Opt2": "ガードレール (AI Guardrails)",
        "Opt3": "ファイヤウォール",
        "Opt4": "サンドボックス",
        "Opt5": "ハブ",
        "Opt6": "スイッチ",
        "Answer_Idx": 1,
        "Explanation": "入力と出力の両方を監視し、安全な対話を維持するためのコンポーネントです。",
        "Link": "https://example.com/ai-guardrails"
    },
    {
        "ID": "DLA-091",
        "Category": "5.手法（応用）",
        "Question": "機械翻訳の評価指標で、人間が作成した正解文とシステムが生成した文の「単語の一致度」を測定するものは？",
        "Opt1": "MSE",
        "Opt2": "F値",
        "Opt3": "BLEUスコア",
        "Opt4": "ROC曲線",
        "Opt5": "AUC",
        "Opt6": "perplexity",
        "Answer_Idx": 2,
        "Explanation": "Bilingual Evaluation Understudy。翻訳の質を客観的に数値化する標準的な指標です。",
        "Link": "https://example.com/bleu-score"
    },
    {
        "ID": "DLA-092",
        "Category": "5.手法（応用）",
        "Question": "言語モデルの予測の「不確実性」や「迷い」を示す指標で、値が小さいほど予測が安定していることを示すものは？",
        "Opt1": "Accuracy",
        "Opt2": "Recall",
        "Opt3": "Perplexity (PPL)",
        "Opt4": "Precision",
        "Opt5": "F1-score",
        "Opt6": "IoU",
        "Answer_Idx": 2,
        "Explanation": "「当惑度」とも訳され、次の単語をどれだけ絞り込めているかの尺度になります。",
        "Link": "https://example.com/perplexity-details"
    },
    {
        "ID": "DLA-093",
        "Category": "5.手法（応用）",
        "Question": "特定のプロンプトを入力することで、AIの安全制限を回避させ、不適切な回答を引き出す攻撃手法を何というか？",
        "Opt1": "SQL注入",
        "Opt2": "プロンプト・インジェクション（脱獄/Jailbreak）",
        "Opt3": "クロスサイトスクリプティング",
        "Opt4": "DDoS攻撃",
        "Opt5": "ブルートフォース攻撃",
        "Opt6": "フィッシング",
        "Answer_Idx": 1,
        "Explanation": "「悪い役割を演じて」などと指示し、本来禁止されている情報を出力させる行為です。",
        "Link": "https://example.com/prompt-injection"
    },
    {
        "ID": "DLA-094",
        "Category": "5.手法（応用）",
        "Question": "画像生成において、特定の構図やポーズを維持したまま、別の画像を生成するための補助ネットワークは？",
        "Opt1": "GAN",
        "Opt2": "VAE",
        "Opt3": "ControlNet",
        "Opt4": "U-Net",
        "Opt5": "ResNet",
        "Opt6": "YOLO",
        "Answer_Idx": 2,
        "Explanation": "線画や深度、ポーズ情報を入力として与え、生成AIの出力を精密に制御します。",
        "Link": "https://example.com/controlnet-details"
    },
    {
        "ID": "DLA-095",
        "Category": "5.手法（応用）",
        "Question": "Transformerベースのモデルで、過去の長い対話履歴などを効率的に記憶（再利用）するための仕組みは？",
        "Opt1": "KVキャッシュ (Key-Value Cache)",
        "Opt2": "ドロップアウト",
        "Opt3": "バッチ正規化",
        "Opt4": "ReLU",
        "Opt5": "Softmax",
        "Opt6": "L1正則化",
        "Answer_Idx": 0,
        "Explanation": "一度計算したKeyとValueを保持しておくことで、推論時の計算量を大幅に削減します。",
        "Link": "https://example.com/kv-cache"
    },
    {
        "ID": "DLA-096",
        "Category": "5.手法（応用）",
        "Question": "エージェントが「新しいもの」や「珍しいもの」に興味を持つように報酬を与える、強化学習の仕組みは？",
        "Opt1": "外的報酬",
        "Opt2": "内的報酬（内発的動機付け）",
        "Opt3": "ペナルティ",
        "Opt4": "バイアス",
        "Opt5": "正規化",
        "Opt6": "標準化",
        "Answer_Idx": 1,
        "Explanation": "報酬が稀な環境でも、好奇心（Intrinsic Motivation）を元に探索を継続させます。",
        "Link": "https://example.com/intrinsic-motivation"
    },
    {
        "ID": "DLA-097",
        "Category": "5.手法（応用）",
        "Question": "ディープラーニングモデルの開発において、データの収集、学習、デプロイ、監視の一連のサイクルを自動化・管理する考え方は？",
        "Opt1": "DevOps",
        "Opt2": "MLOps",
        "Opt3": "DataOps",
        "Opt4": "AIOps",
        "Opt5": "FinOps",
        "Opt6": "SysOps",
        "Answer_Idx": 1,
        "Explanation": "モデルの鮮度を保ち、実社会で安定して運用するためのエンジニアリング手法です。",
        "Link": "https://example.com/mlops-intro"
    },
    {
        "ID": "DLA-098",
        "Category": "5.手法（応用）",
        "Question": "EU（欧州連合）で合意された、世界初の包括的なAI規制法案を何というか？",
        "Opt1": "GDPR",
        "Opt2": "DMA",
        "Opt3": "EU AI法 (AI Act)",
        "Opt4": "DSMA",
        "Opt5": "PIPL",
        "Opt6": "HIPAA",
        "Answer_Idx": 2,
        "Explanation": "リスクに応じてAIを分類し、透明性や安全性の義務を課す画期的な法律です。",
        "Link": "https://example.com/eu-ai-act"
    },
    {
        "ID": "DLA-099",
        "Category": "5.手法（応用）",
        "Question": "モデルの予測に対する「自信のなさ」を可視化し、リスクの高い判断を人間に委ねるために使われる指標の総称は？",
        "Opt1": "確信度（Confidence Score）",
        "Opt2": "正解率",
        "Opt3": "適合率",
        "Opt4": "再現率",
        "Opt5": "F値",
        "Opt6": "損失値",
        "Answer_Idx": 0,
        "Explanation": "ソフトマックスの出力値などを元に、AIの判断の不確かさを定量化します。",
        "Link": "https://example.com/confidence-score"
    },
    {
        "ID": "DLA-100",
        "Category": "5.手法（応用）",
        "Question": "あるモデルが「特定の人間の筆跡」や「特定の作品」を学習データから削除するように再学習・調整するプロセスは？",
        "Opt1": "蒸留",
        "Opt2": "量子化",
        "Opt3": "マシン・アンラーニング (Machine Unlearning)",
        "Opt4": "剪定",
        "Opt5": "正則化",
        "Opt6": "標準化",
        "Answer_Idx": 2,
        "Explanation": "プライバシー保護や著作権対応のため、特定の情報をモデルから「忘れさせる」技術です。",
        "Link": "https://example.com/machine-unlearning"
    },
    {
        "ID": "DLA-101",
        "Category": "5.手法（応用）",
        "Question": "LLMが自ら判断し、外部のツール（ブラウザ、計算機、コード実行環境）を呼び出してタスクを完遂する仕組みを何というか？",
        "Opt1": "AIエージェント",
        "Opt2": "静的モデル",
        "Opt3": "蒸留モデル",
        "Opt4": "ハルシネーション",
        "Opt5": "正則化",
        "Opt6": "標準化",
        "Answer_Idx": 0,
        "Explanation": "推論（Planning）と道具の使用（Tool Use）を組み合わせ、自律的に動くAIの形態です。",
        "Link": "https://example.com/ai-agents"
    },
    {
        "ID": "DLA-102",
        "Category": "5.手法（応用）",
        "Question": "OpenAIが発表した、最長1分間の高品質な動画をテキストから生成できるモデルは？",
        "Opt1": "VGG",
        "Opt2": "Sora",
        "Opt3": "YOLO",
        "Opt4": "BERT",
        "Opt5": "ResNet",
        "Opt6": "MobileNet",
        "Answer_Idx": 1,
        "Explanation": "拡散モデルとTransformerを組み合わせ、時空間パッチを扱うことで一貫性のある動画を生成します。",
        "Link": "https://example.com/sora-details"
    },
    {
        "ID": "DLA-103",
        "Category": "5.手法（応用）",
        "Question": "LLMの総合的な知識や推論能力を測定するために用いられる、57の被験分野を含む標準的なベンチマークは？",
        "Opt1": "BLEU",
        "Opt2": "ROUGE",
        "Opt3": "MMLU (Massive Multitask Language Understanding)",
        "Opt4": "GLUE",
        "Opt5": "SQuAD",
        "Opt6": "ImageNet",
        "Answer_Idx": 2,
        "Explanation": "大学入試レベルの知識を含む多肢選択式問題で、モデルの賢さを測る世界的な基準です。",
        "Link": "https://example.com/mmlu-benchmark"
    },
    {
        "ID": "DLA-104",
        "Category": "5.手法（応用）",
        "Question": "タンパク質の3次元構造をアミノ酸配列から高精度に予測し、創薬や生物学に革命を起こしたAIモデルは？",
        "Opt1": "AlphaFold",
        "Opt2": "AlphaGo",
        "Opt3": "AlphaZero",
        "Opt4": "GPT-4",
        "Opt5": "Stable Diffusion",
        "Opt6": "VGG",
        "Answer_Idx": 0,
        "Explanation": "Google DeepMindが開発。数十年かかっていた構造解析を瞬時に行うことを可能にしました。",
        "Link": "https://example.com/alphafold-details"
    },
    {
        "ID": "DLA-105",
        "Category": "5.手法（応用）",
        "Question": "自動運転の文脈で、カメラやLiDARなどの複数のセンサー情報を統合して周囲を認識することを何というか？",
        "Opt1": "センサーフュージョン",
        "Opt2": "データ拡張",
        "Opt3": "量子化",
        "Opt4": "蒸留",
        "Opt5": "剪定",
        "Opt6": "正則化",
        "Answer_Idx": 0,
        "Explanation": "異なる特性を持つセンサーを組み合わせることで、夜間や悪天候下でも高い安全性を確保します。",
        "Link": "https://example.com/sensor-fusion"
    },
    {
        "ID": "DLA-106",
        "Category": "5.手法（応用）",
        "Question": "LLMが長い文章を読み込む際、Attentionの計算コストを抑えつつ、数百万トークンの入力を可能にする技術は？",
        "Opt1": "Ring Attention",
        "Opt2": "ReLU",
        "Opt3": "Batch Norm",
        "Opt4": "Dropout",
        "Opt5": "L2正則化",
        "Opt6": "早期終了",
        "Answer_Idx": 0,
        "Explanation": "計算を複数のGPUに分散させ、環状に情報を伝播させることで超長文の処理を実現します。",
        "Link": "https://example.com/ring-attention"
    },
    {
        "ID": "DLA-107",
        "Category": "5.手法（応用）",
        "Question": "画像生成において、特定の単語（「猫」など）が生成画像に反映されないように重みをマイナスに調整するプロンプトを何というか？",
        "Opt1": "ポジティブプロンプト",
        "Opt2": "ネガティブプロンプト",
        "Opt3": "ゼロショット",
        "Opt4": "フューショット",
        "Opt5": "CoT",
        "Opt6": "RAG",
        "Answer_Idx": 1,
        "Explanation": "生成したくない要素（例：歪んだ手、低い画質）を指定するために多用されます。",
        "Link": "https://example.com/negative-prompt"
    },
    {
        "ID": "DLA-108",
        "Category": "5.手法（応用）",
        "Question": "AIが既存の科学論文を読み込み、新しい仮説を提案したり実験計画を立てたりする「科学のためのAI」を何というか？",
        "Opt1": "AI for Science (AI4S)",
        "Opt2": "Generative AI",
        "Opt3": "Edge AI",
        "Opt4": "Explainable AI",
        "Opt5": "Federated Learning",
        "Opt6": "AutoML",
        "Answer_Idx": 0,
        "Explanation": "物理、化学、材料科学などの分野でAIを活用し、研究スピードを加速させる取り組みです。",
        "Link": "https://example.com/ai4s-details"
    },
    {
        "ID": "DLA-109",
        "Category": "5.手法（応用）",
        "Question": "Transformerベースのモデルにおいて、推論速度を上げるために、複数のトークンを一度に並列予測する手法は？",
        "Opt1": "投機的デコーディング (Speculative Decoding)",
        "Opt2": "逐次デコーディング",
        "Opt3": "ビームサーチ",
        "Opt4": "貪欲法",
        "Opt5": "量子化",
        "Opt6": "剪定",
        "Answer_Idx": 0,
        "Explanation": "小さなモデルが先行して予測し、大きなモデルがそれを一括検証することで高速化します。",
        "Link": "https://example.com/speculative-decoding"
    },
    {
        "ID": "DLA-110",
        "Category": "5.手法（応用）",
        "Question": "医療AIにおいて、X線写真などの画像データと、電子カルテのテキストデータを組み合わせて診断を行う手法は？",
        "Opt1": "シングルモーダル学習",
        "Opt2": "マルチモーダル学習",
        "Opt3": "教師なし学習",
        "Opt4": "強化学習",
        "Opt5": "次元削減",
        "Opt6": "クラスタリング",
        "Answer_Idx": 1,
        "Explanation": "画像とテキストの両方の文脈を理解することで、より精度の高い診断支援が可能になります。",
        "Link": "https://example.com/medical-multimodal"
    },
    {
        "ID": "DLA-111",
        "Category": "5.手法（応用）",
        "Question": "LLMの性能評価において、既存のベンチマークが学習データに含まれている（カンニング状態）ことを何というか？",
        "Opt1": "汎化",
        "Opt2": "データ汚染 (Data Contamination)",
        "Opt3": "オーバーフィッティング",
        "Opt4": "ハルシネーション",
        "Opt5": "アンダーフィッティング",
        "Opt6": "正則化",
        "Answer_Idx": 1,
        "Explanation": "ベンチマーク問題が学習データに混入していると、真の能力を測定できなくなります。",
        "Link": "https://example.com/data-contamination"
    },
    {
        "ID": "DLA-112",
        "Category": "5.手法（応用）",
        "Question": "モデルに「〜として振る舞ってください」と役割を与えることで、回答の質やトーンを制御する手法は？",
        "Opt1": "ペルソナ・プロンプティング",
        "Opt2": "ネガティブプロンプト",
        "Opt3": "Few-shot",
        "Opt4": "蒸留",
        "Opt5": "量子化",
        "Opt6": "剪定",
        "Answer_Idx": 0,
        "Explanation": "「専門家として」「子供向けに」などの役割（System Prompt）を指定する手法です。",
        "Link": "https://example.com/persona-prompt"
    },
    {
        "ID": "DLA-113",
        "Category": "5.手法（応用）",
        "Question": "ロボットが人間の動作映像を見て、その動作を真似ることで新しいスキルを獲得する学習法は？",
        "Opt1": "逆強化学習",
        "Opt2": "模倣学習 (Imitation Learning)",
        "Opt3": "能動学習",
        "Opt4": "半教師あり学習",
        "Opt5": "転移学習",
        "Opt6": "自己学習",
        "Answer_Idx": 1,
        "Explanation": "報酬関数の設計が難しいタスクでも、手本を示すことで効率的に学習できます。",
        "Link": "https://example.com/imitation-learning"
    },
    {
        "ID": "DLA-114",
        "Category": "5.手法（応用）",
        "Question": "複数のLLMを戦わせ、どちらの回答が優れているかを人間に投票させることでランキングを作る評価サイトは？",
        "Opt1": "LMSYS Chatbot Arena",
        "Opt2": "ImageNet",
        "Opt3": "Kaggle",
        "Opt4": "GitHub",
        "Opt5": "arXiv",
        "Opt6": "Stack Overflow",
        "Answer_Idx": 0,
        "Explanation": "Eloレーティングを用いた、モデルの「使い心地」を測る最も信頼性の高い指標の一つです。",
        "Link": "https://example.com/chatbot-arena"
    },
    {
        "ID": "DLA-115",
        "Category": "5.手法（応用）",
        "Question": "画像生成AIが学習データにある著作権物をそのまま出力してしまうリスクを軽減する技術は？",
        "Opt1": "著作権フィルタリング / コンテンツ・モデレーション",
        "Opt2": "蒸留",
        "Opt3": "量子化",
        "Opt4": "剪定",
        "Opt5": "正則化",
        "Opt6": "標準化",
        "Answer_Idx": 0,
        "Explanation": "出力段階で既存の著作物との類似性をチェックし、ブロックする仕組みです。",
        "Link": "https://example.com/ai-copyright-tech"
    },
    {
        "ID": "DLA-116",
        "Category": "5.手法（応用）",
        "Question": "プロンプトの最後に「この回答が間違っていたら100ドルの罰金を払います」といった感情的な重みを加える手法は？",
        "Opt1": "Emotional Stimuli (エモーショナル・プロンプト)",
        "Opt2": "Zero-shot",
        "Opt3": "CoT",
        "Opt4": "RAG",
        "Opt5": "RLHF",
        "Opt6": "PEFT",
        "Answer_Idx": 0,
        "Explanation": "LLMに心理的なプレッシャーに近い重みを与えることで、精度が向上するという研究結果があります。",
        "Link": "https://example.com/emotional-prompt"
    },
    {
        "ID": "DLA-117",
        "Category": "5.手法（応用）",
        "Question": "動画から人物の動き（ボーン）を抽出し、それを別のキャラクターに適用して動かす技術は？",
        "Opt1": "モーションキャプチャ（AIベース）",
        "Opt2": "物体検出",
        "Opt3": "セグメンテーション",
        "Opt4": "超解像",
        "Opt5": "異常検知",
        "Opt6": "回帰",
        "Answer_Idx": 0,
        "Explanation": "姿勢推定モデル（OpenPose等）を用いて、動画内の動きをデジタル化・転移させます。",
        "Link": "https://example.com/ai-motion-capture"
    },
    {
        "ID": "DLA-118",
        "Category": "5.手法（応用）",
        "Question": "LLMにおいて、入力できる最大文字数（トークン数）の制限を何というか？",
        "Opt1": "コンテキストウィンドウ",
        "Opt2": "バッチサイズ",
        "Opt3": "エポック",
        "Opt4": "パディング",
        "Opt5": "ストライド",
        "Opt6": "カーネル",
        "Answer_Idx": 0,
        "Explanation": "この範囲を超えた情報はモデルが忘れてしまうため、長文処理のボトルネックとなります。",
        "Link": "https://example.com/context-window"
    },
    {
        "ID": "DLA-119",
        "Category": "5.手法（応用）",
        "Question": "物理学の法則（微分方程式など）を損失関数に組み込み、データが少なくても物理的に正しい予測をさせるAIは？",
        "Opt1": "PINNs (Physics-Informed Neural Networks)",
        "Opt2": "CNN",
        "Opt3": "RNN",
        "Opt4": "GAN",
        "Opt5": "Transformer",
        "Opt6": "MLP",
        "Answer_Idx": 0,
        "Explanation": "物理的制約を守るように学習させることで、科学シミュレーションの精度を高めます。",
        "Link": "https://example.com/pinns-details"
    },
    {
        "ID": "DLA-120",
        "Category": "5.手法（応用）",
        "Question": "LLMの出力を別のLLM（評価用モデル）が採点し、人間による評価を代替させる手法を何というか？",
        "Opt1": "LLM-as-a-Judge",
        "Opt2": "RLHF",
        "Opt3": "RAG",
        "Opt4": "蒸留",
        "Opt5": "量子化",
        "Opt6": "剪定",
        "Answer_Idx": 0,
        "Explanation": "人間の評価コストを削減し、高速かつ大規模にモデルを評価するために使われます。",
        "Link": "https://example.com/llm-as-a-judge"
    },
    {
        "ID": "DLA-121",
        "Category": "5.手法（応用）",
        "Question": "学習データに含まれる社会的偏見をAIが引き継がないよう、出力の公平性を数学的に評価・補正する技術を何というか？",
        "Opt1": "公平性（Fairness）技術",
        "Opt2": "蒸留",
        "Opt3": "量子化",
        "Opt4": "剪定",
        "Opt5": "正則化",
        "Opt6": "標準化",
        "Answer_Idx": 0,
        "Explanation": "性別、年齢、人種などによる不当な差別を防ぐための評価指標（Demographic Parity等）が含まれます。",
        "Link": "https://example.com/ai-fairness"
    },
    {
        "ID": "DLA-122",
        "Category": "5.手法（応用）",
        "Question": "AIの学習における膨大な消費電力とCO2排出量を削減するため、効率的な学習を重視する考え方を何というか？",
        "Opt1": "Green AI",
        "Opt2": "Red AI",
        "Opt3": "Blue AI",
        "Opt4": "Edge AI",
        "Opt5": "Dark AI",
        "Opt6": "Cloud AI",
        "Answer_Idx": 0,
        "Explanation": "精度の追求だけでなく、環境負荷（カーボンフットプリント）を抑える開発が求められています。",
        "Link": "https://example.com/green-ai"
    },
    {
        "ID": "DLA-123",
        "Category": "5.手法（応用）",
        "Question": "スマートフォンや家電に搭載される、AI（ディープラーニング）の推論処理を高速かつ低電力で行う専用チップの総称は？",
        "Opt1": "CPU",
        "Opt2": "GPU",
        "Opt3": "NPU (Neural Processing Unit)",
        "Opt4": "FPGA",
        "Opt5": "DSP",
        "Opt6": "ASIC",
        "Answer_Idx": 2,
        "Explanation": "行列演算に特化した回路で、エッジデバイスでのリアルタイムAI処理を支えています。",
        "Link": "https://example.com/npu-details"
    },
    {
        "ID": "DLA-124",
        "Category": "5.手法（応用）",
        "Question": "日本政府が策定した、AIの開発者や利用者が守るべき指針をまとめた文書は？",
        "Opt1": "AI開発ガイドライン",
        "Opt2": "AI利活用ガイドライン",
        "Opt3": "AI事業者ガイドライン（第1.0版）",
        "Opt4": "AI倫理憲章",
        "Opt5": "AI安全保障",
        "Opt6": "AI基本法",
        "Answer_Idx": 2,
        "Explanation": "2024年に総務省と経済産業省が既存のガイドラインを統合して策定しました。",
        "Link": "https://example.com/jp-ai-guideline"
    },
    {
        "ID": "DLA-125",
        "Category": "5.手法（応用）",
        "Question": "モデルに悪意のあるデータを学習させ、特定の条件下で誤作動を起こす「バックドア」を仕込む攻撃を何というか？",
        "Opt1": "プロンプト注入",
        "Opt2": "データポイズニング（データ汚染攻撃）",
        "Opt3": "DDoS攻撃",
        "Opt4": "フィッシング",
        "Opt5": "スクレイピング",
        "Opt6": "スニッフィング",
        "Answer_Idx": 1,
        "Explanation": "学習段階で不正なデータを混ぜることで、運用時にモデルの挙動を操作する攻撃です。",
        "Link": "https://example.com/data-poisoning"
    },
    {
        "ID": "DLA-126",
        "Category": "5.手法（応用）",
        "Question": "AIの開発において、学習に使用した著作物の権利者に適切な対価を支払うための議論が進んでいる概念は？",
        "Opt1": "知的財産権",
        "Opt2": "オプトアウト",
        "Opt3": "著作権の非享受利用",
        "Opt4": "報酬還元メカニズム",
        "Opt5": "フェアユース",
        "Opt6": "パブリックドメイン",
        "Answer_Idx": 3,
        "Explanation": "生成AI時代における権利者保護と技術発展のバランスをとるための重要な論点です。",
        "Link": "https://example.com/ai-copyright-reward"
    },
    {
        "ID": "DLA-127",
        "Category": "5.手法（応用）",
        "Question": "複数の深層学習モデルを組み合わせ、1つのモデルが予測に失敗しても他が補うように設計されたシステムは？",
        "Opt1": "アンサンブルシステム",
        "Opt2": "冗長化システム",
        "Opt3": "フォールトトレラントAI",
        "Opt4": "マルチエージェント",
        "Opt5": "シングルポイント",
        "Opt6": "フェイルセーフ",
        "Answer_Idx": 2,
        "Explanation": "ミッションクリティカルな現場（自動運転や医療）で、AIの失敗による被害を防ぐ設計です。",
        "Link": "https://example.com/fault-tolerant-ai"
    },
    {
        "ID": "DLA-128",
        "Category": "5.手法（応用）",
        "Question": "学習済みモデルの重み情報を解析することで、学習データに含まれていた個人情報を復元しようとする攻撃は？",
        "Opt1": "モデル反転攻撃 (Model Inversion Attack)",
        "Opt2": "プロンプト注入",
        "Opt3": "なりすまし",
        "Opt4": "バックドア",
        "Opt5": "踏み台攻撃",
        "Opt6": "中間者攻撃",
        "Answer_Idx": 0,
        "Explanation": "重みデータ自体から秘密の情報が漏洩するリスクを指します。",
        "Link": "https://example.com/model-inversion"
    },
    {
        "ID": "DLA-129",
        "Category": "5.手法（応用）",
        "Question": "個人情報を保護したままAIを学習させるために、データに統計的なノイズを加える手法を何というか？",
        "Opt1": "バッチ正規化",
        "Opt2": "差分プライバシー (Differential Privacy)",
        "Opt3": "L2正則化",
        "Opt4": "ドロップアウト",
        "Opt5": "量子化",
        "Opt6": "蒸留",
        "Answer_Idx": 1,
        "Explanation": "特定の個人のデータが含まれているかどうかを判別不能にする数学的な仕組みです。",
        "Link": "https://example.com/diff-privacy"
    },
    {
        "ID": "DLA-130",
        "Category": "5.手法（応用）",
        "Question": "AIが生成した画像や文章に対し、それが「AI製であること」を証明するために埋め込まれる不可視の情報を何というか？",
        "Opt1": "透かし（ウォーターマーク）",
        "Opt2": "電子署名",
        "Opt3": "ハッシュ値",
        "Opt4": "メタデータ",
        "Opt5": "バーコード",
        "Opt6": "QRコード",
        "Answer_Idx": 0,
        "Explanation": "フェイクニュース対策や著作権保護のために、大手各社が導入を進めている技術です。",
        "Link": "https://example.com/ai-watermark"
    },
    {
        "ID": "DLA-131",
        "Category": "5.手法（応用）",
        "Question": "AIの出力結果が、社会の常識や人間の倫理観と一致するように調整（アライメント）するプロセスの中心的な手法は？",
        "Opt1": "RLHF",
        "Opt2": "RAG",
        "Opt3": "CNN",
        "Opt4": "RNN",
        "Opt5": "SVM",
        "Opt6": "PCA",
        "Answer_Idx": 0,
        "Explanation": "人間のフィードバックを元に報酬モデルを作り、AIを「行儀よく」育てます。",
        "Link": "https://example.com/alignment-ai"
    },
    {
        "ID": "DLA-132",
        "Category": "5.手法（応用）",
        "Question": "AIサービスの提供者が、ユーザーに対して「どのようなデータで学習したか」などの情報を開示する責任を何というか？",
        "Opt1": "説明責任 (Accountability)",
        "Opt2": "透明性 (Transparency)",
        "Opt3": "公平性",
        "Opt4": "機密性",
        "Opt5": "可用性",
        "Opt6": "完全性",
        "Answer_Idx": 1,
        "Explanation": "ブラックボックス問題を解消し、ユーザーが安心してAIを利用するために重要です。",
        "Link": "https://example.com/ai-transparency"
    },
    {
        "ID": "DLA-133",
        "Category": "5.手法（応用）",
        "Question": "画像生成AIを用いて、実在の人物が実際には言っていないことや、していない行動をリアルに捏造した動画は？",
        "Opt1": "ディープフェイク",
        "Opt2": "ディープラーニング",
        "Opt3": "ディープウェブ",
        "Opt4": "ダークウェブ",
        "Opt5": "ハイブリッドAI",
        "Opt6": "バーチャルヒューマン",
        "Answer_Idx": 0,
        "Explanation": "選挙妨害やプライバシー侵害などの悪用が世界的な懸念事項となっています。",
        "Link": "https://example.com/deepfake-risk"
    },
    {
        "ID": "DLA-134",
        "Category": "5.手法（応用）",
        "Question": "あるAIモデルが特定の課題で高い正解率を出しているが、実は背景の特定の色など「本質的でない特徴」を根拠にしていた問題を何というか？",
        "Opt1": "シュプレヒコール問題",
        "Opt2": "ハンス問題（賢いハンス現象）",
        "Opt3": "フレーム問題",
        "Opt4": "停止性問題",
        "Opt5": "P≠NP問題",
        "Opt6": "チューリング・テスト",
        "Answer_Idx": 1,
        "Explanation": "モデルが本質を理解せず、データ内の統計的なショートカットを学習してしまう問題です。",
        "Link": "https://example.com/clever-hans-ai"
    },
    {
        "ID": "DLA-135",
        "Category": "5.手法（応用）",
        "Question": "AIが自律的に学習目標を更新したり、人間が制御できないほど高度な知能を持つことで人類に危害を加えるリスクを何というか？",
        "Opt1": "AIのアライメント問題 / AIの存亡リスク",
        "Opt2": "フレーム問題",
        "Opt3": "シンギュラリティ",
        "Opt4": "ハルシネーション",
        "Opt5": "モード崩壊",
        "Opt6": "勾配消失",
        "Answer_Idx": 0,
        "Explanation": "将来的な超知能（AGI）の出現に備えた、AI安全性の最重要課題です。",
        "Link": "https://example.com/existential-risk"
    },
    {
        "ID": "DLA-136",
        "Category": "5.手法（応用）",
        "Question": "特定の企業や国がAIの計算資源（GPU等）やデータを独占し、経済的・政治的格差が広がることを何というか？",
        "Opt1": "AI格差（AIディバイド）",
        "Opt2": "デジタルデバイド",
        "Opt3": "知識格差",
        "Opt4": "情報格差",
        "Opt5": "技術独占",
        "Opt6": "経済格差",
        "Answer_Idx": 0,
        "Explanation": "AIを活用できる側とできない側の二極化を防ぐための議論が必要です。",
        "Link": "https://example.com/ai-divide"
    },
    {
        "ID": "DLA-137",
        "Category": "5.手法（応用）",
        "Question": "AIモデルのバージョン管理や、学習に使用したソースコード、データの系統（由来）を追跡可能にすることを何というか？",
        "Opt1": "データ・リネージ (Data Lineage)",
        "Opt2": "データ・マイニング",
        "Opt3": "データ・クリーニング",
        "Opt4": "データ・ウェアハウス",
        "Opt5": "データ・レイク",
        "Opt6": "データ・マート",
        "Answer_Idx": 0,
        "Explanation": "モデルがどのように作られたかを遡れるようにし、品質と信頼性を保証します。",
        "Link": "https://example.com/data-lineage-ai"
    },
    {
        "ID": "DLA-138",
        "Category": "5.手法（応用）",
        "Question": "「人間がAIの判断過程に常に関与し、最終決定権を持つ」という設計思想を何というか？",
        "Opt1": "Human-in-the-Loop (HITL)",
        "Opt2": "Full Automation",
        "Opt3": "Black Box",
        "Opt4": "AI-First",
        "Opt5": "Human-Centric",
        "Opt6": "Self-Driving",
        "Answer_Idx": 0,
        "Explanation": "AIを完全に放置せず、人間の責任と監督を組み込むアプローチです。",
        "Link": "https://example.com/hitl-concept"
    },
    {
        "ID": "DLA-139",
        "Category": "5.手法（応用）",
        "Question": "AIのアルゴリズム自体に欠陥がなくても、運用環境の変化（季節やトレンドの変化）で精度が低下することを何というか？",
        "Opt1": "コンセプト・ドリフト",
        "Opt2": "勾配爆発",
        "Opt3": "過学習",
        "Opt4": "アンダーフィッティング",
        "Opt5": "ハルシネーション",
        "Opt6": "モード崩壊",
        "Answer_Idx": 0,
        "Explanation": "学習時の前提条件が時間の経過とともに変わってしまうため、再学習が必要です。",
        "Link": "https://example.com/concept-drift"
    },
    {
        "ID": "DLA-140",
        "Category": "5.手法（応用）",
        "Question": "ディープラーニングを用いた最新の検索技術で、単語の文字列一致ではなく、意味の近さ（ベクトル）で検索する手法は？",
        "Opt1": "セマンティック検索",
        "Opt2": "キーワード検索",
        "Opt3": "正規表現検索",
        "Opt4": "ワイルドカード検索",
        "Opt5": "インデックス検索",
        "Opt6": "ランダム検索",
        "Answer_Idx": 0,
        "Explanation": "「意味」を理解した検索により、ユーザーの意図に沿った情報を的確に提示します。",
        "Link": "https://example.com/semantic-search"
    },
    {
        "ID": "TRD-001",
        "Category": "6.動向",
        "Question": "1956年に開催され、「人工知能（Artificial Intelligence）」という言葉が初めて使われた会議は？",
        "Opt1": "チューリング会議",
        "Opt2": "ダートマス会議",
        "Opt3": "スタンフォード会議",
        "Opt4": "MIT会議",
        "Opt5": "アニマール会議",
        "Opt6": "ロンドン会議",
        "Answer_Idx": 1,
        "Explanation": "ジョン・マッカーシーらが提唱し、AI研究が学問分野として確立されました。",
        "Link": "https://example.com/dartmouth-conference"
    },
    {
        "ID": "TRD-002",
        "Category": "6.動向",
        "Question": "第1次AIブーム（1950年代〜60年代）の主な研究対象であり、迷路やパズルを解く手法は？",
        "Opt1": "推論・探索",
        "Opt2": "知識表現",
        "Opt3": "ディープラーニング",
        "Opt4": "統計的学習",
        "Opt5": "エキスパートシステム",
        "Opt6": "強化学習",
        "Answer_Idx": 0,
        "Explanation": "トイ・プロブレム（おもちゃの問題）を解くことはできましたが、複雑な現実問題には対応できませんでした。",
        "Link": "https://example.com/1st-ai-boom"
    },
    {
        "ID": "TRD-003",
        "Category": "6.動向",
        "Question": "第2次AIブーム（1980年代）の主役であり、専門家の知識を「If-Then」形式でルール化したシステムは？",
        "Opt1": "ニューラルネットワーク",
        "Opt2": "エキスパートシステム",
        "Opt3": "パーセプトロン",
        "Opt4": "遺伝的アルゴリズム",
        "Opt5": "ベイズ推定",
        "Opt6": "サポートベクターマシン",
        "Answer_Idx": 1,
        "Explanation": "専門知識の入力に膨大なコストがかかる「知識獲得のボトルネック」に直面しました。",
        "Link": "https://example.com/expert-system-history"
    },
    {
        "ID": "TRD-004",
        "Category": "6.動向",
        "Question": "第3次AIブーム（2010年代〜）の火付け役となり、大量のデータから自動で特徴を学習する技術は？",
        "Opt1": "記号接地問題",
        "Opt2": "ビッグデータ",
        "Opt3": "機械学習・ディープラーニング",
        "Opt4": "論理プログラミング",
        "Opt5": "探索木",
        "Opt6": "フレーム問題",
        "Answer_Idx": 2,
        "Explanation": "コンピューティング能力の向上とビッグデータの普及により、飛躍的な進化を遂げました。",
        "Link": "https://example.com/3rd-ai-boom"
    },
    {
        "ID": "TRD-005",
        "Category": "6.動向",
        "Question": "1950年にアラン・チューリングが提唱した、機械に知能があるかどうかを判定するためのテストは？",
        "Opt1": "逆チューリングテスト",
        "Opt2": "チューリング・テスト",
        "Opt3": "カプチャ",
        "Opt4": "フレームテスト",
        "Opt5": "記号接地テスト",
        "Opt6": "ダートマス・テスト",
        "Answer_Idx": 1,
        "Explanation": "人間が対話を行い、相手が機械であると見抜けなければ、その機械には知能があるとするテストです。",
        "Link": "https://example.com/turing-test"
    },
    {
        "ID": "TRD-006",
        "Category": "6.動向",
        "Question": "人工知能において、記号（単語など）が現実世界の身体的経験や実体とどう結びつくかという問題は？",
        "Opt1": "フレーム問題",
        "Opt2": "記号接地問題（シンボルグラウンディング問題）",
        "Opt3": "モラベックのパラドックス",
        "Opt4": "シンギュラリティ",
        "Opt5": "チューリング問題",
        "Opt6": "計算量爆発",
        "Answer_Idx": 1,
        "Explanation": "スティーブン・ハルナッドによって提唱された、AIにおける根本的な課題の一つです。",
        "Link": "https://example.com/symbol-grounding"
    },
    {
        "ID": "TRD-007",
        "Category": "6.動向",
        "Question": "AIが有限の処理能力で、現実に起こりうる無限の出来事すべてに対処することは不可能であるという問題は？",
        "Opt1": "記号接地問題",
        "Opt2": "フレーム問題",
        "Opt3": "トイプロブレム",
        "Opt4": "ハルシネーション",
        "Opt5": "モード崩壊",
        "Opt6": "過学習",
        "Answer_Idx": 1,
        "Explanation": "ジョン・マッカーシーとパトリック・ヘイズが提唱。どこまでを考慮すべきかの境界が引けない問題です。",
        "Link": "https://example.com/frame-problem"
    },
    {
        "ID": "TRD-008",
        "Category": "6.動向",
        "Question": "「高度な推論よりも、1歳児レベルの知覚や運動スキルをAIに持たせる方がはるかに難しい」という逆説は？",
        "Opt1": "フレーム問題",
        "Opt2": "モラベックのパラドックス",
        "Opt3": "シンギュラリティ",
        "Opt4": "ムーアの法則",
        "Opt5": "アムダールの法則",
        "Opt6": "ジップの法則",
        "Answer_Idx": 1,
        "Explanation": "チェスや計算は得意でも、歩く、触るといった身体的動作はAIにとって困難であることを指します。",
        "Link": "https://example.com/moravec-paradox"
    },
    {
        "ID": "TRD-009",
        "Category": "6.動向",
        "Question": "2045年頃に到来すると予測される、AIが人類の知能を超える転換点を何というか？",
        "Opt1": "ビッグバン",
        "Opt2": "シンギュラリティ（技術的特異点）",
        "Opt3": "第四次産業革命",
        "Opt4": "AIホライズン",
        "Opt5": "デジタル変革",
        "Opt6": "ポストAI",
        "Answer_Idx": 1,
        "Explanation": "レイ・カーツワイルが提唱。指数関数的な技術の進歩により、予測不可能な変化が起きるとされています。",
        "Link": "https://example.com/singularity-kurzweil"
    },
    {
        "ID": "TRD-010",
        "Category": "6.動向",
        "Question": "2012年の画像認識コンペ（ILSVRC）で、ディープラーニングを用いて圧倒的な勝利を収めたチームのモデルは？",
        "Opt1": "ResNet",
        "Opt2": "AlexNet",
        "Opt3": "VGG",
        "Opt4": "LeNet",
        "Opt5": "Inception",
        "Opt6": "GoogleNet",
        "Answer_Idx": 1,
        "Explanation": "ジェフリー・ヒントン教授らのチームが開発し、第3次AIブームを決定づけました。",
        "Link": "https://example.com/alexnet-impact"
    },
    {
        "ID": "TRD-011",
        "Category": "6.動向",
        "Question": "1997年に、当時のチェス世界チャンピオンであるガルリ・カスパロフを破ったIBMのコンピュータは？",
        "Opt1": "Watson",
        "Opt2": "Deep Blue",
        "Opt3": "AlphaGo",
        "Opt4": "DeepMind",
        "Opt5": "AlphaZero",
        "Opt6": "Siri",
        "Answer_Idx": 1,
        "Explanation": "特定のルールに基づいた推論と探索において、機械が人間を超えた象徴的な出来事です。",
        "Link": "https://example.com/deep-blue-chess"
    },
    {
        "ID": "TRD-012",
        "Category": "6.動向",
        "Question": "2011年に米国のクイズ番組「ジェパディ！」で人間を破った、自然言語処理と知識検索を組み合わせたシステムは？",
        "Opt1": "Deep Blue",
        "Opt2": "Watson",
        "Opt3": "AlphaGo",
        "Opt4": "GPT",
        "Opt5": "ChatGPT",
        "Opt6": "Claude",
        "Answer_Idx": 1,
        "Explanation": "非構造化データから答えを見つけ出す質問応答システムの先駆けとなりました。",
        "Link": "https://example.com/ibm-watson-jeopardy"
    },
    {
        "ID": "TRD-013",
        "Category": "6.動向",
        "Question": "特定のタスク（翻訳、囲碁、画像診断など）に限定されず、人間と同様の幅広い知能を持つAIを何というか？",
        "Opt1": "特化型AI",
        "Opt2": "AGI（汎用人工知能）",
        "Opt3": "生成AI",
        "Opt4": "弱いAI",
        "Opt5": "ナローAI",
        "Opt6": "エッジAI",
        "Answer_Idx": 1,
        "Explanation": "人工知能研究の究極の目標の一つであり、多分野にわたる推論や自己学習が可能です。",
        "Link": "https://example.com/agi-definition"
    },
    {
        "ID": "TRD-014",
        "Category": "6.動向",
        "Question": "2017年にグーグルの研究者らによって発表された、現在の生成AIブームの核心となる論文のタイトルは？",
        "Opt1": "Deep Residual Learning for Image Recognition",
        "Opt2": "Attention Is All You Need",
        "Opt3": "Generative Adversarial Nets",
        "Opt4": "ImageNet Classification with Deep CNNs",
        "Opt5": "Language Models are Few-Shot Learners",
        "Opt6": "Computing Machinery and Intelligence",
        "Answer_Idx": 1,
        "Explanation": "Transformerアーキテクチャを提案し、自然言語処理に革命をもたらしました。",
        "Link": "https://example.com/attention-is-all-you-need"
    },
    {
        "ID": "TRD-015",
        "Category": "6.動向",
        "Question": "人工知能が自律的に攻撃対象を選択し、殺傷する兵器のことを略称で何というか？",
        "Opt1": "LAWS（自律型致死兵器システム）",
        "Opt2": "AGI",
        "Opt3": "ELIZA",
        "Opt4": "ASIMO",
        "Opt5": "DPA",
        "Opt6": "NPU",
        "Answer_Idx": 0,
        "Explanation": "国際的に倫理や規制の議論が活発に行われている分野です。",
        "Link": "https://example.com/laws-ai"
    },
    {
        "ID": "TRD-016",
        "Category": "6.動向",
        "Question": "日本におけるAIの国家戦略として、少子高齢化などの課題を解決する「人間中心の社会」を指す言葉は？",
        "Opt1": "Industry 4.0",
        "Opt2": "Society 5.0",
        "Opt3": "Smart City",
        "Opt4": "Digital Japan",
        "Opt5": "Economy 2.0",
        "Opt6": "SDGs",
        "Answer_Idx": 1,
        "Explanation": "サイバー空間とフィジカル空間を高度に融合させ、誰もが快適に暮らせる社会を目指しています。",
        "Link": "https://example.com/society-5-0"
    },
    {
        "ID": "TRD-017",
        "Category": "6.動向",
        "Question": "AI技術がホワイトカラーの仕事の大部分を自動化し、生産性を向上させる一方で、雇用に影響を与える懸念を何というか？",
        "Opt1": "デジタルデバイド",
        "Opt2": "技術的失業",
        "Opt3": "モード崩壊",
        "Opt4": "フレーム問題",
        "Opt5": "2025年の崖",
        "Opt6": "シンギュラリティ",
        "Answer_Idx": 1,
        "Explanation": "ジョン・メイナード・ケインズが提唱した概念で、現代のAIによる労働代替の議論で再注目されています。",
        "Link": "https://example.com/technological-unemployment"
    },
    {
        "ID": "TRD-018",
        "Category": "6.動向",
        "Question": "「AIガバナンス」において、AIの安全性を確保するために世界各国の政府や企業が合意した、2023年の宣言は？",
        "Opt1": "ブレッチリー宣言",
        "Opt2": "ダートマス宣言",
        "Opt3": "京都AI宣言",
        "Opt4": "シリコンバレー声明",
        "Opt5": "パリ協定",
        "Opt6": "ジュネーブ条約",
        "Answer_Idx": 0,
        "Explanation": "英国で開催されたAI安全サミットにて、28カ国とEUが署名しました。",
        "Link": "https://example.com/bletchley-declaration"
    },
    {
        "ID": "TRD-019",
        "Category": "6.動向",
        "Question": "1960年代に開発された、世界初のチャットボット（人工無脳）とされるプログラムは？",
        "Opt1": "Pearly",
        "Opt2": "ELIZA",
        "Opt3": "Siri",
        "Opt4": "Alexa",
        "Opt5": "Tay",
        "Opt6": "Cortana",
        "Answer_Idx": 1,
        "Explanation": "ジョセフ・ワイゼンバウムによって作られ、精神療法医のように振る舞うことができました。",
        "Link": "https://example.com/eliza-history"
    },
    {
        "ID": "TRD-020",
        "Category": "6.動向",
        "Question": "日本を拠点とする研究組織「全脳アーキテクチャ（WBAI）」が目指している、脳の構造を模したAIの構築方針は？",
        "Opt1": "特化型AI",
        "Opt2": "脳全体の工学的な再現（汎用AIへのアプローチ）",
        "Opt3": "量子AI",
        "Opt4": "記号的AI",
        "Opt5": "論理回路の高速化",
        "Opt6": "ニューロモルフィックチップの開発",
        "Answer_Idx": 1,
        "Explanation": "脳の異なる部位をモデル化したコンポーネントを結合させ、知能を実現しようとしています。",
        "Link": "https://example.com/wbai-approach"
    },
    {
        "ID": "TRD-021",
        "Category": "6.動向",
        "Question": "「ディープラーニングの父」とも呼ばれ、トロント大学教授としてBackpropagationの普及やAlexNetの開発に関わった人物は？",
        "Opt1": "ヤン・ルカン",
        "Opt2": "ヨシュア・ベンジオ",
        "Opt3": "ジェフリー・ヒントン",
        "Opt4": "アンドリュー・ン",
        "Opt5": "デミス・ハサビス",
        "Opt6": "サム・アルトマン",
        "Answer_Idx": 2,
        "Explanation": "2018年にチューリング賞を受賞。現代のAIブームの最重要人物の一人です。",
        "Link": "https://example.com/geoffrey-hinton"
    },
    {
        "ID": "TRD-022",
        "Category": "6.動向",
        "Question": "CNN（畳み込みニューラルネットワーク）の先駆けとなる「LeNet」を開発し、現在はMetaのチーフAIサイエンティストを務める人物は？",
        "Opt1": "ジェフリー・ヒントン",
        "Opt2": "ヤン・ルカン",
        "Opt3": "ヨシュア・ベンジオ",
        "Opt4": "レイ・カーツワイル",
        "Opt5": "ジョン・マッカーシー",
        "Opt6": "マービン・ミンスキー",
        "Answer_Idx": 1,
        "Explanation": "手書き文字認識の自動化に大きく貢献し、ディープラーニングの基礎を築きました。",
        "Link": "https://example.com/yann-lecun"
    },
    {
        "ID": "TRD-023",
        "Category": "6.動向",
        "Question": "2023年のG7広島サミットで閣議決定された、生成AIの国際的なルール作りを目指す枠組みを何というか？",
        "Opt1": "ブレッチリー宣言",
        "Opt2": "広島AIプロセス",
        "Opt3": "EU AI法",
        "Opt4": "デジタル万博",
        "Opt5": "京都議定書",
        "Opt6": "AI開発憲章",
        "Answer_Idx": 1,
        "Explanation": "日本が議長国として主導し、生成AIのリスク管理や透明性について合意されました。",
        "Link": "https://example.com/hiroshima-ai-process"
    },
    {
        "ID": "TRD-024",
        "Category": "6.動向",
        "Question": "AIが生成したコンテンツに対して、著作権法上の保護が認められるために必要とされる要素は？",
        "Opt1": "AIによる大量生成",
        "Opt2": "プロンプトの長さ",
        "Opt3": "「創作的寄与（創作的意図と創作的寄与）」",
        "Opt4": "生成にかかった時間",
        "Opt5": "GPUの計算量",
        "Opt6": "モデルのパラメータ数",
        "Answer_Idx": 2,
        "Explanation": "単にAIに生成させただけでは著作権は認められず、人間が表現を制御・選択している必要があります。",
        "Link": "https://example.com/ai-copyright-jp"
    },
    {
        "ID": "TRD-025",
        "Category": "6.動向",
        "Question": "グーグル傘下のDeepMindを設立し、AlphaGoの開発を指揮した、チェスの天才としても知られる人物は？",
        "Opt1": "サム・アルトマン",
        "Opt2": "デミス・ハサビス",
        "Opt3": "イーロン・マスク",
        "Opt4": "ビル・ゲイツ",
        "Opt5": "ジェンセン・ファン",
        "Opt6": "マーク・ザッカーバーグ",
        "Answer_Idx": 1,
        "Explanation": "汎用人工知能（AGI）の実現を目指し、強化学習と深層学習の融合を推進しました。",
        "Link": "https://example.com/demis-hassabis"
    },
    {
        "ID": "TRD-026",
        "Category": "6.動向",
        "Question": "AIの安全性を評価するため、日本政府が2024年に設置した専門機関の名称は？",
        "Opt1": "AI戦略会議",
        "Opt2": "AI防災センター",
        "Opt3": "日本AIセーフティ・インスティテュート (AISI)",
        "Opt4": "IPA AI部",
        "Opt5": "NEDO AI局",
        "Opt6": "デジタル庁AI課",
        "Answer_Idx": 2,
        "Explanation": "主要国（米英等）と連携し、AIの安全性に関する調査・検証を行う機関です。",
        "Link": "https://example.com/aisi-japan"
    },
    {
        "ID": "TRD-027",
        "Category": "6.動向",
        "Question": "EUのAI法（AI Act）において、個人のプロファイリングや社会的信用スコアなど、許容できないリスクを持つAIの扱いは？",
        "Opt1": "自由な利用",
        "Opt2": "一部制限",
        "Opt3": "原則禁止",
        "Opt4": "政府の許可制",
        "Opt5": "登録制",
        "Opt6": "課税対象",
        "Answer_Idx": 2,
        "Explanation": "人権を侵害する恐れのあるAI利用については、最も厳しい「禁止」のカテゴリーに分類されます。",
        "Link": "https://example.com/eu-ai-risk-levels"
    },
    {
        "ID": "TRD-028",
        "Category": "6.動向",
        "Question": "AIの学習に他者の著作物を利用することについて、日本の著作権法（第30条の4）では原則としてどう規定されているか？",
        "Opt1": "いかなる場合も禁止",
        "Opt2": "権利者の許可が必須",
        "Opt3": "営利目的なら禁止",
        "Opt4": "「享受」を目的としない限り、原則として可能",
        "Opt5": "1枚ごとに課金が必要",
        "Opt6": "完全に自由（制限なし）",
        "Answer_Idx": 3,
        "Explanation": "情報解析などの「非享受利用」であれば、権利者の利益を不当に害さない範囲で利用可能です。",
        "Link": "https://example.com/copyright-30-4"
    },
    {
        "ID": "TRD-029",
        "Category": "6.動向",
        "Question": "スタンフォード大学が毎年発表している、世界のAIの現状をデータに基づき多角的に分析したレポートは？",
        "Opt1": "World AI Report",
        "Opt2": "AI Index Report",
        "Opt3": "Global AI Map",
        "Opt4": "Future of AI",
        "Opt5": "State of AI",
        "Opt6": "Digital Economy Report",
        "Answer_Idx": 1,
        "Explanation": "投資額、特許数、技術トレンドなど、AI研究のベンチマークとなる報告書です。",
        "Link": "https://example.com/ai-index-report"
    },
    {
        "ID": "TRD-030",
        "Category": "6.動向",
        "Question": "AIの開発者や提供者が、モデルの限界や学習データの傾向を明文化して公開する「成分表示」のような文書は？",
        "Opt1": "データシート",
        "Opt2": "モデルカード (Model Cards)",
        "Opt3": "利用規約",
        "Opt4": "プライバシーポリシー",
        "Opt5": "ホワイトペーパー",
        "Opt6": "仕様書",
        "Answer_Idx": 1,
        "Explanation": "Googleのマーガレット・ミッチェルらが提唱した、AIの透明性を高めるための文書です。",
        "Link": "https://example.com/model-cards"
    },
    {
        "ID": "TRD-031",
        "Category": "6.動向",
        "Question": "大規模言語モデルの急速な発展に伴い、AIの安全性を確保するために開発を半年間停止するよう求めた2023年の公開書簡を出した団体は？",
        "Opt1": "IEEE",
        "Opt2": "FLI（Future of Life Institute）",
        "Opt3": "ACM",
        "Opt4": "W3C",
        "Opt5": "ITU",
        "Opt6": "UNESCO",
        "Answer_Idx": 1,
        "Explanation": "イーロン・マスクら著名人が署名し、AIのリスク管理に対する議論を世界的に再燃させました。",
        "Link": "https://example.com/fli-letter"
    },
    {
        "ID": "TRD-032",
        "Category": "6.動向",
        "Question": "日本の「AI事業者ガイドライン」において、AI提供者が重視すべき3つの主要概念は？",
        "Opt1": "スピード・安さ・手軽さ",
        "Opt2": "安全性・信頼性・透明性",
        "Opt3": "利益・シェア・独占",
        "Opt4": "匿名性・自由・無制限",
        "Opt5": "過去・現在・未来",
        "Opt6": "ハード・ソフト・データ",
        "Answer_Idx": 1,
        "Explanation": "人間中心のAI社会を実現するための基本原則として挙げられています。",
        "Link": "https://example.com/ai-operator-guidelines"
    },
    {
        "ID": "TRD-033",
        "Category": "6.動向",
        "Question": "「AIガバナンス」において、法令による強制的な規制ではなく、企業が自主的にルールを守る仕組みを何というか？",
        "Opt1": "ハードロー",
        "Opt2": "ソフトロー",
        "Opt3": "セルフ・レギュレーション",
        "Opt4": "デジュール標準",
        "Opt5": "デファクト標準",
        "Opt6": "慣習法",
        "Answer_Idx": 1,
        "Explanation": "変化の速いAI分野では、法制化（ハードロー）に先行してガイドライン等のソフトローが重視されます。",
        "Link": "https://example.com/soft-law-ai"
    },
    {
        "ID": "TRD-034",
        "Category": "6.動向",
        "Question": "米国のバイデン政権が2023年に署名した、安全で信頼できるAIの開発と利用に関する強力な命令は？",
        "Opt1": "AI権利章典",
        "Opt2": "AI大統領令 (Executive Order on AI)",
        "Opt3": "AI国家法",
        "Opt4": "シリコンバレー協定",
        "Opt5": "ワシントン宣言",
        "Opt6": "テロ対策法",
        "Answer_Idx": 1,
        "Explanation": "政府機関に対し、AIの安全性試験や報告を義務付ける包括的な命令です。",
        "Link": "https://example.com/us-ai-executive-order"
    },
    {
        "ID": "TRD-035",
        "Category": "6.動向",
        "Question": "機械学習のデータセットから、特定の個人のデータや著作権侵害にあたるデータを削除するプロセスを何というか？",
        "Opt1": "データ・キュレーション",
        "Opt2": "マシン・アンラーニング (Machine Unlearning)",
        "Opt3": "データ・マイニング",
        "Opt4": "データ・アノテーション",
        "Opt5": "データ・クリーニング",
        "Opt6": "データ・レイク",
        "Answer_Idx": 1,
        "Explanation": "一度学習したモデルから特定の記憶を「消去」する技術的な試みを指します。",
        "Link": "https://example.com/machine-unlearning-trend"
    },
    {
        "ID": "TRD-036",
        "Category": "6.動向",
        "Question": "1980年代の第5世代コンピュータ計画など、日本がかつて国家プロジェクトとして推進したAI研究の主なアプローチは？",
        "Opt1": "ニューラルネットワーク",
        "Opt2": "論理プログラミング・推論",
        "Opt3": "ビッグデータ解析",
        "Opt4": "強化学習",
        "Opt5": "遺伝的アルゴリズム",
        "Opt6": "パターン認識",
        "Answer_Idx": 1,
        "Explanation": "当時の主流だった「知識」と「論理」に基づくAIを目指しましたが、成果は限定的でした。",
        "Link": "https://example.com/5th-gen-computer"
    },
    {
        "ID": "TRD-037",
        "Category": "6.動向",
        "Question": "画像生成AIにおいて、アーティストが自分の作品を勝手に学習されないよう、画像に目に見えない毒を混ぜる技術は？",
        "Opt1": "ステガノグラフィー",
        "Opt2": "Nightshade（ナイトシェイド）/ Glaze",
        "Opt3": "電子署名",
        "Opt4": "ブロックチェーン",
        "Opt5": "量子暗号",
        "Opt6": "VPN",
        "Answer_Idx": 1,
        "Explanation": "学習したAIのモデルを「汚染（ポイズニング）」し、正しい生成を妨げる対抗技術です。",
        "Link": "https://example.com/nightshade-ai"
    },
    {
        "ID": "TRD-038",
        "Category": "6.動向",
        "Question": "AIによって自動生成された大量のフェイクニュースや画像が、インターネット上の情報の信頼性を損なう懸念を何というか？",
        "Opt1": "情報爆発",
        "Opt2": "デッドインターネット理論（の一部）",
        "Opt3": "2025年の崖",
        "Opt4": "デジタル・デバイド",
        "Opt5": "情報の非対称性",
        "Opt6": "フィルターバブル",
        "Answer_Idx": 1,
        "Explanation": "ネット上のコンテンツの大部分がAI製になり、人間同士の交流が困難になる仮説に関連して議論されます。",
        "Link": "https://example.com/dead-internet-theory"
    },
    {
        "ID": "TRD-039",
        "Category": "6.動向",
        "Question": "AI技術を用いて、人間の脳の活動（脳波や血流）を読み取り、考えている内容を画像や文章として再構成する技術は？",
        "Opt1": "ブレイン・コンピュータ・インターフェース (BCI) / デコード",
        "Opt2": "マインドフルネス",
        "Opt3": "テレパシー",
        "Opt4": "ホログラム",
        "Opt5": "バイオメトリクス",
        "Opt6": "ニューロフィードバック",
        "Answer_Idx": 0,
        "Explanation": "医療や福祉への応用が期待される一方、プライバシー（精神的自由）の観点から議論があります。",
        "Link": "https://example.com/bci-ai"
    },
    {
        "ID": "TRD-040",
        "Category": "6.動向",
        "Question": "生成AIのモデルにおいて、学習データに偏りがあるために、特定の職業＝特定の性別といった固定観念を出力してしまう問題を何というか？",
        "Opt1": "モード崩壊",
        "Opt2": "ハルシネーション",
        "Opt3": "社会的バイアス (Social Bias)",
        "Opt4": "勾配消失",
        "Opt5": "過学習",
        "Opt6": "アンダーフィッティング",
        "Answer_Idx": 2,
        "Explanation": "現実世界の偏見をAIが強化・再生産してしまうリスクとして、深刻に捉えられています。",
        "Link": "https://example.com/ai-social-bias"
    },
    {
        "ID": "TRD-041",
        "Category": "6.動向",
        "Question": "元OpenAIのメンバーによって設立され、憲法AI（Constitutive AI）という安全性を重視した手法で「Claude」を開発する企業は？",
        "Opt1": "Google",
        "Opt2": "Meta",
        "Opt3": "Anthropic",
        "Opt4": "Microsoft",
        "Opt5": "NVIDIA",
        "Opt6": "Mistral AI",
        "Answer_Idx": 2,
        "Explanation": "「AIの安全性」を最優先事項として掲げ、人間とのアライメントを重視したモデルを開発しています。",
        "Link": "https://example.com/anthropic-profile"
    },
    {
        "ID": "TRD-042",
        "Category": "6.動向",
        "Question": "「Llama」シリーズをオープンソースに近い形で公開し、世界のAI開発コミュニティの活性化を主導している企業は？",
        "Opt1": "Google",
        "Opt2": "Apple",
        "Opt3": "Meta",
        "Opt4": "Microsoft",
        "Opt5": "Amazon",
        "Opt6": "Oracle",
        "Answer_Idx": 2,
        "Explanation": "マーク・ザッカーバーグの下、オープンなモデル提供によってエコシステムを構築しています。",
        "Link": "https://example.com/meta-ai-llama"
    },
    {
        "ID": "TRD-043",
        "Category": "6.動向",
        "Question": "LLMの急激な普及に伴い、計算資源である「GPU」の需要が爆発し、時価総額が世界トップクラスとなった企業は？",
        "Opt1": "Intel",
        "Opt2": "AMD",
        "Opt3": "NVIDIA",
        "Opt4": "Arm",
        "Opt5": "Qualcomm",
        "Opt6": "TSMC",
        "Answer_Idx": 2,
        "Explanation": "H100やBlackwellといったAI特化型チップで市場を独占しています。",
        "Link": "https://example.com/nvidia-ai-boom"
    },
    {
        "ID": "TRD-044",
        "Category": "6.動向",
        "Question": "AIの学習と推論にかかる膨大な電力需要を賄うため、テック企業が次世代のクリーンエネルギーとして投資を加速させている分野は？",
        "Opt1": "風力発電",
        "Opt2": "太陽光発電",
        "Opt3": "原子力（小型モジュール炉: SMR等）",
        "Opt4": "バイオマス",
        "Opt5": "地熱発電",
        "Opt6": "石炭火力",
        "Answer_Idx": 2,
        "Explanation": "Amazon",
        "Link": "Google"
    },
    {
        "ID": "TRD-045",
        "Category": "6.動向",
        "Question": "2024年に、ディープラーニングの基礎となる「ホップフィールド・ネットワーク」や「制限ボルツマンマシン」を開発した功績でノーベル物理学賞を受賞した人物は？",
        "Opt1": "ヒントンとホップフィールド",
        "Opt2": "ルカンとベンジオ",
        "Opt3": "ハサビスとジャンパー",
        "Opt4": "アルトマンとブロックマン",
        "Opt5": "ホーキングとペンローズ",
        "Opt6": "チューリングとフォン・ノイマン",
        "Answer_Idx": 0,
        "Explanation": "ジョン・ホップフィールドとジェフリー・ヒントンが、物理学の手法を機械学習に応用した功績で受賞しました。",
        "Link": "https://example.com/nobel-physics-2024"
    },
    {
        "ID": "TRD-046",
        "Category": "6.動向",
        "Question": "AIによるタンパク質の構造予測「AlphaFold」の功績で、2024年にノーベル化学賞を受賞したDeepMindの研究者は？",
        "Opt1": "デミス・ハサビスとジョン・ジャンパー",
        "Opt2": "ジェフリー・ヒントン",
        "Opt3": "ヤン・ルカン",
        "Opt4": "サム・アルトマン",
        "Opt5": "アンドリュー・ン",
        "Opt6": "フェイフェイ・リ",
        "Answer_Idx": 0,
        "Explanation": "AIが科学的発見（AI for Science）の重要な道具であることを世界に示しました。",
        "Link": "https://example.com/nobel-chemistry-2024"
    },
    {
        "ID": "TRD-047",
        "Category": "6.動向",
        "Question": "日本国内において、日本語に特化したLLMの開発を推進している「LLM-jp」プロジェクトを中心となっている組織は？",
        "Opt1": "デジタル庁",
        "Opt2": "国立情報学研究所 (NII)",
        "Opt3": "産総研",
        "Opt4": "理化学研究所",
        "Opt5": "ソフトバンク",
        "Opt6": "NTT",
        "Answer_Idx": 1,
        "Explanation": "国内の研究者が協力し、透明性の高い日本語LLMの構築と公開を進めています。",
        "Link": "https://example.com/llm-jp-nii"
    },
    {
        "ID": "TRD-048",
        "Category": "6.動向",
        "Question": "総務省・経産省が公表した「AI事業者ガイドライン」で、AIを開発・提供・利用するすべての主体に共通して求められる「人間中心の原則」は？",
        "Opt1": "AIの意思決定への盲従",
        "Opt2": "人間の尊厳と自律性の尊重",
        "Opt3": "利益の最大化",
        "Opt4": "完全自動化の推進",
        "Opt5": "責任の免除",
        "Opt6": "データ収集の自由",
        "Answer_Idx": 1,
        "Explanation": "AIはあくまで人間の道具であり、人間が最終的な判断を行うべきという考え方です。",
        "Link": "https://example.com/human-centric-ai"
    },
    {
        "ID": "TRD-049",
        "Category": "6.動向",
        "Question": "AIが生成した情報によって個人の判断が操作され、民主主義のプロセス（選挙など）が脅かされるリスクを何というか？",
        "Opt1": "デジタル・ゲルマン問題",
        "Opt2": "認知操作（コグニティブ・マニピュレーション）",
        "Opt3": "ハルシネーション",
        "Opt4": "モード崩壊",
        "Opt5": "過学習",
        "Opt6": "アンダーフィッティング",
        "Answer_Idx": 1,
        "Explanation": "SNSを通じた情報の拡散と生成AIの組み合わせが、世論形成に悪影響を与えるリスクです。",
        "Link": "https://example.com/cognitive-manipulation"
    },
    {
        "ID": "TRD-050",
        "Category": "6.動向",
        "Question": "特定のAI開発者が市場を独占せず、多くの企業や個人がAIの恩恵を受けられるようにすることを目指す考え方は？",
        "Opt1": "AIの民主化",
        "Opt2": "AIの国有化",
        "Opt3": "AIの私物化",
        "Opt4": "AIの廃止",
        "Opt5": "AIの標準化",
        "Opt6": "AIの匿名化",
        "Answer_Idx": 0,
        "Explanation": "クラウド経由のAPI提供やオープンソース化により、高度なAIが広く利用可能になることを指します。",
        "Link": "https://example.com/democratization-ai"
    },
    {
        "ID": "TRD-051",
        "Category": "6.動向",
        "Question": "AIの学習データとして、人間が書いた文章ではなく「AIが生成したデータ」を使い続けると精度が崩壊する現象を何というか？",
        "Opt1": "モデル崩壊 (Model Collapse)",
        "Opt2": "オーバーフィッティング",
        "Opt3": "勾配消失",
        "Opt4": "ハルシネーション",
        "Opt5": "モード崩壊",
        "Opt6": "コンセプトドリフト",
        "Answer_Idx": 0,
        "Explanation": "AI生成物がネット上に溢れることで、将来のモデルの学習品質が下がる懸念です。",
        "Link": "https://example.com/model-collapse"
    },
    {
        "ID": "TRD-052",
        "Category": "6.動向",
        "Question": "AIが自律的に「自分よりも賢いAI」を設計し続け、知能が爆発的に進化するという概念を何というか？",
        "Opt1": "知能爆発 (Intelligence Explosion)",
        "Opt2": "フレーム問題",
        "Opt3": "記号接地問題",
        "Opt4": "ムーアの法則",
        "Opt5": "アムダールの法則",
        "Opt6": "ジップの法則",
        "Answer_Idx": 0,
        "Explanation": "I.J.グッドが提唱。シンギュラリティ（特異点）に至るプロセスの一つとされます。",
        "Link": "https://example.com/intelligence-explosion"
    },
    {
        "ID": "TRD-053",
        "Category": "6.動向",
        "Question": "AIが生成した偽の音声や映像を用いて、電話やビデオ会議で知人になりすまし、金銭を奪う詐欺を何というか？",
        "Opt1": "フィッシング詐欺",
        "Opt2": "ディープフェイク詐欺（AI詐欺）",
        "Opt3": "ランサムウェア",
        "Opt4": "DDos攻撃",
        "Opt5": "SQLインジェクション",
        "Opt6": "クロスサイトスクリプティング",
        "Answer_Idx": 1,
        "Explanation": "声紋や顔の特徴をAIで再現する高度な詐欺手法が増加しています。",
        "Link": "https://example.com/ai-scam-deepfake"
    },
    {
        "ID": "TRD-054",
        "Category": "6.動向",
        "Question": "LLM開発における計算リソースの確保のため、日本政府が国内企業（さくらインターネット、ソフトバンク等）に対して行っている支援は？",
        "Opt1": "GPUクラウド基盤の整備支援",
        "Opt2": "AIの利用禁止",
        "Opt3": "税金の増税",
        "Opt4": "データの破棄命令",
        "Opt5": "社名の変更勧告",
        "Opt6": "全社員の公務員化",
        "Answer_Idx": 0,
        "Explanation": "経済安全保障の観点から、国内に強力な計算基盤を持つことを支援しています。",
        "Link": "https://example.com/gpu-support-jp"
    },
    {
        "ID": "TRD-055",
        "Category": "6.動向",
        "Question": "1943年に「形式ニューロン」という、現在のニューラルネットワークの数学的基礎となるモデルを提案した人物は？",
        "Opt1": "マカロックとピッツ",
        "Opt2": "ワトソンとクリック",
        "Opt3": "ジョブズとウォズニアック",
        "Opt4": "ゲートとアレン",
        "Opt5": "ヒントンとルカン",
        "Opt6": "マッカーシーとミンスキー",
        "Answer_Idx": 0,
        "Explanation": "ウォーレン・マカロックとウォルター・ピッツ。神経細胞の働きを論理ゲートとして表現しました。",
        "Link": "https://example.com/mcculloch-pitts-neuron"
    },
    {
        "ID": "TRD-056",
        "Category": "6.動向",
        "Question": "AI関連のニュースで頻出する「SoTA (State-of-the-Art)」という言葉の意味は？",
        "Opt1": "過去の遺物",
        "Opt2": "現在の最高水準（最先端）",
        "Opt3": "理論的な限界",
        "Opt4": "初心者のための指針",
        "Opt5": "国家標準規格",
        "Opt6": "秘密のアルゴリズム",
        "Answer_Idx": 1,
        "Explanation": "特定のタスクにおいて、現時点で最も優れた性能を出すモデルを指します。",
        "Link": "https://example.com/sota-definition"
    },
    {
        "ID": "TRD-057",
        "Category": "6.動向",
        "Question": "複数の確率変数の同時分布を、各変数の条件付き分布からのサンプリングを繰り返すことで推定する手法は？",
        "Opt1": "ギブスサンプリング (MCMCの一種)",
        "Opt2": "勾配降下法",
        "Opt3": "主成分分析",
        "Opt4": "クラスタリング",
        "Opt5": "バックプロパゲーション",
        "Opt6": "正則化",
        "Answer_Idx": 0,
        "Explanation": "ベイズ統計や生成モデルの推論で使われる、数理統計的な重要手法です。",
        "Link": "https://example.com/gibbs-sampling"
    },
    {
        "ID": "TRD-058",
        "Category": "6.動向",
        "Question": "データの平均が0、分散が1になるように変換する処理を「標準化」というが、最小値を0、最大値を1にする処理を何というか？",
        "Opt1": "正則化",
        "Opt2": "正規化 (Normalization / Min-Max Scaling)",
        "Opt3": "量子化",
        "Opt4": "剪定",
        "Opt5": "蒸留",
        "Opt6": "平滑化",
        "Answer_Idx": 1,
        "Explanation": "データのスケールを一定範囲に収める、機械学習の必須の前処理です。",
        "Link": "https://example.com/normalization-scaling"
    },
    {
        "ID": "TRD-059",
        "Category": "6.動向",
        "Question": "日本のAI開発スタートアップとして、大規模言語モデル「PLaMo」を開発し、国内最大級の計算資源を持つ企業は？",
        "Opt1": "Preferred Networks (PFN)",
        "Opt2": "メルカリ",
        "Opt3": "LINE",
        "Opt4": "楽天",
        "Opt5": "ソニー",
        "Opt6": "トヨタ",
        "Answer_Idx": 0,
        "Explanation": "日本を代表するAI企業で、ハードウェアからソフトウェアまで一貫して開発しています。",
        "Link": "https://example.com/pfn-plamo"
    },
    {
        "ID": "TRD-060",
        "Category": "6.動向",
        "Question": "G検定の目的でもある、AIを正しく理解し、適切に活用するための知識や能力を総称して何というか？",
        "Opt1": "AIリテラシー",
        "Opt2": "デジタルデバイド",
        "Opt3": "プログラミング能力",
        "Opt4": "数学的能力",
        "Opt5": "AI独占",
        "Opt6": "データサイエンス",
        "Answer_Idx": 0,
        "Explanation": "社会全体がAIと共存するために、全てのビジネスパーソンに求められる素養です。",
        "Link": "https://example.com/ai-literacy"
    },
    {
        "ID": "MS-001",
        "Category": "7.数理・統計",
        "Question": "線形代数において、行列の行と列を入れ替えた行列を何というか？",
        "Opt1": "逆行列",
        "Opt2": "転置行列",
        "Opt3": "単位行列",
        "Opt4": "対角行列",
        "Opt5": "零行列",
        "Opt6": "正方行列",
        "Answer_Idx": 1,
        "Explanation": "機械学習の数式（例：重みベクトルと入力ベクトルの積）で頻繁に登場します。",
        "Link": "https://example.com/math-transpose"
    },
    {
        "ID": "MS-002",
        "Category": "7.数理・統計",
        "Question": "ある正方行列 $A$ に対して、$AX = XA = I$（単位行列）となる行列 $X$ を何というか？",
        "Opt1": "転置行列",
        "Opt2": "逆行列",
        "Opt3": "対角行列",
        "Opt4": "随伴行列",
        "Opt5": "行列式",
        "Opt6": "ランク",
        "Answer_Idx": 1,
        "Explanation": "方程式を解く際や、正規方程式による回帰分析などで重要な役割を果たします。",
        "Link": "https://example.com/math-inverse"
    },
    {
        "ID": "MS-003",
        "Category": "7.数理・統計",
        "Question": "ベクトルの「大きさ」を定義する指標であり、L1ノルム（絶対値の和）やL2ノルム（二乗和の平方根）などの総称は？",
        "Opt1": "スカラー",
        "Opt2": "テンソル",
        "Opt3": "ノルム",
        "Opt4": "次元",
        "Opt5": "ランク",
        "Opt6": "トレース",
        "Answer_Idx": 2,
        "Explanation": "過学習を防ぐ「正則化」において、重みの大きさを制限するために使われます。",
        "Link": "https://example.com/math-norm"
    },
    {
        "ID": "MS-004",
        "Category": "7.数理・統計",
        "Question": "2つのベクトルのなす角の近さを表し、自然言語処理の単語の類似度計算などで多用される指標は？",
        "Opt1": "ユーグリッド距離",
        "Opt2": "コサイン類似度",
        "Opt3": "マンハッタン距離",
        "Opt4": "ハミング距離",
        "Opt5": "マハラノビス距離",
        "Opt6": "相関係数",
        "Answer_Idx": 1,
        "Explanation": "ベクトルの「向き」の近さを-1から1の範囲で評価します。",
        "Link": "https://example.com/math-cosine-similarity"
    },
    {
        "ID": "MS-005",
        "Category": "7.数理・統計",
        "Question": "ある行列 $A$ とベクトル $x$ に対して、$Ax = \\lambda x$ が成り立つときの定数 $\\lambda$ を何というか？",
        "Opt1": "固有値",
        "Opt2": "固有ベクトル",
        "Opt3": "主成分",
        "Opt4": "寄与率",
        "Opt5": "偏微分",
        "Opt6": "勾配",
        "Answer_Idx": 0,
        "Explanation": "主成分分析（PCA）において、データのばらつきが最大となる方向を求める際に使われます。",
        "Link": "https://example.com/math-eigenvalue"
    },
    {
        "ID": "MS-006",
        "Category": "7.数理・統計",
        "Question": "関数 $y = f(x)$ において、入力 $x$ が微小に変化したときの $y$ の変化率を求める操作は？",
        "Opt1": "積分",
        "Opt2": "微分",
        "Opt3": "シグマ計算",
        "Opt4": "行列積",
        "Opt5": "畳み込み",
        "Opt6": "プーリング",
        "Answer_Idx": 1,
        "Explanation": "ニューラルネットワークの学習（重み更新）において、誤差を最小化する方向を探るために必須です。",
        "Link": "https://example.com/math-derivative"
    },
    {
        "ID": "MS-007",
        "Category": "7.数理・統計",
        "Question": "複数の変数を持つ関数において、特定の1つの変数のみに着目して微分することを何というか？",
        "Opt1": "全微分",
        "Opt2": "偏微分",
        "Opt3": "二階微分",
        "Opt4": "積分",
        "Opt5": "合成関数の微分",
        "Opt6": "連鎖律",
        "Answer_Idx": 1,
        "Explanation": "ディープラーニングでは、各重みパラメータごとに偏微分を行って勾配を求めます。",
        "Link": "https://example.com/math-partial-derivative"
    },
    {
        "ID": "MS-008",
        "Category": "7.数理・統計",
        "Question": "勾配降下法において、現在の重みから「勾配の何倍」の値を引くかを決定するハイパーパラメータは？",
        "Opt1": "バッチサイズ",
        "Opt2": "エポック数",
        "Opt3": "学習率（学習係数）",
        "Opt4": "モーメンタム",
        "Opt5": "ドロップアウト率",
        "Opt6": "隠れ層の数",
        "Answer_Idx": 2,
        "Explanation": "学習率が大きすぎると発散し、小さすぎると収束に時間がかかります。",
        "Link": "https://example.com/math-learning-rate"
    },
    {
        "ID": "MS-009",
        "Category": "7.数理・統計",
        "Question": "ニューラルネットワークの多層構造において、各層の微分を効率よく計算するために用いられる数学的ルールは？",
        "Opt1": "ベイズの定理",
        "Opt2": "連鎖律（チェインルール）",
        "Opt3": "中心極限定理",
        "Opt4": "大数の法則",
        "Opt5": "ピタゴラスの定理",
        "Opt6": "余弦定理",
        "Answer_Idx": 1,
        "Explanation": "このルールにより、出力層の誤差を逆方向に伝播させて各層の重みを更新できます。",
        "Link": "https://example.com/math-chain-rule"
    },
    {
        "ID": "MS-010",
        "Category": "7.数理・統計",
        "Question": "確率変数がとる値とその起こりやすさ（確率）を対応させたものを何というか？",
        "Opt1": "期待値",
        "Opt2": "分散",
        "Opt3": "確率分布",
        "Opt4": "標準偏差",
        "Opt5": "中央値",
        "Opt6": "最頻値",
        "Answer_Idx": 2,
        "Explanation": "正規分布やベルヌーイ分布など、データの特徴を数学的にモデル化する基礎です。",
        "Link": "https://example.com/math-dist"
    },
    {
        "ID": "MS-011",
        "Category": "7.数理・統計",
        "Question": "平均 $0$、分散 $1$ の左右対称な釣鐘型の確率分布を何というか？",
        "Opt1": "二項分布",
        "Opt2": "ポアソン分布",
        "Opt3": "標準正規分布",
        "Opt4": "一様分布",
        "Opt5": "指数分布",
        "Opt6": "カイ二乗分布",
        "Answer_Idx": 2,
        "Explanation": "多くの自然現象やデータの誤差がこの分布に従うと仮定される、最も重要な分布です。",
        "Link": "https://example.com/math-normal-dist"
    },
    {
        "ID": "MS-012",
        "Category": "7.数理・統計",
        "Question": "データのばらつき（平均からの離れ具合の二乗和の平均）を表す指標は？",
        "Opt1": "平均値",
        "Opt2": "中央値",
        "Opt3": "分散",
        "Opt4": "四分位範囲",
        "Opt5": "最頻値",
        "Opt6": "合計",
        "Answer_Idx": 2,
        "Explanation": "分散の平方根をとったものが「標準偏差」です。",
        "Link": "https://example.com/math-variance"
    },
    {
        "ID": "MS-013",
        "Category": "7.数理・統計",
        "Question": "2つの変数の関係の強さを $-1$ から $1$ の範囲で表す指標は？",
        "Opt1": "分散",
        "Opt2": "標準偏差",
        "Opt3": "ピアソンの相関係数",
        "Opt4": "寄与率",
        "Opt5": "決定係数",
        "Opt6": "p値",
        "Answer_Idx": 2,
        "Explanation": "1に近いと正の相関、-1に近いと負の相関、0に近いと相関がないことを示します。",
        "Link": "https://example.com/math-correlation"
    },
    {
        "ID": "MS-014",
        "Category": "7.数理・統計",
        "Question": "「ある事象が起きた」という条件のもとで、別の事象が起きる確率を何というか？",
        "Opt1": "同時確率",
        "Opt2": "条件付き確率",
        "Opt3": "周辺確率",
        "Opt4": "事後確率",
        "Opt5": "事前確率",
        "Opt6": "尤度",
        "Answer_Idx": 1,
        "Explanation": "スパムメール判定（ナイーブベイズ分類器）などの理論的根拠となります。",
        "Link": "https://example.com/math-conditional-prob"
    },
    {
        "ID": "MS-015",
        "Category": "7.数理・統計",
        "Question": "新しいデータ（証拠）が得られたときに、ある仮説が正しい確率（事後確率）を更新する公式は？",
        "Opt1": "中心極限定理",
        "Opt2": "ベイズの定理",
        "Opt3": "チェビシェフの不等式",
        "Opt4": "ガウスの法則",
        "Opt5": "大数の法則",
        "Opt6": "三平方の定理",
        "Answer_Idx": 1,
        "Explanation": "機械学習におけるベイズ推論の核となる非常に重要な定理です。",
        "Link": "https://example.com/math-bayes-theorem"
    },
    {
        "ID": "MS-016",
        "Category": "7.数理・統計",
        "Question": "手元のデータ（標本）から、背後にある真のデータ（母集団）の性質を推測する統計学の分野は？",
        "Opt1": "記述統計学",
        "Opt2": "推測統計学",
        "Opt3": "ベイズ統計学",
        "Opt4": "多変量解析",
        "Opt5": "数理計画法",
        "Opt6": "幾何学",
        "Answer_Idx": 1,
        "Explanation": "限られたデータからモデルを構築する機械学習は、推測統計の考え方に近いです。",
        "Link": "https://example.com/math-inferential-stats"
    },
    {
        "ID": "MS-017",
        "Category": "7.数理・統計",
        "Question": "統計的仮定検定において、「差がない」とする仮説を何というか？",
        "Opt1": "対立仮説",
        "Opt2": "帰無仮説",
        "Opt3": "第一種の過誤",
        "Opt4": "第二種の過誤",
        "Opt5": "有意水準",
        "Opt6": "棄却域",
        "Answer_Idx": 1,
        "Explanation": "検定では、この帰無仮説が否定（棄却）されることで、有意な差があることを証明します。",
        "Link": "https://example.com/math-null-hypothesis"
    },
    {
        "ID": "MS-018",
        "Category": "7.数理・統計",
        "Question": "統計的検定の結果、帰無仮説が正しいとした場合に、得られたデータ以上の極端な値が得られる確率を何というか？",
        "Opt1": "t値",
        "Opt2": "F値",
        "Opt3": "p値",
        "Opt4": "信頼区間",
        "Opt5": "有意水準",
        "Opt6": "検出力",
        "Answer_Idx": 2,
        "Explanation": "一般に p < 0.05（5%）であれば、「有意である」と判断されます。",
        "Link": "https://example.com/math-p-value"
    },
    {
        "ID": "MS-019",
        "Category": "7.数理・統計",
        "Question": "母集団からランダムに標本を取り出すとき、標本サイズが大きくなるほど標本平均が母平均に近づく法則は？",
        "Opt1": "中心極限定理",
        "Opt2": "大数の法則",
        "Opt3": "べき乗則",
        "Opt4": "ムーアの法則",
        "Opt5": "パレートの法則",
        "Opt6": "指数関数的増大",
        "Answer_Idx": 1,
        "Explanation": "データ量が増えるほど、統計的な推定の信頼性が高まる根拠となります。",
        "Link": "https://example.com/math-law-of-large-numbers"
    },
    {
        "ID": "MS-020",
        "Category": "7.数理・統計",
        "Question": "母集団がどのような分布であっても、標本サイズが十分に大きければ、標本平均の分布は正規分布に従うという定理は？",
        "Opt1": "大数の法則",
        "Opt2": "中心極限定理",
        "Opt3": "ベイズの定理",
        "Opt4": "テイラー展開",
        "Opt5": "ラグランジュの未定乗数法",
        "Opt6": "最小二乗法",
        "Answer_Idx": 1,
        "Explanation": "多くの統計手法が「データが正規分布に従う」と仮定できる理論的支柱です。",
        "Link": "https://example.com/math-clt"
    },
    {
        "ID": "MS-021",
        "Category": "7.数理・統計",
        "Question": "コイントスのように、結果が「成功」か「失敗」の2通りしかない試行を1回行った時の確率分布は？",
        "Opt1": "ポアソン分布",
        "Opt2": "ベルヌーイ分布",
        "Opt3": "二項分布",
        "Opt4": "正規分布",
        "Opt5": "一様分布",
        "Opt6": "指数分布",
        "Answer_Idx": 1,
        "Explanation": "ロジスティック回帰や2値分類の基礎となる、最もシンプルな離散確率分布です。",
        "Link": "https://example.com/bernoulli-dist"
    },
    {
        "ID": "MS-022",
        "Category": "7.数理・統計",
        "Question": "ベルヌーイ試行を独立に $n$ 回繰り返したとき、成功した回数 $k$ が従う確率分布を何というか？",
        "Opt1": "ベルヌーイ分布",
        "Opt2": "二項分布",
        "Opt3": "ポアソン分布",
        "Opt4": "幾何分布",
        "Opt5": "正規分布",
        "Opt6": "カイ二乗分布",
        "Answer_Idx": 1,
        "Explanation": "「10回コインを投げて表が3回出る確率」などを求める際に使われます。",
        "Link": "https://example.com/binomial-dist"
    },
    {
        "ID": "MS-023",
        "Category": "7.数理・統計",
        "Question": "一定の時間や範囲内で、めったに起こらない稀な事象が発生する回数を表す確率分布は？",
        "Opt1": "二項分布",
        "Opt2": "ポアソン分布",
        "Opt3": "指数分布",
        "Opt4": "正規分布",
        "Opt5": "t分布",
        "Opt6": "F分布",
        "Answer_Idx": 1,
        "Explanation": "コールセンターへの着信数や、システム障害の発生回数のモデル化に使われます。",
        "Link": "https://example.com/poisson-dist"
    },
    {
        "ID": "MS-024",
        "Category": "7.数理・統計",
        "Question": "情報理論において、ある事象が起きた時の「驚き」の度合いを数値化したものを何というか？",
        "Opt1": "期待値",
        "Opt2": "自己情報量",
        "Opt3": "エントロピー",
        "Opt4": "尤度",
        "Opt5": "分散",
        "Opt6": "偏り",
        "Answer_Idx": 1,
        "Explanation": "確率が低い事象ほど、その情報量（驚き）は大きくなります。",
        "Link": "https://example.com/information-content"
    },
    {
        "ID": "MS-025",
        "Category": "7.数理・統計",
        "Question": "確率変数における平均的な「情報の不確実性」や「乱雑さ」を表す指標は？",
        "Opt1": "平均偏差",
        "Opt2": "分散",
        "Opt3": "平均情報量（エントロピー）",
        "Opt4": "相関係数",
        "Opt5": "共分散",
        "Opt6": "尖度",
        "Answer_Idx": 2,
        "Explanation": "決定木の分割基準や、分類問題の損失関数（交差エントロピー）の基礎概念です。",
        "Link": "https://example.com/entropy-basics"
    },
    {
        "ID": "MS-026",
        "Category": "7.数理・統計",
        "Question": "2つの確率分布 $P$ と $Q$ の間の「違い」や「距離」を測定するために用いられる指標は？",
        "Opt1": "コサイン類似度",
        "Opt2": "KLダイバージェンス（カルバック・ライブラー情報量）",
        "Opt3": "標準偏差",
        "Opt4": "四分位範囲",
        "Opt5": "マハラノビス距離",
        "Opt6": "ユークリッド距離",
        "Answer_Idx": 1,
        "Explanation": "生成モデル（VAE等）において、近似分布と真の分布を近づけるために多用されます。",
        "Link": "https://example.com/kl-divergence"
    },
    {
        "ID": "MS-027",
        "Category": "7.数理・統計",
        "Question": "手元のデータが「どの確率分布から発生した可能性が最も高いか」を基準にパラメータを推定する手法は？",
        "Opt1": "最小二乗法",
        "Opt2": "最尤推定法",
        "Opt3": "ベイズ推定",
        "Opt4": "主成分分析",
        "Opt5": "k-means法",
        "Opt6": "リッジ回帰",
        "Answer_Idx": 1,
        "Explanation": "観測されたデータの「尤（もっと）もらしさ」を最大化するように重みを決定します。",
        "Link": "https://example.com/maximum-likelihood"
    },
    {
        "ID": "MS-028",
        "Category": "7.数理・統計",
        "Question": "最小二乗法において、残差（予測値と実測値の差）の何を最小にするように直線を引くか？",
        "Opt1": "絶対値の和",
        "Opt2": "二乗和",
        "Opt3": "平均値",
        "Opt4": "最大値",
        "Opt5": "中央値",
        "Opt6": "分散",
        "Answer_Idx": 1,
        "Explanation": "二乗することで計算を容易にし（微分可能）、大きな誤差に対して大きなペナルティを与えます。",
        "Link": "https://example.com/least-squares-method"
    },
    {
        "ID": "MS-029",
        "Category": "7.数理・統計",
        "Question": "制約条件のある中で関数の極値を求めるために、ラグランジュ乗数を用いる数学的手法は？",
        "Opt1": "テイラー展開",
        "Opt2": "ラグランジュの未定乗数法",
        "Opt3": "ニュートン法",
        "Opt4": "勾配降下法",
        "Opt5": "モンテカルロ法",
        "Opt6": "シンプソンの公式",
        "Answer_Idx": 1,
        "Explanation": "サポートベクターマシン（SVM）の最適化問題などで決定的な役割を果たします。",
        "Link": "https://example.com/lagrange-multipliers"
    },
    {
        "ID": "MS-030",
        "Category": "7.数理・統計",
        "Question": "線形代数において、ベクトル $v$ に行列 $A$ を掛けた時、方向が変わらずに長さだけが変わるようなベクトルを何というか？",
        "Opt1": "零ベクトル",
        "Opt2": "固有ベクトル",
        "Opt3": "逆ベクトル",
        "Opt4": "法線ベクトル",
        "Opt5": "単位ベクトル",
        "Opt6": "基底ベクトル",
        "Answer_Idx": 1,
        "Explanation": "データ圧縮や特徴抽出の際に、重要なデータの「軸」を見つけるために使われます。",
        "Link": "https://example.com/eigenvector"
    },
    {
        "ID": "MS-031",
        "Category": "7.数理・統計",
        "Question": "行列の対角成分（左上から右下への対角線上の要素）の和を何というか？",
        "Opt1": "行列式",
        "Opt2": "トレース（跡）",
        "Opt3": "ランク",
        "Opt4": "転置",
        "Opt5": "ノルム",
        "Opt6": "逆行列",
        "Answer_Idx": 1,
        "Explanation": "行列の性質を調べるための指標の一つで、固有値の総和とも一致します。",
        "Link": "https://example.com/matrix-trace"
    },
    {
        "ID": "MS-032",
        "Category": "7.数理・統計",
        "Question": "多変数関数の局所的な「平坦さ」や「曲がり具合」を表す、二階偏導関数を並べた行列を何というか？",
        "Opt1": "ヤコビ行列",
        "Opt2": "ヘッセ行列",
        "Opt3": "分散共分散行列",
        "Opt4": "相関行列",
        "Opt5": "単位行列",
        "Opt6": "隣接行列",
        "Answer_Idx": 1,
        "Explanation": "最適化アルゴリズム（2次近似を用いる手法）において、収束を早めるために使われます。",
        "Link": "https://example.com/hessian-matrix"
    },
    {
        "ID": "MS-033",
        "Category": "7.数理・統計",
        "Question": "ベクトル値関数（入力も出力もベクトルの関数）の一次偏導関数を並べた行列を何というか？",
        "Opt1": "ヘッセ行列",
        "Opt2": "ヤコビ行列（ヤコビアン）",
        "Opt3": "逆行列",
        "Opt4": "対角行列",
        "Opt5": "回転行列",
        "Opt6": "投影行列",
        "Answer_Idx": 1,
        "Explanation": "座標変換や、ニューラルネットワークの逆伝播における微分の計算に関連します。",
        "Link": "https://example.com/jacobian-matrix"
    },
    {
        "ID": "MS-034",
        "Category": "7.数理・統計",
        "Question": "データの分布において、平均値の周りでの「非対称性（歪み）」を表す指標は？",
        "Opt1": "分散",
        "Opt2": "標準偏差",
        "Opt3": "歪度（わいど）",
        "Opt4": "尖度（せんど）",
        "Opt5": "中央値",
        "Opt6": "範囲",
        "Answer_Idx": 2,
        "Explanation": "右に裾が長いか、左に裾が長いか（正規分布からのズレ）を示します。",
        "Link": "https://example.com/skewness"
    },
    {
        "ID": "MS-035",
        "Category": "7.数理・統計",
        "Question": "データの分布において、山の「鋭さ」や「裾の重さ」を表す指標は？",
        "Opt1": "平均",
        "Opt2": "歪度",
        "Opt3": "尖度",
        "Opt4": "相関係数",
        "Opt5": "決定係数",
        "Opt6": "共分散",
        "Answer_Idx": 2,
        "Explanation": "正規分布（尖度0、定義によっては3）に比べて、どれだけ尖っているかを示します。",
        "Link": "https://example.com/kurtosis"
    },
    {
        "ID": "MS-036",
        "Category": "7.数理・統計",
        "Question": "2つの確率変数の間の連動性を表し、その値をそれぞれの標準偏差で割ると相関係数になる指標は？",
        "Opt1": "分散",
        "Opt2": "共分散",
        "Opt3": "回帰係数",
        "Opt4": "決定係数",
        "Opt5": "寄与率",
        "Opt6": "p値",
        "Answer_Idx": 1,
        "Explanation": "共分散が正なら一方が増えるともう一方も増える傾向があり、負なら逆の傾向があります。",
        "Link": "https://example.com/covariance"
    },
    {
        "ID": "MS-037",
        "Category": "7.数理・統計",
        "Question": "正規分布の平均から標準偏差の何倍（σ数）離れているかで表される、データの相対的な位置は？",
        "Opt1": "p値",
        "Opt2": "zスコア（標準化得点）",
        "Opt3": "t値",
        "Opt4": "F値",
        "Opt5": "R二乗値",
        "Opt6": "バイアス",
        "Answer_Idx": 1,
        "Explanation": "異なる単位や尺度のデータを比較するために、平均0、分散1に変換した値です。",
        "Link": "https://example.com/z-score"
    },
    {
        "ID": "MS-038",
        "Category": "7.数理・統計",
        "Question": "標本平均のばらつき（標本分布の標準偏差）のことを特に何というか？",
        "Opt1": "標準偏差",
        "Opt2": "分散",
        "Opt3": "標準誤差 (Standard Error)",
        "Opt4": "バイアス",
        "Opt5": "四分位偏差",
        "Opt6": "変動係数",
        "Answer_Idx": 2,
        "Explanation": "母集団の平均を推定する際の精度を表す指標として使われます。",
        "Link": "https://example.com/standard-error"
    },
    {
        "ID": "MS-039",
        "Category": "7.数理・統計",
        "Question": "回帰モデルの当てはまりの良さを表し、0から1の範囲で、1に近いほどモデルがデータを説明できているとする指標は？",
        "Opt1": "相関係数",
        "Opt2": "決定係数 ($R^2$)",
        "Opt3": "自由度",
        "Opt4": "標準誤差",
        "Opt5": "p値",
        "Opt6": "残差",
        "Answer_Idx": 1,
        "Explanation": "目的変数の変動のうち、どれだけの割合をモデルで説明できたかを示します。",
        "Link": "https://example.com/coefficient-of-determination"
    },
    {
        "ID": "MS-040",
        "Category": "7.数理・統計",
        "Question": "複雑な関数を、ある一点の周りで多項式（無限和）として近似する数学的手法は？",
        "Opt1": "フーリエ変換",
        "Opt2": "テイラー展開",
        "Opt3": "ラプラス変換",
        "Opt4": "特異値分解",
        "Opt5": "LU分解",
        "Opt6": "コレスキー分解",
        "Answer_Idx": 1,
        "Explanation": "勾配降下法やヘッセ行列による関数の近似（最適化理論）の基礎となります。",
        "Link": "https://example.com/taylor-expansion"
    },
    {
        "ID": "MS-041",
        "Category": "7.数理・統計",
        "Question": "正方行列だけでなく、任意の形状の行列を3つの行列の積に分解する手法で、PCAの計算にも使われるのは？",
        "Opt1": "LU分解",
        "Opt2": "特異値分解 (SVD)",
        "Opt3": "QR分解",
        "Opt4": "コレスキー分解",
        "Opt5": "固有値分解",
        "Opt6": "スペクトル分解",
        "Answer_Idx": 1,
        "Explanation": "データの圧縮やノイズ除去、推薦システム（行列分解）などで非常に重要な役割を果たします。",
        "Link": "https://example.com/svd-details"
    },
    {
        "ID": "MS-042",
        "Category": "7.数理・統計",
        "Question": "主成分分析（PCA）において、各主成分がデータ全体の情報の何割を説明しているかを表す指標は？",
        "Opt1": "相関係数",
        "Opt2": "寄与率",
        "Opt3": "決定係数",
        "Opt4": "累積密度",
        "Opt5": "標準偏差",
        "Opt6": "分散",
        "Answer_Idx": 1,
        "Explanation": "第1主成分から順に寄与率を確認し、何次元まで圧縮するかを判断する基準になります。",
        "Link": "https://example.com/pca-contribution"
    },
    {
        "ID": "MS-043",
        "Category": "7.数理・統計",
        "Question": "ベイズ統計において、解析的に事後分布を求めるのが難しい場合に、乱数を用いて近似的に分布を求める手法の総称は？",
        "Opt1": "最小二乗法",
        "Opt2": "MCMC（マルコフ連鎖モンテカルロ法）",
        "Opt3": "勾配降下法",
        "Opt4": "主成分分析",
        "Opt5": "k-means",
        "Opt6": "EMアルゴリズム",
        "Answer_Idx": 1,
        "Explanation": "高次元なパラメータを持つ複雑なモデルの推定において不可欠な手法です。",
        "Link": "https://example.com/mcmc-basics"
    },
    {
        "ID": "MS-044",
        "Category": "7.数理・統計",
        "Question": "MCMCアルゴリズムの一つで、各変数の条件付き分布から順番にサンプリングを行う手法は？",
        "Opt1": "メトロポリス・ヘイスティングス法",
        "Opt2": "ギブスサンプリング",
        "Opt3": "ハミルトニアンモンテカルロ法",
        "Opt4": "棄却サンプリング",
        "Opt5": "逆関数法",
        "Opt6": "ブートストラップ法",
        "Answer_Idx": 1,
        "Explanation": "多変数の事後分布を効率的にサンプリングするための代表的な手法です。",
        "Link": "https://example.com/gibbs-sampling-details"
    },
    {
        "ID": "MS-045",
        "Category": "7.数理・統計",
        "Question": "ニューラルネットワークの活性化関数「シグモイド関数」の微分値の最大値はいくらか？",
        "Opt1": 1,
        "Opt2": 0.5,
        "Opt3": 0.25,
        "Opt4": 0.1,
        "Opt5": 0,
        "Opt6": "無限大",
        "Answer_Idx": 2,
        "Explanation": "最大値が0.25と小さいため、層を重ねるごとに勾配が消失する原因となります。",
        "Link": "https://example.com/sigmoid-derivative"
    },
    {
        "ID": "MS-046",
        "Category": "7.数理・統計",
        "Question": "ReLU関数の微分において、入力が正（x > 0）の時の値は常にいくつか？",
        "Opt1": 0,
        "Opt2": 1,
        "Opt3": "x",
        "Opt4": "e^x",
        "Opt5": 0.5,
        "Opt6": "不定",
        "Answer_Idx": 1,
        "Explanation": "勾配が1のまま伝播するため、シグモイド関数に比べて勾配消失が起きにくい性質があります。",
        "Link": "https://example.com/relu-math"
    },
    {
        "ID": "MS-047",
        "Category": "7.数理・統計",
        "Question": "2つの確率変数が「独立」であるとき、その共分散の値は必ずいくつになるか？",
        "Opt1": 1,
        "Opt2": 0,
        "Opt3": -1,
        "Opt4": "無限大",
        "Opt5": "データの平均値",
        "Opt6": "計算不能",
        "Answer_Idx": 1,
        "Explanation": "独立であれば相関も0になります（逆は必ずしも真ではありません）。",
        "Link": "https://example.com/independence-covariance"
    },
    {
        "ID": "MS-048",
        "Category": "7.数理・統計",
        "Question": "あるイベントが「平均してλ回起こる」とき、次にそのイベントが起こるまでの「時間間隔」が従う分布は？",
        "Opt1": "ポアソン分布",
        "Opt2": "指数分布",
        "Opt3": "二項分布",
        "Opt4": "幾何分布",
        "Opt5": "正規分布",
        "Opt6": "対数正規分布",
        "Answer_Idx": 1,
        "Explanation": "ポアソン過程に関連する分布で、機械の故障間隔などのモデル化に使われます。",
        "Link": "https://example.com/exponential-dist"
    },
    {
        "ID": "MS-049",
        "Category": "7.数理・統計",
        "Question": "複数のモデルの予測値（確率分布）の平均をとる際、エントロピーの観点から「最も偏りがない」分布を選ぶ原理は？",
        "Opt1": "最大エントロピー原理",
        "Opt2": "最小二乗原理",
        "Opt3": "最尤原理",
        "Opt4": "不確定性原理",
        "Opt5": "大数の法則",
        "Opt6": "中心極限定理",
        "Answer_Idx": 0,
        "Explanation": "制約条件を満たしつつ、最も不確実性が大きい（情報を捨てない）分布を採用する考え方です。",
        "Link": "https://example.com/max-entropy"
    },
    {
        "ID": "MS-050",
        "Category": "7.数理・統計",
        "Question": "多変量解析において、説明変数同士に強い相関があることで回帰係数の推定が不安定になる現象は？",
        "Opt1": "オーバーフィッティング",
        "Opt2": "多重共線性（マルチコリニアリティ）",
        "Opt3": "不均一分散",
        "Opt4": "自己相関",
        "Opt5": "バイアス",
        "Opt6": "ハルシネーション",
        "Answer_Idx": 1,
        "Explanation": "変数の選択や、リッジ回帰などの正則化による対処が必要になります。",
        "Link": "https://example.com/multico"
    },
    {
        "ID": "MS-051",
        "Category": "7.数理・統計",
        "Question": "リッジ回帰（L2正則化）において、損失関数に加えられるペナルティ項は、重みベクトルの何に基づいているか？",
        "Opt1": "L1ノルム（絶対値の和）",
        "Opt2": "L2ノルム（二乗和）",
        "Opt3": "無限大ノルム",
        "Opt4": "行列式",
        "Opt5": "トレース",
        "Opt6": "固有値",
        "Answer_Idx": 1,
        "Explanation": "重みの二乗和を抑えることで、特定の重みが極端に大きくなるのを防ぎます。",
        "Link": "https://example.com/l2-regularization"
    },
    {
        "ID": "MS-052",
        "Category": "7.数理・統計",
        "Question": "Lasso回帰（L1正則化）のペナルティ項の幾何学的な形状（等高線）はどのような形か？",
        "Opt1": "円形",
        "Opt2": "正方形（ダイヤモンド型）",
        "Opt3": "楕円形",
        "Opt4": "直線",
        "Opt5": "双曲線",
        "Opt6": "放物線",
        "Answer_Idx": 1,
        "Explanation": "角の部分で最適解が得られやすいため、一部の重みが完全に0になり（スパース性）、変数選択の効果があります。",
        "Link": "https://example.com/l1-lasso-shape"
    },
    {
        "ID": "MS-053",
        "Category": "7.数理・統計",
        "Question": "「モデルの複雑さ」と「データの適合度」のバランスをとるための指標で、値が小さいほど良いモデルとされるのは？",
        "Opt1": "相関係数",
        "Opt2": "AIC（赤池情報量規準）",
        "Opt3": "決定係数",
        "Opt4": "寄与率",
        "Opt5": "p値",
        "Opt6": "F値",
        "Answer_Idx": 1,
        "Explanation": "過学習を避け、汎化性能の高いモデルを選択するための統計的指標です。",
        "Link": "https://example.com/aic-details"
    },
    {
        "ID": "MS-054",
        "Category": "7.数理・統計",
        "Question": "多クラス分類において、各クラスの出力スコアを合計が1になる確率分布に変換する関数は？",
        "Opt1": "シグモイド関数",
        "Opt2": "ソフトマックス関数",
        "Opt3": "ステップ関数",
        "Opt4": "tanh関数",
        "Opt5": "ReLU関数",
        "Opt6": "Identity関数",
        "Answer_Idx": 1,
        "Explanation": "指数関数を用いることで、最大値を強調しつつ確率として扱えるようにします。",
        "Link": "https://example.com/softmax-math"
    },
    {
        "ID": "MS-055",
        "Category": "7.数理・統計",
        "Question": "「誤差（Loss）」を各重みで偏微分したベクトルを何というか？",
        "Opt1": "ヘッセ行列",
        "Opt2": "勾配 (Gradient)",
        "Opt3": "ヤコビアン",
        "Opt4": "スカラー",
        "Opt5": "テンソル",
        "Opt6": "バイアス",
        "Answer_Idx": 1,
        "Explanation": "勾配の逆方向に重みを動かすことで、誤差を最小化していきます（勾配降下法）。",
        "Link": "https://example.com/gradient-descent-math"
    },
    {
        "ID": "MS-056",
        "Category": "7.数理・統計",
        "Question": "行列 $A$ が正則である（逆行列が存在する）ための必要十分条件は？",
        "Opt1": "行列式が0である",
        "Opt2": "行列式が0でない",
        "Opt3": "転置行列が等しい",
        "Opt4": "単位行列である",
        "Opt5": "ランクが0である",
        "Opt6": "正方行列でない",
        "Answer_Idx": 1,
        "Explanation": "行列式（det）が0になるとき、その行列は「特異」であると言い、逆行列を持ちません。",
        "Link": "https://example.com/determinant-inverse"
    },
    {
        "ID": "MS-057",
        "Category": "7.数理・統計",
        "Question": "確率変数 $X$ の期待値を $E[X]$ とするとき、定数 $a",
        "Opt1": "b$ に対して $E[aX + b]$ はどうなるか？",
        "Opt2": "E[X]",
        "Opt3": "aE[X] + b",
        "Opt4": "a^2E[X] + b",
        "Opt5": "E[aX] + E[b]",
        "Opt6": "aE[X]",
        "Answer_Idx": "E[X] + b",
        "Explanation": 1,
        "Link": "期待値には線形性があり、和の期待値は期待値の和になります。"
    },
    {
        "ID": "MS-058",
        "Category": "7.数理・統計",
        "Question": "2つの事象 $A$ と $B$ が同時に起こる確率 $P(A \\cap B)$ が $P(A)P(B)$ と等しいとき、この2つは？",
        "Opt1": "排反である",
        "Opt2": "独立である",
        "Opt3": "従属である",
        "Opt4": "条件付きである",
        "Opt5": "相関がある",
        "Opt6": "同一である",
        "Answer_Idx": 1,
        "Explanation": "一方が起きることが、もう一方が起きる確率に影響を与えない状態です。",
        "Link": "https://example.com/prob-independence"
    },
    {
        "ID": "MS-059",
        "Category": "7.数理・統計",
        "Question": "ニューラルネットワークの学習で、データの順序をランダムに入れ替えて（シャッフルして）学習させる主な理由は？",
        "Opt1": "計算を速くするため",
        "Opt2": "特定のデータの並び順への依存（バイアス）を防ぐため",
        "Opt3": "メモリ消費を抑えるため",
        "Opt4": "層を深くするため",
        "Opt5": "活性化関数を正則化するため",
        "Opt6": "ラベルを増やすため",
        "Answer_Idx": 1,
        "Explanation": "ミニバッチ学習において、各ステップでの勾配が特定の傾向に偏るのを防ぎ、収束を安定させます。",
        "Link": "https://example.com/data-shuffling"
    },
    {
        "ID": "MS-060",
        "Category": "7.数理・統計",
        "Question": "分類問題において、真のラベルが1であるものを1と予測した割合（再現率）と、0と予測した割合（偽陰性率）の合計は？",
        "Opt1": 0.5,
        "Opt2": 1,
        "Opt3": "不明",
        "Opt4": "精度の値",
        "Opt5": "F値",
        "Opt6": 0,
        "Answer_Idx": 1,
        "Explanation": "1つのデータに対する予測は「当たるか外れるか」のどちらかであるため、確率は合計1になります。",
        "Link": "https://example.com/recall-fnr"
    },
    {
        "ID": "MS-061",
        "Category": "7.数理・統計",
        "Question": "統計的仮説検定において、帰無仮説が真であるにもかかわらず、誤って棄却してしまうことを何というか？",
        "Opt1": "第一種の過誤",
        "Opt2": "第二種の過誤",
        "Opt3": "標準誤差",
        "Opt4": "信頼区間",
        "Opt5": "有意確率",
        "Opt6": "検出力",
        "Answer_Idx": 0,
        "Explanation": "「あわてんぼうの誤り」とも呼ばれ、実際には差がないのに差があると判断するミスです。",
        "Link": "https://example.com/type-1-error"
    },
    {
        "ID": "MS-062",
        "Category": "7.数理・統計",
        "Question": "帰無仮説が偽（実際には差がある）であるときに、正しく帰無仮説を棄却できる確率を何というか？",
        "Opt1": "有意水準",
        "Opt2": "検出力 (Power)",
        "Opt3": "第一種の過誤",
        "Opt4": "p値",
        "Opt5": "zスコア",
        "Opt6": "決定係数",
        "Answer_Idx": 1,
        "Explanation": "「1 - 第二種の過誤」で計算され、実験や分析が差を見抜く力の強さを表します。",
        "Link": "https://example.com/statistical-power"
    },
    {
        "ID": "MS-063",
        "Category": "7.数理・統計",
        "Question": "行列において、線形独立な行（または列）の最大数を何というか？",
        "Opt1": "トレース",
        "Opt2": "行列式",
        "Opt3": "ランク (階数)",
        "Opt4": "次元",
        "Opt5": "ノルム",
        "Opt6": "固有値",
        "Answer_Idx": 2,
        "Explanation": "行列が保持している「実質的な情報の次元数」を意味します。",
        "Link": "https://example.com/matrix-rank"
    },
    {
        "ID": "MS-064",
        "Category": "7.数理・統計",
        "Question": "情報理論において、2つの確率変数が共有している情報の量を表し、片方を知ることでもう片方をどれだけ予測できるかを示す指標は？",
        "Opt1": "自己情報量",
        "Opt2": "エントロピー",
        "Opt3": "相互情報量",
        "Opt4": "KLダイバージェンス",
        "Opt5": "交差エントロピー",
        "Opt6": "分散",
        "Answer_Idx": 2,
        "Explanation": "変数間の非線形な依存関係を捉えることができる指標です。",
        "Link": "https://example.com/mutual-information"
    },
    {
        "ID": "MS-065",
        "Category": "7.数理・統計",
        "Question": "低次元では分離不可能なデータを、高次元空間へ写像することで線形分離可能にするSVMなどで使われる手法は？",
        "Opt1": "主成分分析",
        "Opt2": "カーネル法",
        "Opt3": "ドロップアウト",
        "Opt4": "プーリング",
        "Opt5": "正則化",
        "Opt6": "標準化",
        "Answer_Idx": 1,
        "Explanation": "カーネル関数を用いることで、高次元の計算を直接行わずに非線形な境界を学習できます。",
        "Link": "https://example.com/kernel-method"
    },
    {
        "ID": "MS-066",
        "Category": "7.数理・統計",
        "Question": "線形回帰において、L1正則化とL2正則化を組み合わせて使用する手法を何というか？",
        "Opt1": "リッジ回帰",
        "Opt2": "Lasso回帰",
        "Opt3": "Elastic Net",
        "Opt4": "ロジスティック回帰",
        "Opt5": "多項式回帰",
        "Opt6": "ステップワイズ法",
        "Answer_Idx": 2,
        "Explanation": "Lassoの変数選択機能と、リッジの安定性を両立させた手法です。",
        "Link": "https://example.com/elastic-net"
    },
    {
        "ID": "MS-067",
        "Category": "7.数理・統計",
        "Question": "ある標本データから重複を許して何度も再サンプリングを行い、統計量の分布を推定する手法を何というか？",
        "Opt1": "ジャックナイフ法",
        "Opt2": "ブートストラップ法",
        "Opt3": "モンテカルロ法",
        "Opt4": "交差検証法",
        "Opt5": "ホールドアウト法",
        "Opt6": "正則化",
        "Answer_Idx": 1,
        "Explanation": "データが少ない場合でも、平均や標準偏差の信頼区間を推定するのに役立ちます。",
        "Link": "https://example.com/bootstrap-method"
    },
    {
        "ID": "MS-068",
        "Category": "7.数理・統計",
        "Question": "中心極限定理において、標本のサイズ $n$ が大きくなるにつれて、標本平均の分布の分散はどう変化するか？",
        "Opt1": "大きくなる",
        "Opt2": "小さくなる ($\\sigma^2/n$)",
        "Opt3": "変わらない",
        "Opt4": "0になる",
        "Opt5": "1になる",
        "Opt6": "マイナスになる",
        "Answer_Idx": 1,
        "Explanation": "サンプル数が増えるほど標本平均のばらつきが減り、母平均の推定精度が上がります。",
        "Link": "https://example.com/clt-variance"
    },
    {
        "ID": "MS-069",
        "Category": "7.数理・統計",
        "Question": "確率変数 $X",
        "Opt1": "Y$ の独立性を判断する際、 $P(X",
        "Opt2": "Y) = P(X)P(Y)$ が成り立つことと等価な条件は？",
        "Opt3": "共分散が最大である",
        "Opt4": "自己情報量が等しい",
        "Opt5": "相互情報量が0である",
        "Opt6": "相関係数が1である",
        "Answer_Idx": "期待値が0である",
        "Explanation": "分散が等しい",
        "Link": 2
    },
    {
        "ID": "MS-070",
        "Category": "7.数理・統計",
        "Question": "データ $x$ と重み $w$ の内積 $w^T x$ を計算した際、その値が幾何学的に表しているものは何か？",
        "Opt1": "データと重みの距離",
        "Opt2": "データと重みの類似度（射影）",
        "Opt3": "データの分散",
        "Opt4": "データの平均",
        "Opt5": "データの次元数",
        "Opt6": "データのランク",
        "Answer_Idx": 1,
        "Explanation": "内積が大きいほど、重みベクトルが向いている方向とデータの方向が一致していることを示します。",
        "Link": "https://example.com/dot-product-meaning"
    },
    {
        "ID": "MS-071",
        "Category": "7.数理・統計",
        "Question": "関数 $f(x)$ の期待値 $E[f(x)]$ を、確率密度関数 $p(x)$ を用いた積分の形で表す際に正しい形式は？",
        "Opt1": "$\\int p(x) dx$",
        "Opt2": "$\\int f(x) p(x) dx$",
        "Opt3": "$\\int f(x) dx$",
        "Opt4": "$\\int (f(x) - p(x)) dx$",
        "Opt5": "$\\int f(x)^2 dx$",
        "Opt6": "$\\int \\frac{f(x)}{p(x)} dx$",
        "Answer_Idx": 1,
        "Explanation": "各値にその発生確率を掛けて合計（積分）する、期待値の定義そのものです。",
        "Link": "https://example.com/expectation-integral"
    },
    {
        "ID": "MS-072",
        "Category": "7.数理・統計",
        "Question": "ベイズ推論において、分母にある「データが得られる確率 $P(D)$」を何というか？",
        "Opt1": "事前確率",
        "Opt2": "事後確率",
        "Opt3": "尤度",
        "Opt4": "周辺尤度（エビデンス）",
        "Opt5": "条件付き確率",
        "Opt6": "損失",
        "Answer_Idx": 3,
        "Explanation": "事後確率を1に正規化するための定数としての役割を持ちます。",
        "Link": "https://example.com/marginal-likelihood"
    },
    {
        "ID": "MS-073",
        "Category": "7.数理・統計",
        "Question": "誤差逆伝播法において、合成関数の微分を分解する際に使われる「連鎖律」を数式で表すと？",
        "Opt1": "$\\frac{dy}{dx} = \\frac{dy}{du} \\cdot \\frac{du}{dx}$",
        "Opt2": "$\\frac{dy}{dx} = \\frac{dy}{du} + \\frac{du}{dx}$",
        "Opt3": "$\\frac{dy}{dx} = \\frac{du}{dy} \\cdot \\frac{dx}{du}$",
        "Opt4": "$\\frac{dy}{dx} = y \\cdot u$",
        "Opt5": "$y' = f'(x)g'(x)$",
        "Opt6": "$y' = f'(g'(x))$",
        "Answer_Idx": 0,
        "Explanation": "中間変数 $u$ を介して微分を掛け合わせることで、深い層まで勾配を伝えます。",
        "Link": "https://example.com/chain-rule-formula"
    },
    {
        "ID": "MS-074",
        "Category": "7.数理・統計",
        "Question": "ロジスティック回帰の損失関数として使われる、予測確率と正解ラベルの乖離を測る関数は？",
        "Opt1": "平均二乗誤差",
        "Opt2": "対数損失（交差エントロピー損失）",
        "Opt3": "ヒンジ損失",
        "Opt4": "絶対損失",
        "Opt5": "ハブ損失",
        "Opt6": "コサイン損失",
        "Answer_Idx": 1,
        "Explanation": "確率分布の間の距離を最小化する最尤推定の考え方に基づいています。",
        "Link": "https://example.com/log-loss"
    },
    {
        "ID": "MS-075",
        "Category": "7.数理・統計",
        "Question": "行列 $A$ が対角行列であるとき、その固有値はどうなるか？",
        "Opt1": "すべて1になる",
        "Opt2": "すべて0になる",
        "Opt3": "対角成分の値そのものになる",
        "Opt4": "行列式に等しくなる",
        "Opt5": "逆行列の成分になる",
        "Opt6": "計算できない",
        "Answer_Idx": 2,
        "Explanation": "対角行列は計算が非常に容易なため、行列の「対角化」は重要な数学的処理です。",
        "Link": "https://example.com/diagonal-matrix-eigenvalues"
    },
    {
        "ID": "MS-076",
        "Category": "7.数理・統計",
        "Question": "統計学において、グループ間の平均値の差が統計的に有意かどうかを「分散」を用いて判定する手法は？",
        "Opt1": "回帰分析",
        "Opt2": "分散分析 (ANOVA)",
        "Opt3": "主成分分析",
        "Opt4": "因子分析",
        "Opt5": "判別分析",
        "Opt6": "クラスター分析",
        "Answer_Idx": 1,
        "Explanation": "3つ以上のグループの平均を比較する際によく使われます。",
        "Link": "https://example.com/anova-details"
    },
    {
        "ID": "MS-077",
        "Category": "7.数理・統計",
        "Question": "決定木の不純度を測る指標の一つで、 $1 - \\sum p_i^2$ で定義されるものは？",
        "Opt1": "エントロピー",
        "Opt2": "ジニ係数",
        "Opt3": "情報利得",
        "Opt4": "寄与率",
        "Opt5": "相関係数",
        "Opt6": "決定係数",
        "Answer_Idx": 1,
        "Explanation": "CARTアルゴリズムなどで、データの分割の良さを評価するために使われます。",
        "Link": "https://example.com/gini-impurity"
    },
    {
        "ID": "MS-078",
        "Category": "7.数理・統計",
        "Question": "データの「中央値」が平均値よりも極端に小さい場合、その分布の形状はどうなっている可能性が高いか？",
        "Opt1": "左右対称である",
        "Opt2": "右（正の方向）に長い裾を引いている",
        "Opt3": "左（負の方向）に長い裾を引いている",
        "Opt4": "一様である",
        "Opt5": "二峰性である",
        "Opt6": "垂直である",
        "Answer_Idx": 1,
        "Explanation": "一部の非常に大きな値（外れ値）が平均を引き上げている状態です（所得分布など）。",
        "Link": "https://example.com/mean-median-skew"
    },
    {
        "ID": "MS-079",
        "Category": "7.数理・統計",
        "Question": "正規分布のグラフにおいて、変曲点（カーブの曲がり方が変わる点）はどこにあるか？",
        "Opt1": "平均 $\\mu$ の位置",
        "Opt2": "$\\mu \\pm \\sigma$（標準偏差1倍）の位置",
        "Opt3": "$\\mu \\pm 2\\sigma$ の位置",
        "Opt4": "常に0の位置",
        "Opt5": "最大値の位置",
        "Opt6": "存在しない",
        "Answer_Idx": 1,
        "Explanation": "この範囲にデータ全体の約68%が含まれるという統計的特徴と密接に関連します。",
        "Link": "https://example.com/normal-inflection"
    },
    {
        "ID": "MS-080",
        "Category": "7.数理・統計",
        "Question": "数学的に「もっともらしい」結果を出すために、AIを学ぶ人が最初に理解すべき「データからモデルを推定する」一連のプロセスを総称して？",
        "Opt1": "プログラミング",
        "Opt2": "統計的推論",
        "Opt3": "ハードウェア設計",
        "Opt4": "データ拡張",
        "Opt5": "量子化",
        "Opt6": "剪定",
        "Answer_Idx": 1,
        "Explanation": "これこそが機械学習の正体であり、数理・統計を学ぶ究極の目的です。",
        "Link": "https://example.com/statistical-inference-conclusion"
    }
]
