[
    {
        "ID": "AI-001",
        "Category": "1.AIとは",
        "Question": "「人工知能（Artificial Intelligence）」という言葉を1956年のダートマス会議で初めて提唱した人物は？",
        "Opt1": "アラン・チューリング",
        "Opt2": "ジョン・マッカーシー",
        "Opt3": "マービン・ミンスキー",
        "Opt4": "アーサー・サミュエル",
        "Opt5": "ジョン・サール",
        "Opt6": "フェイフェイ・リ",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】提唱者はジョン・マッカーシーです。チューリングは「計算する機械と知性」を書き、サミュエルは「機械学習」という言葉を広めました。歴史の起点として最頻出です。\""
    },
    {
        "ID": "AI-002",
        "Category": "1.AIとは",
        "Question": "アラン・チューリングが提唱した「チューリング・テスト」の説明として正しいものはどれか？",
        "Opt1": "機械が意識を持っているかどうかを脳科学的に判定するテスト",
        "Opt2": "機械が人間と見分けがつかないような知的な対話ができるかを判定するテスト",
        "Opt3": "チェスや囲碁で人間を負かすことができるかを判定するテスト",
        "Opt4": "フレーム問題を解決できる能力があるかを判定するテスト",
        "Opt5": "機械が自己複製可能かどうかを判定するテスト",
        "Opt6": "記号を実世界と結びつけられるかを判定するテスト",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】チューリング・テストは「見分けがつかないこと」を基準にします。意識の有無ではなく、振る舞い（対話）を重視します。[判定者] --- [壁] --- [人間/機械]\""
    },
    {
        "ID": "AI-003",
        "Category": "1.AIとは",
        "Question": "第1次AIブーム（1950年代〜60年代）における主要な研究対象は？",
        "Opt1": "エキスパートシステム",
        "Opt2": "ディープラーニング",
        "Opt3": "推論・探索",
        "Opt4": "ビッグデータ解析",
        "Opt5": "知識表現",
        "Opt6": "遺伝的アルゴリズム",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】第1次＝推論・探索、第2次＝知識、第3次＝学習。この区分は絶対暗記です。迷路やパズルといった「トイ・プロブレム」が限界でした。\""
    },
    {
        "ID": "AI-004",
        "Category": "1.AIとは",
        "Question": "ジョン・サールが「中国語の部屋」という思考実験を通じて批判した概念は？",
        "Opt1": "弱いAI",
        "Opt2": "強いAI",
        "Opt3": "特化型AI",
        "Opt4": "記号接地問題",
        "Opt5": "フレーム問題",
        "Opt6": "ムーアの法則",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】「強いAI（人間のような意識・理解を持つAI）」への批判です。部屋の中にいる人はマニュアルに従っているだけで中国語を「理解」していない、という議論です。\""
    },
    {
        "ID": "AI-005",
        "Category": "1.AIとは",
        "Question": "現在の主流である「特定のタスク（画像認識や翻訳など）のみを実行するAI」を指す言葉として適切なものは？",
        "Opt1": "強いAI",
        "Opt2": "汎用AI",
        "Opt3": "弱いAI",
        "Opt4": "超人工知能",
        "Opt5": "AGI",
        "Opt6": "シンギュラリティ",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】現在のAIはすべて「弱いAI（特化型AI）」に分類されます。人間のように多目的にこなすのは「汎用AI（AGI）」です。\""
    },
    {
        "ID": "AI-006",
        "Category": "1.AIとは",
        "Question": "第1次AIブームの限界を指摘した「トイ・プロブレム（おもちゃの問題）」の説明として正しいものは？",
        "Opt1": "子供のおもちゃを作る程度の技術しかなかったこと",
        "Opt2": "迷路やチェスのようなルールが定義された問題は解けるが、複雑な現実問題は解けないこと",
        "Opt3": "データの蓄積が不十分で学習が進まなかったこと",
        "Opt4": "コンピュータの価格が高すぎて一般普及しなかったこと",
        "Opt5": "専門家の知識を入力するのが困難だったこと",
        "Opt6": "インターネットがなかったため通信できなかったこと",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】現実世界はルールが曖昧で複雑すぎるため、当時の「探索木」の手法では計算量が爆発してしまいました。\""
    },
    {
        "ID": "AI-007",
        "Category": "1.AIとは",
        "Question": "探索アルゴリズムにおいて、開始点からゴールまでを「現在の深さ」を優先して探していく手法は？",
        "Opt1": "幅優先探索",
        "Opt2": "深さ優先探索",
        "Opt3": "最良優先探索",
        "Opt4": "A*アルゴリズム",
        "Opt5": "双方向探索",
        "Opt6": "ランダムウォーク",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】[木構造] --- [深さ優先：縦に深く進む] / [幅優先：横に広がりながら進む]。深さ優先はメモリ消費が少ない傾向にあります。\""
    },
    {
        "ID": "AI-008",
        "Category": "1.AIとは",
        "Question": "チェスなどの対戦ゲームで使われる、相手が最善を尽くすと仮定して自分の損失を最小にする探索手法は？",
        "Opt1": "ハフマン法",
        "Opt2": "ミニマックス法",
        "Opt3": "モンテカルロ法",
        "Opt4": "バックプロパゲーション",
        "Opt5": "k-近傍法",
        "Opt6": "主成分分析",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】対戦相手も賢いと仮定して、最悪のシナリオの中で最もマシな手（MIN値の中のMAX）を選ぶ手法です。\""
    },
    {
        "ID": "AI-009",
        "Category": "1.AIとは",
        "Question": "1960年代に開発された、精神療法医のように振る舞う世界初のチャットボットの名前は？",
        "Opt1": "Siri",
        "Opt2": "Watson",
        "Opt3": "ELIZA",
        "Opt4": "SHRDLU",
        "Opt5": "Cyc",
        "Opt6": "Pearly",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】イライザ（ELIZA）は「人工無脳」と呼ばれ、単純なルール（オウム返しなど）で人間に知能を感じさせました。\""
    },
    {
        "ID": "AI-010",
        "Category": "1.AIとは",
        "Question": "第2次AIブーム（1980年代）において、専門家の知識をルール化して組み込んだシステムを何というか？",
        "Opt1": "ディープラーニング",
        "Opt2": "エキスパートシステム",
        "Opt3": "パーセプトロン",
        "Opt4": "遺伝的アルゴリズム",
        "Opt5": "サポートベクターマシン",
        "Opt6": "強化学習",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】「もし〜なら、〜せよ」というIF-THEN形式の知識（ルール）を大量に詰め込んだシステムです。\""
    },
    {
        "ID": "AI-011",
        "Category": "1.AIとは",
        "Question": "エキスパートシステムの代表例で、血液中の細菌感染症の診断を支援したシステムは？",
        "Opt1": "DENDRAL",
        "Opt2": "MYCIN",
        "Opt3": "AlphaGo",
        "Opt4": "Deep Blue",
        "Opt5": "Shakey",
        "Opt6": "ASIMO",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】MYCIN（マイシン）は医学分野、DENDRAL（デンドラル）は化学分野（分子構造解析）のエキスパートシステムです。\""
    },
    {
        "ID": "AI-012",
        "Category": "1.AIとは",
        "Question": "第2次AIブームの挫折原因となった「知識獲得のボトルネック」の説明として正しいものは？",
        "Opt1": "コンピュータのメモリが不足して知識が保存できなかったこと",
        "Opt2": "専門家の知識を抽出し、矛盾なく入力することが非常に困難だったこと",
        "Opt3": "知識を入力する作業員が不足したこと",
        "Opt4": "著作権の問題で知識を利用できなかったこと",
        "Opt5": "知識の更新速度が遅かったこと",
        "Opt6": "人工知能が知識を拒絶したこと",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】専門家が「無意識に使っている暗黙知」を言語化してコンピュータに入力するのは、膨大なコストと矛盾の問題がありました。\""
    },
    {
        "ID": "AI-013",
        "Category": "1.AIとは",
        "Question": "「有限の処理能力しかないAIが、現実に起こりうるあらゆる出来事すべてに対処することはできない」という問題は？",
        "Opt1": "記号接地問題",
        "Opt2": "フレーム問題",
        "Opt3": "モラベックのパラドックス",
        "Opt4": "シンギュラリティ",
        "Opt5": "チューリングテスト",
        "Opt6": "過学習",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】マッカーシーとヘイズが提唱。爆弾付きの台車からバッテリーを回収するロボットの例え（洞窟の問題）が有名です。\""
    },
    {
        "ID": "AI-014",
        "Category": "1.AIとは",
        "Question": "「AIにとって、高度な推論（チェス等）よりも1歳児レベルの身体能力（歩く等）を再現する方がはるかに難しい」という逆説は？",
        "Opt1": "フレーム問題",
        "Opt2": "記号接地問題",
        "Opt3": "モラベックのパラドックス",
        "Opt4": "ムーアの法則",
        "Opt5": "アムダールの法則",
        "Opt6": "ノーフリーランチ定理",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】ハンス・モラベックが指摘。進化の過程で獲得した身体能力は「無意識」すぎるため、プログラム化が困難です。\""
    },
    {
        "ID": "AI-015",
        "Category": "1.AIとは",
        "Question": "「シマウマ」という記号（単語）と、実際のシマウマ（縞模様の動物）がどう結びつくか、という問題を何というか？",
        "Opt1": "フレーム問題",
        "Opt2": "記号接地問題（シンボルグラウンディング問題）",
        "Opt3": "オントロジー問題",
        "Opt4": "特徴量設計問題",
        "Opt5": "バイアス問題",
        "Opt6": "ハルシネーション",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】スティーブン・ハルナッドが提唱。AIには単語の意味（実体）がわからないという、意味論的な課題です。\""
    },
    {
        "ID": "AI-016",
        "Category": "1.AIとは",
        "Question": "1984年から現在も継続されている、世界中の常識をすべて入力しようとしているプロジェクトの名前は？",
        "Opt1": "DeepMind",
        "Opt2": "OpenAI",
        "Opt3": "Cycプロジェクト",
        "Opt4": "Human Brain Project",
        "Opt5": "Geneva Project",
        "Opt6": "CERN",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】ダグラス・レナートらが率いるプロジェクト。第2次ブームの考え方（知識が重要）を究極まで突き詰めようとしています。\""
    },
    {
        "ID": "AI-017",
        "Category": "1.AIとは",
        "Question": "知識の記述において、概念（エンティティ）同士の関係を体系化したものを何というか？",
        "Opt1": "トポロジー",
        "Opt2": "オントロジー",
        "Opt3": "タイポロジー",
        "Opt4": "エコロジー",
        "Opt5": "ニューロロジー",
        "Opt6": "バイオロジー",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】「何が存在するか」を定義する概念体系。情報の整理・共有に使われます。第2次ブームで重要視されました。\""
    },
    {
        "ID": "AI-018",
        "Category": "1.AIとは",
        "Question": "1980年代に日本が国家プロジェクトとして推進したが、十分な成果が出ずに終了した開発計画は？",
        "Opt1": "情報スーパーハイウェイ構想",
        "Opt2": "第5世代コンピュータ開発計画",
        "Opt3": "Society 5.0",
        "Opt4": "デジタル庁創設計画",
        "Opt5": "ニューロコンピュータ計画",
        "Opt6": "スーパー京開発計画",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】並列推論マシンを目指しましたが、当時はハード性能が追いつかず、時代を先取りしすぎたと言われています。\""
    },
    {
        "ID": "AI-019",
        "Category": "1.AIとは",
        "Question": "「特徴量（データのどこに注目するか）」を人間が設計するのではなく、機械が自ら学習する手法は？",
        "Opt1": "エキスパートシステム",
        "Opt2": "ディープラーニング",
        "Opt3": "決定木",
        "Opt4": "線形回帰",
        "Opt5": "k-means法",
        "Opt6": "統計モデル",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】第3次ブーム（現在）の核。以前の機械学習は人間が「特徴量」を教えていましたが、深層学習は自ら見つけます。\""
    },
    {
        "ID": "AI-020",
        "Category": "1.AIとは",
        "Question": "2012年の画像認識コンペILSVRCで、ディープラーニングを用いて圧倒的勝利を収めブームの火付け役となったチームは？",
        "Opt1": "Googleチーム",
        "Opt2": "Microsoftチーム",
        "Opt3": "トロント大学（SuperVision）チーム",
        "Opt4": "スタンフォード大学チーム",
        "Opt5": "MITチーム",
        "Opt6": "IBMチーム",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】ジェフリー・ヒントン教授率いるチーム。これにより、画像認識の世界がディープラーニング一色になりました。\""
    },
    {
        "ID": "AI-021",
        "Category": "1.AIとは",
        "Question": "2045年にAIの知能が人類全体を超えるという予測を何というか？",
        "Opt1": "ビッグバン",
        "Opt2": "シンギュラリティ（技術的特異点）",
        "Opt3": "ポストヒューマン",
        "Opt4": "第四次産業革命",
        "Opt5": "AIエクスプロージョン",
        "Opt6": "デジタルトランスフォーメーション",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】レイ・カーツワイルが提唱。指数関数的な技術進歩により、予測不可能な変化が起きるとされる時点です。\""
    },
    {
        "ID": "AI-022",
        "Category": "1.AIとは",
        "Question": "AI研究における「合理的なエージェント」の定義として最も適切なものは？",
        "Opt1": "常に人間と同じ感情を持って行動する機械",
        "Opt2": "自身の利益を最大化するために不正を行うプログラム",
        "Opt3": "与えられた目的を達成するために、期待される成果を最大化するよう行動するシステム",
        "Opt4": "計算速度が世界一速いコンピュータ",
        "Opt5": "絶対にミスをしない完璧なロボット",
        "Opt6": "自律的に反乱を起こすAI",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】現代のAI教科書の標準的な定義（ラッセル＆ノーヴィグ）。感情や人間らしさではなく「目的達成の効率」を重視します。\""
    },
    {
        "ID": "AI-023",
        "Category": "1.AIとは",
        "Question": "1943年に「形式ニューロン」という、神経細胞を単純な論理ゲートとしてモデル化した人物は？",
        "Opt1": "マカロックとピッツ",
        "Opt2": "ワトソンとクリック",
        "Opt3": "ジョブズとウォズニアック",
        "Opt4": "フォン・ノイマン",
        "Opt5": "チューリング",
        "Opt6": "ヒントン",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】ウォーレン・マカロック（神経生理学者）とウォルター・ピッツ（数学者）。これがニューラルネットワークの数学的起源です。\""
    },
    {
        "ID": "AI-024",
        "Category": "1.AIとは",
        "Question": "「機械学習（Machine Learning）」という言葉を、1959年に初めて定義した人物は？",
        "Opt1": "ジョン・マッカーシー",
        "Opt2": "アーサー・サミュエル",
        "Opt3": "フランク・ローゼンブラット",
        "Opt4": "ドナルド・ヘッブ",
        "Opt5": "ジェフリー・ヒントン",
        "Opt6": "ヤン・ルカン",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】サミュエルはチェス（チェッカー）のプログラムを通じて「明示的にプログラミングされなくても学習する能力」を定義しました。\""
    },
    {
        "ID": "AI-025",
        "Category": "1.AIとは",
        "Question": "1958年に発表された、視覚の仕組みを模した単純なニューラルネットワークの名前は？",
        "Opt1": "パーセプトロン",
        "Opt2": "ネオコグニトロン",
        "Opt3": "アレックスネット",
        "Opt4": "バックプロパゲーション",
        "Opt5": "シグモイド関数",
        "Opt6": "サポートベクターマシン",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】フランク・ローゼンブラットが考案。単層パーセプトロンは線形分離可能な問題しか解けないという弱点がありました。\""
    },
    {
        "ID": "AI-026",
        "Category": "1.AIとは",
        "Question": "1969年に『パーセプトロン』という著書で、単層パーセプトロンは「排他的論理和（XOR）」を解けないと批判し、第1次ブームを終わらせた人物は？",
        "Opt1": "ジョン・マッカーシー",
        "Opt2": "ミンスキーとパパート",
        "Opt3": "ジェフリー・ヒントン",
        "Opt4": "アラン・チューリング",
        "Opt5": "スティーブ・ジョブズ",
        "Opt6": "ビル・ゲイツ",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】マービン・ミンスキーとシーモア・パパート。この数学的な批判により、ニューラルネットワーク研究は冬の時代に入りました。\""
    },
    {
        "ID": "AI-027",
        "Category": "1.AIとは",
        "Question": "探索において、ゴールから逆向きにたどって開始点に到達できるかを確認する手法を何というか？",
        "Opt1": "前方推論",
        "Opt2": "後方推論",
        "Opt3": "深さ優先探索",
        "Opt4": "幅優先探索",
        "Opt5": "山登り法",
        "Opt6": "遺伝的アルゴリズム",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】診断システムなど、「結果（病気）」から「原因（知識）」をたどる際に有効です。第2次ブームの推論エンジンで使われました。\""
    },
    {
        "ID": "AI-028",
        "Category": "1.AIとは",
        "Question": "チェスで当時の世界王者ガルリ・カスパロフを1997年に破ったIBMのコンピュータは？",
        "Opt1": "Watson",
        "Opt2": "Deep Blue",
        "Opt3": "AlphaGo",
        "Opt4": "DeepMind",
        "Opt5": "Siri",
        "Opt6": "Alexa",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】ディープ・ブルー。これは「探索と推論」と「高速な専用回路」による勝利であり、現在のディープラーニングとは異なります。\""
    },
    {
        "ID": "AI-029",
        "Category": "1.AIとは",
        "Question": "2011年に米国のクイズ番組「ジェパディ！」で人間を破ったIBMのシステムは？",
        "Opt1": "Deep Blue",
        "Opt2": "Watson",
        "Opt3": "AlphaGo",
        "Opt4": "ChatGPT",
        "Opt5": "ELIZA",
        "Opt6": "Claude",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】ワトソン（Watson）。膨大な百科事典やWikipediaから、自然言語処理を駆使して答えを導き出しました。\""
    },
    {
        "ID": "AI-030",
        "Category": "1.AIとは",
        "Question": "2016年に囲碁の世界王者（イ・セドル）を破った、Google DeepMind社開発のAIは？",
        "Opt1": "Deep Blue",
        "Opt2": "Watson",
        "Opt3": "AlphaGo",
        "Opt4": "GPT-4",
        "Opt5": "Stable Diffusion",
        "Opt6": "Midjourney",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】アルファ碁（AlphaGo）。ディープラーニングと強化学習（モンテカルロ木探索）を組み合わせた画期的なモデルです。\""
    },
    {
        "ID": "AI-031",
        "Category": "1.AIとは",
        "Question": "「強いAI」と「弱いAI」の分類を提唱した哲学者は？",
        "Opt1": "アラン・チューリング",
        "Opt2": "ジョン・サール",
        "Opt3": "イマヌエル・カント",
        "Opt4": "フリードリヒ・ニーチェ",
        "Opt5": "ミシェル・フーコー",
        "Opt6": "ルネ・デカルト",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】中国語の部屋の提唱者と同じ、ジョン・サールです。G検定では哲学者の名前もよく出ます。\""
    },
    {
        "ID": "AI-032",
        "Category": "1.AIとは",
        "Question": "AIの学習において、データの中に含まれる「人間社会の偏見」をそのまま学習してしまう問題を何というか？",
        "Opt1": "フレーム問題",
        "Opt2": "社会的バイアス",
        "Opt3": "記号接地問題",
        "Opt4": "次元の呪い",
        "Opt5": "オーバーフィッティング",
        "Opt6": "勾配消失",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】「AIは客観的」と思われがちですが、学習データが偏っていると（例：特定の職業に特定の性別が多い等）、差別的な出力をします。\""
    },
    {
        "ID": "AI-033",
        "Category": "1.AIとは",
        "Question": "「知識」をコンピュータが扱えるように記述する手法全般を指す言葉は？",
        "Opt1": "データサイエンス",
        "Opt2": "知識表現",
        "Opt3": "深層学習",
        "Opt4": "自然言語処理",
        "Opt5": "画像認識",
        "Opt6": "音声合成",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】Knowledge Representation。第2次ブームの核となる研究分野です。\""
    },
    {
        "ID": "AI-034",
        "Category": "1.AIとは",
        "Question": "ある問題を解くのに適した「知識」が存在せず、単純な探索だけでは解決できない状態を何というか？",
        "Opt1": "トイプロブレム",
        "Opt2": "知識獲得のボトルネック",
        "Opt3": "次元の爆発",
        "Opt4": "計算量の爆発",
        "Opt5": "フレーム問題",
        "Opt6": "ハルシネーション",
        "Answer_Idx": 3,
        "Explanation": "\"【解説】迷路が複雑になると、考えられるパターンの数が指数関数的に増え、宇宙が終わるまでの時間でも計算が終わらなくなります。\""
    },
    {
        "ID": "AI-035",
        "Category": "1.AIとは",
        "Question": "AI研究における「コネクショニズム」という立場が重視するものは？",
        "Opt1": "論理的な記号操作",
        "Opt2": "ニューラルネットワーク（脳の仕組み）",
        "Opt3": "エキスパートシステム",
        "Opt4": "IF-THENルール",
        "Opt5": "辞書データの入力",
        "Opt6": "ハードウェアの高速化",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】脳を模したネットワークによる学習を重視する派閥。対立するのは記号処理を重視する「記号主義」です。\""
    },
    {
        "ID": "AI-036",
        "Category": "1.AIとは",
        "Question": "1970年代にスタンフォード大学で開発された、初期の自律型移動ロボットの名前は？",
        "Opt1": "ASIMO",
        "Opt2": "Shakey",
        "Opt3": "AIBO",
        "Opt4": "Pepper",
        "Opt5": "Spot",
        "Opt6": "Roomba",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】シェーキー（Shakey）。カメラで周囲を認識し、推論して移動する「震えるほど不器用な」ロボットです。\""
    },
    {
        "ID": "AI-037",
        "Category": "1.AIとは",
        "Question": "1970年代にテリー・ウィノグラードが開発した、仮想の積み木（ブロック）を操作する自然言語理解システムは？",
        "Opt1": "ELIZA",
        "Opt2": "SHRDLU",
        "Opt3": "Cyc",
        "Opt4": "Siri",
        "Opt5": "Google Assistant",
        "Opt6": "Cortana",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】シュルドゥル（SHRDLU）。「赤い積み木を青い箱に入れて」などの複雑な命令を理解できました。\""
    },
    {
        "ID": "AI-038",
        "Category": "1.AIとは",
        "Question": "AIの「自律性」に関し、自分で自分の目的を書き換えてしまうことへの倫理的懸念を何というか？",
        "Opt1": "アライメント問題",
        "Opt2": "フレーム問題",
        "Opt3": "記号接地問題",
        "Opt4": "シンギュラリティ",
        "Opt5": "チューリングテスト",
        "Opt6": "チューリング完全",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】人間の意図とAIの目的がズレてしまう問題。将来の強力なAIにおける重要なリスク管理課題です。\""
    },
    {
        "ID": "AI-039",
        "Category": "1.AIとは",
        "Question": "コンピュータ科学の父と呼ばれ、現代の「プログラム内蔵方式」のコンピュータの基礎を築いた人物は？",
        "Opt1": "アラン・チューリング",
        "Opt2": "ジョン・フォン・ノイマン",
        "Opt3": "スティーブ・ウォズニアック",
        "Opt4": "ビル・ゲイツ",
        "Opt5": "ジェームズ・ワット",
        "Opt6": "トーマス・エジソン",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】ノイマン。現代のPCは「ノイマン型コンピュータ」と呼ばれます。チューリングは理論的基礎、ノイマンは実用的構造。混同注意。\""
    },
    {
        "ID": "AI-040",
        "Category": "1.AIとは",
        "Question": "第2次AIブームの終わりを招いた要因の一つとして、適切なものはどれか？",
        "Opt1": "インターネットの普及により知識が漏洩したこと",
        "Opt2": "パーソナルコンピュータ（PC）の性能向上により、高価な専用AIマシンが不要になったこと",
        "Opt3": "ディープラーニングが発明されたこと",
        "Opt4": "AIが人類を支配し始めたこと",
        "Opt5": "数学的に不可能であることが証明されたこと",
        "Opt6": "電力が不足したこと",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】専用のLispマシン等が売れなくなり、AIビジネスが衰退しました。これを「AIの冬の時代」と呼びます。\""
    },
    {
        "ID": "AI-041",
        "Category": "1.AIとは",
        "Question": "エキスパートシステムの構築において、人間から知識を引き出しコンピュータに実装する専門家を何というか？",
        "Opt1": "プログラマー",
        "Opt2": "ナレッジエンジニア",
        "Opt3": "データサイエンティスト",
        "Opt4": "システムアナリスト",
        "Opt5": "プロジェクトマネージャー",
        "Opt6": "UIデザイナー",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】知識工学（Knowledge Engineering）に携わる専門職。第2次ブームで活躍しました。\""
    },
    {
        "ID": "AI-042",
        "Category": "1.AIとは",
        "Question": "人間の脳の仕組みを模して、物理的な回路として知能を実現しようとするアプローチを何というか？",
        "Opt1": "記号的AI",
        "Opt2": "ニューロコンピュータ",
        "Opt3": "クラウドコンピュータ",
        "Opt4": "メインフレーム",
        "Opt5": "組み込みシステム",
        "Opt6": "エッジコンピューティング",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】ソフトウェアではなく、ハードウェアそのものを脳に似せる考え方です。\""
    },
    {
        "ID": "AI-043",
        "Category": "1.AIとは",
        "Question": "AIがチェスや将棋で人間を負かしても、それが知能の証明にはならないとする「ゴールが移動する現象」を何というか？",
        "Opt1": "フレーム問題",
        "Opt2": "AI効果",
        "Opt3": "シンギュラリティ",
        "Opt4": "モラベックのパラドックス",
        "Opt5": "チューリングテスト",
        "Opt6": "記号接地問題",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】「中身の仕組みがわかってしまうと、それは単なる計算であり知能ではない」と心理的に感じてしまう現象です。\""
    },
    {
        "ID": "AI-044",
        "Category": "1.AIとは",
        "Question": "記号接地問題を解決するためのアプローチとして、「AIにも肉体（センサーやアクチュエータ）が必要である」とする考え方は？",
        "Opt1": "記号主義",
        "Opt2": "身体性（エンボディメント）",
        "Opt3": "論理主義",
        "Opt4": "構築主義",
        "Opt5": "神秘主義",
        "Opt6": "行動主義",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】実際に動いて触って経験しないと、言葉の本当の意味は理解できないという主張です。ロドニー・ブルックスらが提唱。\""
    },
    {
        "ID": "AI-045",
        "Category": "1.AIとは",
        "Question": "第3次AIブーム（2010年代以降）を支える三要素として不適切なものは？",
        "Opt1": "ディープラーニングの進化",
        "Opt2": "ビッグデータの普及",
        "Opt3": "ハードウェア（GPU等）の性能向上",
        "Opt4": "エキスパートシステムの普及",
        "Opt5": "インターネットの発展",
        "Opt6": "クラウドコンピューティングの拡大",
        "Answer_Idx": 3,
        "Explanation": "\"【解説】エキスパートシステムは第2次の遺物です。現在は「アルゴリズム・データ・計算資源」が三種の神器です。\""
    },
    {
        "ID": "AI-046",
        "Category": "1.AIとは",
        "Question": "Googleのレイ・カーツワイルが予測した、技術進歩が加速し、もはや後戻りできない変革が起きる概念を何というか？",
        "Opt1": "収穫加速の法則",
        "Opt2": "ムーアの法則",
        "Opt3": "ギルダーの法則",
        "Opt4": "メトカーフの法則",
        "Opt5": "ジップの法則",
        "Opt6": "パレートの法則",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】「シンギュラリティ」の根拠となる考え方。一度技術が進歩すると、次の進歩はその技術を使ってより速く起きるという法則です。\""
    },
    {
        "ID": "AI-047",
        "Category": "1.AIとは",
        "Question": "「特化型AI」の反対語で、人間のようにあらゆる知的作業を行えるAIを指す略称は？",
        "Opt1": "RNN",
        "Opt2": "CNN",
        "Opt3": "AGI",
        "Opt4": "GAN",
        "Opt5": "LSTM",
        "Opt6": "NLP",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】Artificial General Intelligence（汎用人工知能）。\""
    },
    {
        "ID": "AI-048",
        "Category": "1.AIとは",
        "Question": "人工知能における「ダートマス会議（1956年）」の開催期間は約どれくらいだったか？",
        "Opt1": "1日",
        "Opt2": "3日間",
        "Opt3": "2ヶ月間",
        "Opt4": "1年間",
        "Opt5": "5年間",
        "Opt6": "1時間",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】約2ヶ月間のワークショップでした。たった数日で決まったわけではありません。ひっかけで出ることがあります。\""
    },
    {
        "ID": "AI-049",
        "Category": "1.AIとは",
        "Question": "AIの安全性や倫理に関して、2017年に多くの研究者が署名し採択された23項目の原則は？",
        "Opt1": "アシロマ原則",
        "Opt2": "ジュネーブ諸原則",
        "Opt3": "京都議定書",
        "Opt4": "ブレッチリー宣言",
        "Opt5": "広島AIプロセス",
        "Opt6": "AI開発憲章",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】米国のカリフォルニア州アシロマで開催された会議でまとめられた、AIの有益性を確保するための指針です。\""
    },
    {
        "ID": "AI-050",
        "Category": "1.AIとは",
        "Question": "日本政府が提唱している、サイバー空間とフィジカル空間を高度に融合させた未来社会の姿は？",
        "Opt1": "Industry 4.0",
        "Opt2": "Society 5.0",
        "Opt3": "Smart City 2.0",
        "Opt4": "Digital Japan",
        "Opt5": "Web 3.0",
        "Opt6": "Metaverse Society",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】Society 1.0(狩猟)、2.0(農耕)、3.0(工業)、4.0(情報)に続く、AIやIoTを活用した5番目の社会です。\""
    },
    {
        "ID": "TRD-001",
        "Category": "2.動向",
        "Question": "1950年に「計算する機械と知性」を執筆し、機械に知能があるかを判定する「イミテーション・ゲーム」を提唱した人物は？",
        "Opt1": "ジョン・マッカーシー",
        "Opt2": "マービン・ミンスキー",
        "Opt3": "アラン・チューリング",
        "Opt4": "クロード・シャノン",
        "Opt5": "アーサー・サミュエル",
        "Opt6": "ジェフリー・ヒントン",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】アラン・チューリングです。後に「チューリング・テスト」と呼ばれるこの手法は、対話を通じて人間と見分けがつかなければ知能があるとする機能主義的な考えに基づいています。\""
    },
    {
        "ID": "TRD-002",
        "Category": "2.動向",
        "Question": "1956年のダートマス会議で「人工知能（Artificial Intelligence）」という言葉を初めて提唱した人物は？",
        "Opt1": "アラン・チューリング",
        "Opt2": "ジョン・マッカーシー",
        "Opt3": "ジョン・サール",
        "Opt4": "レイ・カーツワイル",
        "Opt5": "フランク・ローゼンブラット",
        "Opt6": "ヤン・ルカン",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】ジョン・マッカーシーです。「チューリング＝理論的父」「マッカーシー＝名付け親」という区別は超頻出のひっかけポイントです。\""
    },
    {
        "ID": "TRD-003",
        "Category": "2.動向",
        "Question": "第1次AIブーム（1950年代〜60年代）における主要なアプローチはどれか？",
        "Opt1": "知識表現とエキスパートシステム",
        "Opt2": "ビッグデータと機械学習",
        "Opt3": "推論と探索",
        "Opt4": "ニューラルネットワークの多層化",
        "Opt5": "統計的自然言語処理",
        "Opt6": "強化学習",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】第1次は「推論・探索」、第2次は「知識」、第3次は「学習」です。この3区分を入れ替えて出すのがひっかけの王道です。\""
    },
    {
        "ID": "TRD-004",
        "Category": "2.動向",
        "Question": "第1次AIブームが終焉した主な要因となった、迷路やパズルといった単純な問題しか解けない限界を何というか？",
        "Opt1": "フレーム問題",
        "Opt2": "トイ・プロブレム（おもちゃの問題）",
        "Opt3": "記号接地問題",
        "Opt4": "モラベックのパラドックス",
        "Opt5": "シンギュラリティ",
        "Opt6": "計算量爆発",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】マービン・ミンスキーが指摘した言葉です。現実社会の複雑な問題には太刀打ちできないことが露呈し、ブームが去りました。\""
    },
    {
        "ID": "TRD-005",
        "Category": "2.動向",
        "Question": "1960年代に開発された、精神療法医を模倣し世界初のチャットボットとも言われるシステムは？",
        "Opt1": "SHRDLU",
        "Opt2": "ELIZA",
        "Opt3": "Watson",
        "Opt4": "AlphaGo",
        "Opt5": "MYCIN",
        "Opt6": "Deep Blue",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】ジョセフ・ワイゼンバウムが開発したELIZA（イライザ）です。単純なパターンマッチングによる「オウム返し」で人間に知能を感じさせました。\""
    },
    {
        "ID": "TRD-006",
        "Category": "2.動向",
        "Question": "1970年代にテリー・ウィノグラードが開発した、仮想の積み木（ブロック）を操作する自然言語理解システムは？",
        "Opt1": "ELIZA",
        "Opt2": "SHRDLU",
        "Opt3": "Cyc",
        "Opt4": "Siri",
        "Opt5": "Wabot",
        "Opt6": "DENDRAL",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】SHRDLU（シュルドル）です。「赤い積み木を移動させて」などの複雑な命令を理解できましたが、積み木の世界（限定された世界）の中だけでの成功でした。\""
    },
    {
        "ID": "TRD-007",
        "Category": "2.動向",
        "Question": "第2次AIブーム（1980年代）の核心となった、専門家の知識をルールとして教え込むシステムを何というか？",
        "Opt1": "ディープラーニング",
        "Opt2": "エキスパートシステム",
        "Opt3": "パーセプトロン",
        "Opt4": "遺伝的アルゴリズム",
        "Opt5": "サポートベクターマシン",
        "Opt6": "量子コンピュータ",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】専門家の知識（IF-THEN形式）を大量に詰め込んだシステムです。実社会への応用が期待されました。\""
    },
    {
        "ID": "TRD-008",
        "Category": "2.動向",
        "Question": "第2次AIブーム期に開発された、血液中の細菌感染症を診断するエキスパートシステムは？",
        "Opt1": "DENDRAL",
        "Opt2": "MYCIN",
        "Opt3": "AlphaGo",
        "Opt4": "Deep Blue",
        "Opt5": "Shakey",
        "Opt6": "ASIMO",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】MYCIN（マイシン）です。抗生物質の処方ミスを防ぐ目的で開発されました。DENDRALは化学分野（分子構造解析）です。\""
    },
    {
        "ID": "TRD-009",
        "Category": "2.動向",
        "Question": "エキスパートシステムの限界として、知識を矛盾なく抽出しコンピュータに入力する作業が非常に困難であることを何というか？",
        "Opt1": "フレーム問題",
        "Opt2": "知識獲得のボトルネック",
        "Opt3": "記号接地問題",
        "Opt4": "ムーアの法則",
        "Opt5": "次元の呪い",
        "Opt6": "オーバーフィッティング",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】エドワード・ファイゲンバウムらが直面した課題です。人間が無意識に使っている「暗黙知」を言語化するのは極めて困難でした。\""
    },
    {
        "ID": "TRD-010",
        "Category": "2.動向",
        "Question": "「AIにおいて、記号が実世界の実体と結びついていない」という問題を、スティーブン・ハルナッドは何と呼んだか？",
        "Opt1": "フレーム問題",
        "Opt2": "記号接地問題（シンボルグラウンディング問題）",
        "Opt3": "モラベックのパラドックス",
        "Opt4": "チューリング・テスト",
        "Opt5": "中国語の部屋",
        "Opt6": "計算量爆発",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】記号（単語）の意味をAIが真に理解していない問題を指します。シマウマの単語を知っていても、実物を見ても認識できない状態などが例です。[記号] -- (断絶) -- [実体]\""
    },
    {
        "ID": "TRD-011",
        "Category": "2.動向",
        "Question": "「AIが有限の処理能力で、現実に起こりうる無限の出来事すべてに対処することは不可能である」という問題は？",
        "Opt1": "記号接地問題",
        "Opt2": "フレーム問題",
        "Opt3": "トイ・プロブレム",
        "Opt4": "シンギュラリティ",
        "Opt5": "ムーアの法則",
        "Opt6": "次元の呪い",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】マッカーシーとヘイズが提唱。爆弾付き台車の例（洞窟の問題）で有名です。どこまでを考慮すべきかの境界が引けない問題です。\""
    },
    {
        "ID": "TRD-012",
        "Category": "2.動向",
        "Question": "「高度な推論よりも、1歳児レベルの知覚や運動スキルをAIに持たせる方がはるかに難しい」という逆説は？",
        "Opt1": "フレーム問題",
        "Opt2": "記号接地問題",
        "Opt3": "モラベックのパラドックス",
        "Opt4": "シンギュラリティ",
        "Opt5": "アムダールの法則",
        "Opt6": "ブルックスの法則",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】ハンス・モラベックが提唱。進化の過程で獲得した身体能力は、論理的思考よりも高度な計算が必要であることを指します。[論理：易] ↔ [運動：難]\""
    },
    {
        "ID": "TRD-013",
        "Category": "2.動向",
        "Question": "ジョン・サールが「強いAI（意識を持つAI）」の可能性を否定するために提唱した思考実験は？",
        "Opt1": "トロッコ問題",
        "Opt2": "中国語の部屋",
        "Opt3": "シュレディンガーの猫",
        "Opt4": "テセウスの船",
        "Opt5": "マクスウェルの悪魔",
        "Opt6": "囚人のジレンマ",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】部屋の中にいる人がマニュアル（プログラム）に従って中国語を処理しても、その人は意味を「理解」していないとする反論です。\""
    },
    {
        "ID": "TRD-014",
        "Category": "2.動向",
        "Question": "現在のAIのように、特定のタスク（画像認識や翻訳など）に特化して知能を発揮するものを何と呼ぶか？",
        "Opt1": "汎用AI (AGI)",
        "Opt2": "強いAI",
        "Opt3": "弱いAI (Narrow AI)",
        "Opt4": "超人工知能",
        "Opt5": "自律型AI",
        "Opt6": "マルチモーダルAI",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】ジョン・サールによる分類。人間のような意識を持つものを「強いAI」、特定の道具としてのAIを「弱いAI」と呼びました。\""
    },
    {
        "ID": "TRD-015",
        "Category": "2.動向",
        "Question": "1984年から現在も継続されている、人間が持つ「常識」をすべてコンピュータに入力しようとするプロジェクトは？",
        "Opt1": "DeepMindプロジェクト",
        "Opt2": "OpenAIプロジェクト",
        "Opt3": "Cycプロジェクト",
        "Opt4": "Human Brain Project",
        "Opt5": "Geneva Project",
        "Opt6": "CERN",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】ダグラス・レナートらが率いるプロジェクトです。第2次ブームの「知識が重要」という考えを究極まで突き詰めようとしています。\""
    },
    {
        "ID": "TRD-016",
        "Category": "2.動向",
        "Question": "知識の記述において、概念（エンティティ）同士の関係を体系化したものを何というか？",
        "Opt1": "トポロジー",
        "Opt2": "オントロジー",
        "Opt3": "タイポロジー",
        "Opt4": "エコロジー",
        "Opt5": "ニューロロジー",
        "Opt6": "バイオロジー",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】「何が存在するか」を定義する概念体系。情報の整理・共有に使われ、ウェブ（セマンティック・ウェブ）などでも重要です。\""
    },
    {
        "ID": "TRD-017",
        "Category": "2.動向",
        "Question": "1980年代に日本が国家プロジェクトとして推進したが、十分な成果が出ずに終了した開発計画は？",
        "Opt1": "情報スーパーハイウェイ構想",
        "Opt2": "第5世代コンピュータ開発計画",
        "Opt3": "Society 5.0",
        "Opt4": "デジタル庁創設計画",
        "Opt5": "ニューロコンピュータ計画",
        "Opt6": "スーパー京開発計画",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】並列推論マシンを目指しましたが、ハードウェア性能の不足やLispマシンの衰退などの影響を受けました。\""
    },
    {
        "ID": "TRD-018",
        "Category": "2.動向",
        "Question": "1997年にIBMのコンピュータ「ディープ・ブルー」がチェスの世界王者を破った。この相手の名前は？",
        "Opt1": "イ・セドル",
        "Opt2": "ガルリ・カスパロフ",
        "Opt3": "藤井聡太",
        "Opt4": "ケン・ジェニングス",
        "Opt5": "ディープ・ハサビス",
        "Opt6": "フェイフェイ・リ",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】ガルリ・カスパロフです。これは探索と力任せの計算による勝利であり、現在の深層学習とは手法が異なります。\""
    },
    {
        "ID": "TRD-019",
        "Category": "2.動向",
        "Question": "2011年に米国のクイズ番組「ジェパディ！」で人間を破ったIBMのシステムは？",
        "Opt1": "Deep Blue",
        "Opt2": "Watson",
        "Opt3": "AlphaGo",
        "Opt4": "ChatGPT",
        "Opt5": "ELIZA",
        "Opt6": "Siri",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】Watson（ワトソン）です。膨大な非構造化データから答えを導き出す「自然言語処理」の力を示しました。\""
    },
    {
        "ID": "TRD-020",
        "Category": "2.動向",
        "Question": "2016年に囲碁の世界王者を破ったAlphaGoが用いた、強化学習と組み合わせた探索手法は？",
        "Opt1": "幅優先探索",
        "Opt2": "深さ優先探索",
        "Opt3": "モンテカルロ木探索",
        "Opt4": "A*アルゴリズム",
        "Opt5": "双方向探索",
        "Opt6": "山登り法",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】AlphaGoはディープラーニングと「モンテカルロ木探索」を組み合わせ、膨大な選択肢を効率的に絞り込みました。\""
    },
    {
        "ID": "TRD-021",
        "Category": "2.動向",
        "Question": "第3次AIブーム（2010年代〜現在）を支える三要素として最も不適切なものは？",
        "Opt1": "ビッグデータの蓄積",
        "Opt2": "GPU（グラフィックス・プロセッシング・ユニット）の普及",
        "Opt3": "ディープラーニングの進化",
        "Opt4": "エキスパートシステムの普及",
        "Opt5": "クラウドコンピューティングの発展",
        "Opt6": "インターネットの普及",
        "Answer_Idx": 3,
        "Explanation": "\"【解説】エキスパートシステムは「第2次ブーム」の主役です。第3次はデータ、計算資源、アルゴリズムの三本柱です。\""
    },
    {
        "ID": "TRD-022",
        "Category": "2.動向",
        "Question": "2012年の画像認識コンペILSVRCで、ディープラーニングを用いて圧倒的精度で優勝したチームは？",
        "Opt1": "Googleチーム",
        "Opt2": "Microsoftチーム",
        "Opt3": "トロント大学（SuperVision）チーム",
        "Opt4": "スタンフォード大学チーム",
        "Opt5": "MITチーム",
        "Opt6": "IBMチーム",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】ジェフリー・ヒントン教授率いるチームです。モデル名はAlexNet。これが現在のディープラーニングブームの直接の引き金となりました。\""
    },
    {
        "ID": "TRD-023",
        "Category": "2.動向",
        "Question": "レイ・カーツワイルが提唱した、AIが人類全体の知能を上回る時点を何と呼ぶか？",
        "Opt1": "ビッグバン",
        "Opt2": "シンギュラリティ（技術的特異点）",
        "Opt3": "ポストヒューマン",
        "Opt4": "第四次産業革命",
        "Opt5": "AIエクスプロージョン",
        "Opt6": "デジタルトランスフォーメーション",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】2045年に到来すると予測されています。彼は「収穫加速の法則」に基づき、技術進歩が指数関数的に進むと主張しています。\""
    },
    {
        "ID": "TRD-024",
        "Category": "2.動向",
        "Question": "日本政府が提唱する、サイバー空間とフィジカル空間を高度に融合させた未来社会の姿は？",
        "Opt1": "Industry 4.0",
        "Opt2": "Society 5.0",
        "Opt3": "Smart City 2.0",
        "Opt4": "Digital Japan",
        "Opt5": "Web 3.0",
        "Opt6": "Metaverse Society",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】Society 5.0です。狩猟、農耕、工業、情報の後の5番目の社会を指します。\""
    },
    {
        "ID": "TRD-025",
        "Category": "2.動向",
        "Question": "EUで2024年に成立した、AIをリスクに応じて4段階に分類し、厳格に規制する法律は？",
        "Opt1": "GDPR",
        "Opt2": "DMA",
        "Opt3": "EU AI法（人工知能法）",
        "Opt4": "DSA",
        "Opt5": "AI権利章典",
        "Opt6": "プライバシー・シールド",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】世界初の包括的なAI規制法。低リスクから「許容できないリスク（禁止）」までを分類し、違反には高額な制裁金が課されます。\""
    },
    {
        "ID": "TRD-026",
        "Category": "2.動向",
        "Question": "AIがもっともらしい嘘をつく（事実とは異なる内容を生成する）現象を何というか？",
        "Opt1": "オーバーフィッティング",
        "Opt2": "ハルシネーション（幻覚）",
        "Opt3": "モード崩壊",
        "Opt4": "勾配消失",
        "Opt5": "正則化",
        "Opt6": "次元の呪い",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】大規模言語モデル（LLM）が確率的に単語を繋げる性質上、事実に基づかない情報を自信満々に出力してしまう現象です。\""
    },
    {
        "ID": "TRD-027",
        "Category": "2.動向",
        "Question": "AIの安全性や倫理において、AIの目的と人間の価値観・意図を一致させることを何というか？",
        "Opt1": "チューニング",
        "Opt2": "アライメント",
        "Opt3": "バリデーション",
        "Opt4": "正則化",
        "Opt5": "標準化",
        "Opt6": "量子化",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】AIが人間の意図から外れた有害な行動をとらないよう調整することを指します（AIアライメント）。\""
    },
    {
        "ID": "TRD-028",
        "Category": "2.動向",
        "Question": "日本の著作権法第30条の4において、AIの学習（情報解析）のための著作物利用について正しい説明は？",
        "Opt1": "いかなる場合も権利者の許諾が必要",
        "Opt2": "営利目的の場合は利用できない",
        "Opt3": "「享受」を目的としない限り原則として権利者の許諾なく利用できる",
        "Opt4": "AIが生成した画像にも自動的に著作権が発生する",
        "Opt5": "非営利目的の教育機関のみ利用できる",
        "Opt6": "学習に使用したデータはすべて公開しなければならない",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】日本は世界的に見てもAI学習に柔軟な法律を持っていますが、権利者の利益を不当に害する場合は例外となります。\""
    },
    {
        "ID": "TRD-029",
        "Category": "2.動向",
        "Question": "2023年のG7サミットで主導された、生成AIの国際的なルール作りを目指す枠組みは？",
        "Opt1": "広島AIプロセス",
        "Opt2": "ブレッチリー宣言",
        "Opt3": "EU AI法",
        "Opt4": "デジタル万博構想",
        "Opt5": "大阪トラック",
        "Opt6": "OECD指針",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】日本が議長国として主導。生成AIの著作権、偽情報、プライバシーなどの課題に対し、国際的な指針を策定しました。\""
    },
    {
        "ID": "TRD-030",
        "Category": "2.動向",
        "Question": "AIによる自動意思決定（採用選考や融資審査など）において、なぜその結論に至ったかを説明できる能力を何というか？",
        "Opt1": "頑健性",
        "Opt2": "説明可能性（XAI）",
        "Opt3": "透明性",
        "Opt4": "公平性",
        "Opt5": "再現性",
        "Opt6": "保守性",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】Explainable AIの略。特に医療や金融など、人命や権利に関わる分野で重要視されています。\""
    },
    {
        "ID": "TRD-031",
        "Category": "2.動向",
        "Question": "AIが特定の属性（人種、性別など）に対して不利益な判断を下してしまう問題を何というか？",
        "Opt1": "社会的バイアス",
        "Opt2": "過学習",
        "Opt3": "ハルシネーション",
        "Opt4": "アンダーフィッティング",
        "Opt5": "情報漏洩",
        "Opt6": "データ不足",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】学習データに含まれる現実世界の偏見をAIが学習・増幅してしまうリスクです。\""
    },
    {
        "ID": "TRD-032",
        "Category": "2.動向",
        "Question": "AIの計算に欠かせない「GPU（画像処理装置）」の主要メーカーで、現在市場を独占している企業は？",
        "Opt1": "Intel",
        "Opt2": "AMD",
        "Opt3": "NVIDIA",
        "Opt4": "Google",
        "Opt5": "Apple",
        "Opt6": "Microsoft",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】もともとゲーム用の画像処理用だったGPUが、AIの並列計算に非常に適していることが判明し、需要が爆発しました。\""
    },
    {
        "ID": "TRD-033",
        "Category": "2.動向",
        "Question": "AIが人間に近い外見や動作を持つようになると、ある時点で不快感を感じるようになる現象は？",
        "Opt1": "フレーム問題",
        "Opt2": "不気味な谷",
        "Opt3": "記号接地問題",
        "Opt4": "モラベックのパラドックス",
        "Opt5": "シンギュラリティ",
        "Opt6": "チューリングテスト",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】ロボット工学者の森政弘が提唱。親近感が上がっていく途中で、突然強い嫌悪感に変わる谷があるという理論です。\""
    },
    {
        "ID": "TRD-034",
        "Category": "2.動向",
        "Question": "AIが生成した画像や文章に、人間が作ったものではないことを示す電子的な情報を埋め込む技術は？",
        "Opt1": "ブロックチェーン",
        "Opt2": "電子署名",
        "Opt3": "電子透かし（ウォーターマーク）",
        "Opt4": "VPN",
        "Opt5": "暗号化",
        "Opt6": "ステガノグラフィー",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】偽情報対策や著作権保護のため、AI生成物へのラベル付け技術が注目されています。\""
    },
    {
        "ID": "TRD-035",
        "Category": "2.動向",
        "Question": "2017年に発表され、現在のChatGPTなどの基幹技術となったTransformerモデルを提案した論文のタイトルは？",
        "Opt1": "Deep Residual Learning",
        "Opt2": "Attention Is All You Need",
        "Opt3": "Generative Adversarial Nets",
        "Opt4": "ImageNet Classification",
        "Opt5": "Computing Machinery and Intelligence",
        "Opt6": "The Master Algorithm",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】Googleの研究者らによる論文。これにより自然言語処理が飛躍的に進化しました。\""
    },
    {
        "ID": "TRD-036",
        "Category": "2.動向",
        "Question": "日本において、AIのリスク管理や安全性を検証するために2024年に設置された機関は？",
        "Opt1": "IPA",
        "Opt2": "NEDO",
        "Opt3": "AIセーフティ・インスティテュート（AISI）",
        "Opt4": "デジタル庁",
        "Opt5": "産総研",
        "Opt6": "理化学研究所",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】米英のAISIと連携し、AIの安全性試験やガイドライン策定を行う専門機関です。\""
    },
    {
        "ID": "TRD-037",
        "Category": "2.動向",
        "Question": "「AIの冬の時代」と呼ばれる、研究予算が削減され期待が剥落した時期は、歴史上何度あったとされるか？",
        "Opt1": "0回",
        "Opt2": "1回",
        "Opt3": "2回",
        "Opt4": "3回",
        "Opt5": "5回",
        "Opt6": "毎年起きている",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】1970年代（第1次後）と1990年代（第2次後）の2回です。現在は第3次ブームが継続中です。\""
    },
    {
        "ID": "TRD-038",
        "Category": "2.動向",
        "Question": "AI分野における「AIの民主化」という言葉の意味として適切なものは？",
        "Opt1": "AIを政府が管理すること",
        "Opt2": "一部の富裕層だけがAIを使えるようにすること",
        "Opt3": "専門知識がなくても、誰もがAIの恩恵を受けたり開発に関わったりできるようにすること",
        "Opt4": "AIに選挙権を与えること",
        "Opt5": "AIの利用を有料化すること",
        "Opt6": "AI開発を禁止すること",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】クラウドサービスやオープンソース化により、AIの利用ハードルを下げることを指します。\""
    },
    {
        "ID": "TRD-039",
        "Category": "2.動向",
        "Question": "大規模言語モデルの「大規模」とは、一般的に何が膨大であることを指すか？",
        "Opt1": "メモリ消費量",
        "Opt2": "学習データのテキスト量とパラメータ数",
        "Opt3": "開発者の人数",
        "Opt4": "電気代",
        "Opt5": "サーバーの物理的な大きさ",
        "Opt6": "ユーザー数",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】数十億〜数千億ものパラメータ（重み）を持つモデルを指します。\""
    },
    {
        "ID": "TRD-040",
        "Category": "2.動向",
        "Question": "AIを悪用したサイバー攻撃（高度なフィッシングや自動ハッキング）への対策を何というか？",
        "Opt1": "AIセキュリティ",
        "Opt2": "AIフォー・グッド",
        "Opt3": "AIガバナンス",
        "Opt4": "AIレジリエンス",
        "Opt5": "AIセーフティ",
        "Opt6": "サイバーAI",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】AIを守る（安全性）だけでなく、AIを使った攻撃から守る視点も重要です。\""
    },
    {
        "ID": "TRD-041",
        "Category": "2.動向",
        "Question": "2015年にイーロン・マスク、サム・アルトマンらが設立した、AIをすべての人に開放することを目指した組織は？",
        "Opt1": "DeepMind",
        "Opt2": "OpenAI",
        "Opt3": "Anthropic",
        "Opt4": "Mistral AI",
        "Opt5": "Meta AI",
        "Opt6": "Hugging Face",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】当初は「AIの民主化」と「安全なAGI」を目指して設立されました。\""
    },
    {
        "ID": "TRD-042",
        "Category": "2.動向",
        "Question": "AIが自身のプログラムを自分で改良し続け、知能が爆発的に向上することを何というか？",
        "Opt1": "知能爆発",
        "Opt2": "フレーム問題",
        "Opt3": "記号接地問題",
        "Opt4": "バックプロパゲーション",
        "Opt5": "強化学習",
        "Opt6": "ドロップアウト",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】I.J.グッドが提唱。シンギュラリティに至る一つのプロセスとされています。\""
    },
    {
        "ID": "TRD-043",
        "Category": "2.動向",
        "Question": "AIの学習において、人間が正解データを手作業で作成する作業を何というか？",
        "Opt1": "プログラミング",
        "Opt2": "アノテーション",
        "Opt3": "デバッグ",
        "Opt4": "コンパイル",
        "Opt5": "スケーリング",
        "Opt6": "正規化",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】「この画像は犬です」といったラベルを付ける作業。第3次ブームの下支えをしています。\""
    },
    {
        "ID": "TRD-044",
        "Category": "2.動向",
        "Question": "AIの意思決定プロセスが人間には理解できないほど複雑であることを何というか？",
        "Opt1": "ホワイトボックス",
        "Opt2": "ブラックボックス",
        "Opt3": "サンドボックス",
        "Opt4": "ツールボックス",
        "Opt5": "パンドラの箱",
        "Opt6": "スカイネット",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】ディープラーニングの多層構造が、内部でどのように重みが調整されたか説明困難であることを指します。\""
    },
    {
        "ID": "TRD-045",
        "Category": "2.動向",
        "Question": "「特化型AI」の反対語で、人間のようにあらゆる知的作業を行えるAIを指す略称は？",
        "Opt1": "RNN",
        "Opt2": "CNN",
        "Opt3": "AGI (人工汎用知能)",
        "Opt4": "GAN",
        "Opt5": "LSTM",
        "Opt6": "NLP",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】Artificial General Intelligenceの略。まだ実現はしていません。\""
    },
    {
        "ID": "TRD-046",
        "Category": "2.動向",
        "Question": "米国のバイデン政権が2023年に署名した、安全で信頼できるAIの開発と利用に関する命令は？",
        "Opt1": "AI大統領令",
        "Opt2": "AI法案",
        "Opt3": "シリコンバレー協定",
        "Opt4": "ワシントン宣言",
        "Opt5": "デジタルトラスト令",
        "Opt6": "国家AI法",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】政府機関に対し、AIの安全性評価や報告を義務付ける強力な権限を与えました。\""
    },
    {
        "ID": "TRD-047",
        "Category": "2.動向",
        "Question": "特定の企業がモデルを秘匿する「クローズドなAI」に対し、ソースコードやモデルの重みを公開するAIを何というか？",
        "Opt1": "プロプライエタリAI",
        "Opt2": "オープンソースAI",
        "Opt3": "プライベートAI",
        "Opt4": "ダークAI",
        "Opt5": "シャドーAI",
        "Opt6": "ローカルAI",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】MetaのLlamaやMistral AIなどが代表例です。\""
    },
    {
        "ID": "TRD-048",
        "Category": "2.動向",
        "Question": "AIの開発者が、学習データの内容やモデルの性能、倫理的な制約などを明記した文書を何というか？",
        "Opt1": "ライセンス条項",
        "Opt2": "モデルカード",
        "Opt3": "プライバシーポリシー",
        "Opt4": "ホワイトペーパー",
        "Opt5": "仕様書",
        "Opt6": "利用規約",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】AIの透明性を高めるための「成分表示」のような役割を果たします。\""
    },
    {
        "ID": "TRD-049",
        "Category": "2.動向",
        "Question": "AI開発における「スケーリング則」とは何に関する法則か？",
        "Opt1": "AIのサイズを小さくすると精度が上がること",
        "Opt2": "計算量・データ量・パラメータ数を増やすほど精度が向上すること",
        "Opt3": "AIの開発コストが時間とともに下がること",
        "Opt4": "AIが人間を支配すること",
        "Opt5": "AIの消費電力が減ること",
        "Opt6": "AIの学習時間が短くなること",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】OpenAIの研究などで示された、物量投入が性能に直結するという現在のトレンドの根拠です。\""
    },
    {
        "ID": "TRD-050",
        "Category": "2.動向",
        "Question": "AIにおける「マルチモーダル」とはどのような意味か？",
        "Opt1": "一つの言語だけを扱うこと",
        "Opt2": "テキスト・画像・音声など複数の種類のデータを同時に扱えること",
        "Opt3": "一度に一つのタスクしかできないこと",
        "Opt4": "インターネットに接続されていないこと",
        "Opt5": "白黒画像しか認識できないこと",
        "Opt6": "感情を持っていないこと",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】複数の情報を統合して理解・生成できる最新のトレンドです。\""
    },
    {
        "ID": "TRD-051",
        "Category": "2.動向",
        "Question": "1943年に「形式ニューロン」という、神経細胞を単純な論理ゲートとしてモデル化した人物は？",
        "Opt1": "マカロックとピッツ",
        "Opt2": "ワトソンとクリック",
        "Opt3": "ジョブズとウォズニアック",
        "Opt4": "フォン・ノイマン",
        "Opt5": "チューリング",
        "Opt6": "ヒントン",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】ウォーレン・マカロックとウォルター・ピッツ。これがニューラルネットワークの数学的起源です。\""
    },
    {
        "ID": "TRD-052",
        "Category": "2.動向",
        "Question": "1958年に発表された、視覚の仕組みを模した単純なニューラルネットワークの名前は？",
        "Opt1": "パーセプトロン",
        "Opt2": "ネオコグニトロン",
        "Opt3": "アレックスネット",
        "Opt4": "バックプロパゲーション",
        "Opt5": "シグモイド関数",
        "Opt6": "サポートベクターマシン",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】フランク・ローゼンブラットが考案。単層パーセプトロンは線形分離可能な問題しか解けないという弱点がありました。\""
    },
    {
        "ID": "TRD-053",
        "Category": "2.動向",
        "Question": "1969年に著書『パーセプトロン』で単層パーセプトロンの限界を指摘し、第1次ブームを終わらせたのは？",
        "Opt1": "ジョン・マッカーシー",
        "Opt2": "ミンスキーとパパート",
        "Opt3": "ジェフリー・ヒントン",
        "Opt4": "アラン・チューリング",
        "Opt5": "スティーブ・ジョブズ",
        "Opt6": "ビル・ゲイツ",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】マービン・ミンスキーとシーモア・パパート。この数学的な批判により、NN研究は冬の時代に入りました。\""
    },
    {
        "ID": "TRD-054",
        "Category": "2.動向",
        "Question": "1980年代に第2次ブームの火付け役となった「多層パーセプトロン」の学習アルゴリズムは？",
        "Opt1": "勾配降下法",
        "Opt2": "誤差逆伝播法（バックプロパゲーション）",
        "Opt3": "k-近傍法",
        "Opt4": "主成分分析",
        "Opt5": "正則化",
        "Opt6": "ランダムフォレスト",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】ラメルハートらによって再発見・普及。多層化することでXOR問題を解決しました。\""
    },
    {
        "ID": "TRD-055",
        "Category": "2.動向",
        "Question": "1980年代に福島邦彦氏が開発した、視覚野の仕組みを模した「畳み込みニューラルネットワーク（CNN）」の原型は？",
        "Opt1": "パーセプトロン",
        "Opt2": "ネオコグニトロン",
        "Opt3": "アレックスネット",
        "Opt4": "自己組織化マップ",
        "Opt5": "ホップフィールドネットワーク",
        "Opt6": "ボルツマンマシン",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】ネオコグニトロン。位置ずれや変形に強い認識能力を持っていました。\""
    },
    {
        "ID": "TRD-056",
        "Category": "2.動向",
        "Question": "2012年のILSVRCで、ドロップアウトやReLU関数を導入して優勝したモデルの名前は？",
        "Opt1": "VGGNet",
        "Opt2": "GoogLeNet",
        "Opt3": "AlexNet",
        "Opt4": "ResNet",
        "Opt5": "LeNet",
        "Opt6": "MobileNet",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】アレックス・クリジェフスキーらが開発。第3次ブームの象徴的モデルです。\""
    },
    {
        "ID": "TRD-057",
        "Category": "2.動向",
        "Question": "ディープラーニングにおいて層を深くしすぎると勾配が消えてしまう問題を回避した、2015年優勝のモデルは？",
        "Opt1": "VGGNet",
        "Opt2": "GoogLeNet",
        "Opt3": "AlexNet",
        "Opt4": "ResNet",
        "Opt5": "LeNet",
        "Opt6": "Inception",
        "Answer_Idx": 3,
        "Explanation": "\"【解説】Residual Network。残差接続（スキップ結合）を導入し、152層という驚異的な深さを実現しました。\""
    },
    {
        "ID": "TRD-058",
        "Category": "2.動向",
        "Question": "AIが自身で学習するための「正解データ」を自動生成したり、自ら環境と対話して学ぶ手法を何というか？",
        "Opt1": "教師あり学習",
        "Opt2": "教師なし学習",
        "Opt3": "強化学習",
        "Opt4": "転移学習",
        "Opt5": "能動学習",
        "Opt6": "半教師あり学習",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】AlphaGoなどが採用。報酬を最大化するように試行錯誤します。\""
    },
    {
        "ID": "TRD-059",
        "Category": "2.動向",
        "Question": "2023年に開催された「AI安全サミット」において、28カ国とEUが署名した宣言は？",
        "Opt1": "広島宣言",
        "Opt2": "ブレッチリー宣言",
        "Opt3": "京都AI宣言",
        "Opt4": "シリコンバレー声明",
        "Opt5": "パリ協定",
        "Opt6": "ジュネーブ条約",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】英国で開催。最先端AIのリスクに対して国際的に協力することを合意しました。\""
    },
    {
        "ID": "TRD-060",
        "Category": "2.動向",
        "Question": "「説明可能なAI（XAI）」の手法で、画像のどの部分を見てAIが判断したかを可視化する代表的手法は？",
        "Opt1": "LIME",
        "Opt2": "Grad-CAM",
        "Opt3": "SHAP",
        "Opt4": "Attention Map",
        "Opt5": "ResNet",
        "Opt6": "Softmax",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】特に画像認識において、判断根拠をヒートマップで示す手法です。\""
    },
    {
        "ID": "TRD-061",
        "Category": "2.動向",
        "Question": "AIの学習データから特定の個人の情報を「消去する」技術を何というか？",
        "Opt1": "データクリーニング",
        "Opt2": "マシン・アンラーニング",
        "Opt3": "量子化",
        "Opt4": "蒸留",
        "Opt5": "アノテーション",
        "Opt6": "正則化",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】プライバシー保護や「忘れられる権利」への対応として研究されています。\""
    },
    {
        "ID": "TRD-062",
        "Category": "2.動向",
        "Question": "AIのハルシネーションを防ぐため、外部の信頼できる情報源を検索して回答させる手法は？",
        "Opt1": "Fine-tuning",
        "Opt2": "RAG（検索拡張生成）",
        "Opt3": "Prompt Engineering",
        "Opt4": "Reinforcement Learning",
        "Opt5": "Pre-training",
        "Opt6": "Distillation",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】Retrieval-Augmented Generation。正確性を高めるための必須技術です。\""
    },
    {
        "ID": "TRD-063",
        "Category": "2.動向",
        "Question": "大規模言語モデルの学習にAIが生成したデータばかりを使っていると、モデルが劣化する現象を何というか？",
        "Opt1": "オーバーフィッティング",
        "Opt2": "モデル崩壊",
        "Opt3": "勾配消失",
        "Opt4": "ハルシネーション",
        "Opt5": "モード崩壊",
        "Opt6": "過学習",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】Model Collapse。インターネット上がAI生成物で溢れることへの懸念です。\""
    },
    {
        "ID": "TRD-064",
        "Category": "2.動向",
        "Question": "AIが作成したフェイクニュースが拡散されやすい原因として、最も不適切なものは？",
        "Opt1": "AIの文章が完璧だから",
        "Opt2": "SNSのアルゴリズムが関心を引く情報を優先するから",
        "Opt3": "人間がAIを信じ込んでいるから",
        "Opt4": "インターネットが高速だから",
        "Opt5": "政府が推奨しているから",
        "Opt6": "偽情報の方が文字数が多いから",
        "Answer_Idx": 4,
        "Explanation": "\"【解説】政府が偽情報を推奨することはありません。SNSの構造的欠陥が主な原因です。\""
    },
    {
        "ID": "TRD-065",
        "Category": "2.動向",
        "Question": "AIの判定結果に対して、人間が異議申し立てをしたり最終的な判断を下したりする権利の概念を何というか？",
        "Opt1": "AIの権利",
        "Opt2": "人間による介在 (Human-in-the-loop)",
        "Opt3": "自動決定権",
        "Opt4": "データポータビリティ権",
        "Opt5": "忘れられる権利",
        "Opt6": "知る権利",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】AIにすべてを任せず、人間が管理のループに入り続けることの重要性です。\""
    },
    {
        "ID": "TRD-066",
        "Category": "2.動向",
        "Question": "AIが生成した絵画がコンテストで優勝した際、著作権の論点となるのは？",
        "Opt1": "AI自体に人格を認めるべきか",
        "Opt2": "人間がどれだけ創作的に関与（プロンプト等）したか",
        "Opt3": "キャンバスの価格",
        "Opt4": "絵の具の質",
        "Opt5": "コンテストの賞金額",
        "Opt6": "AIの計算速度",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】創作的寄与がなければ、AI生成物は著作物と認められないのが一般的です。\""
    },
    {
        "ID": "TRD-067",
        "Category": "2.動向",
        "Question": "AIの学習データにノイズを加え、個人の特定を困難にしつつ統計的な性質を維持する技術は？",
        "Opt1": "データオーグメンテーション",
        "Opt2": "ディファレンシャル・プライバシー（差分プライバシー）",
        "Opt3": "転移学習",
        "Opt4": "能動学習",
        "Opt5": "蒸留",
        "Opt6": "量子化",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】プライバシー保護のための高度な数学的手法です。\""
    },
    {
        "ID": "TRD-068",
        "Category": "2.動向",
        "Question": "特定の企業がモデルを秘匿する「クローズドなAI」の代表例は？",
        "Opt1": "Llama (Meta)",
        "Opt2": "Mistral AI",
        "Opt3": "GPT-4 (OpenAI)",
        "Opt4": "Stable Diffusion",
        "Opt5": "Falcon",
        "Opt6": "BERT",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】GPT-4などは中身が非公開のプロプライエタリ（独占的）なモデルです。\""
    },
    {
        "ID": "TRD-069",
        "Category": "2.動向",
        "Question": "AIを「人の代替」としてではなく、人の能力を拡張するものとして捉える考え方は？",
        "Opt1": "AI（Artificial Intelligence）",
        "Opt2": "IA（Intelligence Augmentation / 知能増幅）",
        "Opt3": "AGI",
        "Opt4": "Singularity",
        "Opt5": "Automation",
        "Opt6": "Digitization",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】人間とAIが協力し、人間の可能性を広げるという視点です。\""
    },
    {
        "ID": "TRD-070",
        "Category": "2.動向",
        "Question": "AIにおける「責任あるAI（Responsible AI）」の四要素に含まれないものはどれか？",
        "Opt1": "公平性",
        "Opt2": "透明性",
        "Opt3": "説明可能性",
        "Opt4": "利益の最大化",
        "Opt5": "信頼性",
        "Opt6": "プライバシー保護",
        "Answer_Idx": 3,
        "Explanation": "\"【解説】企業の利益よりも、社会的責任や安全性が優先されます。\""
    },
    {
        "ID": "ML-001",
        "Category": "3.機械学習手法",
        "Question": "線形回帰において、予測値と実測値の差の二乗和を最小にするようにパラメータを決定する手法は？",
        "Opt1": "最尤推定法",
        "Opt2": "最小二乗法",
        "Opt3": "勾配降下法",
        "Opt4": "正則化法",
        "Opt5": "主成分分析",
        "Opt6": "k-近傍法",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】最小二乗法（Ordinary Least Squares）は、残差の二乗和を最小化する最も基本的な手法です。[図解：散布図の中に引かれた直線と、各点から直線への垂直な線の二乗和イメージ]\""
    },
    {
        "ID": "ML-002",
        "Category": "3.機械学習手法",
        "Question": "回帰問題において、モデルの複雑さを抑えるために重みの絶対値を損失関数に加える「L1正則化」を用いた手法は？",
        "Opt1": "リッジ回帰（Ridge）",
        "Opt2": "ラッソ回帰（Lasso）",
        "Opt3": "エラスティックネット",
        "Opt4": "ロジスティック回帰",
        "Opt5": "多項式回帰",
        "Opt6": "ステップワイズ法",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】L1＝ラッソ(Lasso)です。一部の重みを完全に0にする特徴があり、変数選択に利用されます。L2＝リッジ(Ridge)との入れ替えひっかけが頻出です。\""
    },
    {
        "ID": "ML-003",
        "Category": "3.機械学習手法",
        "Question": "ロジスティック回帰の説明として正しいものはどれか？",
        "Opt1": "連続値を予測するための回帰モデルである",
        "Opt2": "出力にシグモイド関数を用い、あるクラスに属する確率を予測するモデルである",
        "Opt3": "教師なし学習の一種である",
        "Opt4": "決定木を多数組み合わせたモデルである",
        "Opt5": "データの次元を圧縮するためのモデルである",
        "Opt6": "距離に基づいてクラスタリングを行うモデルである",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】「回帰」という名前ですが、実際には「分類」問題に使われます。出力は0から1の範囲に収まる確率となります。\""
    },
    {
        "ID": "ML-004",
        "Category": "3.機械学習手法",
        "Question": "サポートベクターマシン（SVM）において、データが線形分離できない場合に高次元空間に写像して境界を引く技術は？",
        "Opt1": "マージン最大化",
        "Opt2": "ソフトマージン",
        "Opt3": "カーネルトリック",
        "Opt4": "サポートベクトル",
        "Opt5": "正則化",
        "Opt6": "標準化",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】カーネルトリックにより、複雑な境界を効率的に計算できます。高次元へ飛ばして「スパッ」と切るイメージです。[図解：2次元で混ざった点が3次元空間で平面で分けられる様子]\""
    },
    {
        "ID": "ML-005",
        "Category": "3.機械学習手法",
        "Question": "SVMにおいて、境界線（決定境界）から最も近いデータ点との距離を最大化することを何というか？",
        "Opt1": "カーネル化",
        "Opt2": "次元の呪い",
        "Opt3": "マージン最大化",
        "Opt4": "正則化",
        "Opt5": "スラック変数",
        "Opt6": "特異値分解",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】この距離（マージン）を最大化することで、未知のデータに対する汎化性能を高めます。この時の「最も近い点」がサポートベクトルです。\""
    },
    {
        "ID": "ML-006",
        "Category": "3.機械学習手法",
        "Question": "決定木において、分割の良さを測る指標として「不純度」が用いられるが、代表的な指標を2つ選ぶとしたらどれか？",
        "Opt1": "ジニ係数 / エントロピー",
        "Opt2": "正解率 / 適合率",
        "Opt3": "平均二乗誤差 / MAE",
        "Opt4": "相関係数 / 決定係数",
        "Opt5": "累積寄与率 / 固有値",
        "Opt6": "シルエット係数 / エルボー",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】ジニ係数（Gini Impurity）やエントロピーが低い（＝純度が高い）状態を目指してデータを分割していきます。\""
    },
    {
        "ID": "ML-007",
        "Category": "3.機械学習手法",
        "Question": "決定木が学習データに過剰に適合（過学習）するのを防ぐために、不要な枝を取り除く操作は？",
        "Opt1": "スケーリング",
        "Opt2": "剪定（プルーニング）",
        "Opt3": "バギング",
        "Opt4": "ブースティング",
        "Opt5": "白色化",
        "Opt6": "正則化",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】剪定には、作りながら止める「事前剪定」と、作った後に削る「事後剪定」があります。[図解：複雑な木から、末端の細かい枝がカットされる様子]\""
    },
    {
        "ID": "ML-008",
        "Category": "3.機械学習手法",
        "Question": "複数のモデルを組み合わせて、個々のモデルよりも高い精度を得る手法の総称は？",
        "Opt1": "転移学習",
        "Opt2": "能動学習",
        "Opt3": "アンザンブル学習",
        "Opt4": "強化学習",
        "Opt5": "半教師あり学習",
        "Opt6": "自己教師あり学習",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】「3人寄れば文殊の知恵」の考え方です。バギング、ブースティング、スタッキングが3大手法です。\""
    },
    {
        "ID": "ML-009",
        "Category": "3.機械学習手法",
        "Question": "学習データから重複を許してランダムにサンプルを抽出し、複数のモデルを並列に学習させる手法は？",
        "Opt1": "バギング（Bagging）",
        "Opt2": "ブースティング（Boosting）",
        "Opt3": "スタッキング（Stacking）",
        "Opt4": "ワンショット学習",
        "Opt5": "蒸留",
        "Opt6": "量子化",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】バギングは「Bootstrap Aggregating」の略。代表例はランダムフォレストです。\""
    },
    {
        "ID": "ML-010",
        "Category": "3.機械学習手法",
        "Question": "ランダムフォレストにおいて、個々の決定木が学習する際に使用する「特徴量」の選び方は？",
        "Opt1": "すべての特徴量をすべての木で使う",
        "Opt2": "人間が選んだ重要な特徴量だけを使う",
        "Opt3": "各分割ごとにランダムに一部の特徴量を選び出す",
        "Opt4": "相関係数が高い順に使う",
        "Opt5": "次元圧縮した後の主成分を使う",
        "Opt6": "前の木が失敗した特徴量を優先的に使う",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】「データのランダム抽出」と「特徴量のランダム選択」の2重のランダム性により、多様な木を作り、過学習を防ぎます。\""
    },
    {
        "ID": "ML-011",
        "Category": "3.機械学習手法",
        "Question": "前のモデルの「予測ミス」を重視して、逐次的にモデルを学習させていくアンサンブル手法は？",
        "Opt1": "バギング",
        "Opt2": "ブースティング",
        "Opt3": "スタッキング",
        "Opt4": "ホールドアウト",
        "Opt5": "クロスバリデーション",
        "Opt6": "グリッドサーチ",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】ブースティングは「直列」に学習します。代表例はAdaBoostやXGBoost、LightGBMです。[図解：弱学習器1→ミスに重み付け→弱学習器2...と続く流れ]\""
    },
    {
        "ID": "ML-012",
        "Category": "3.機械学習手法",
        "Question": "未知のデータに対して「距離が近い順にk個のデータ」を選び、多数決でクラスを決定する手法は？",
        "Opt1": "k-means法",
        "Opt2": "k-近傍法（k-NN）",
        "Opt3": "決定木",
        "Opt4": "ロジスティック回帰",
        "Opt5": "ナイーブベイズ",
        "Opt6": "階層的クラスタリング",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】k-NN（Nearest Neighbor）。k-means（クラスタリング/教師なし）との名称ひっかけが非常に多いです。\""
    },
    {
        "ID": "ML-013",
        "Category": "3.機械学習手法",
        "Question": "教師なし学習において、データをあらかじめ決められた数（k個）のグループに分ける代表的な手法は？",
        "Opt1": "k-近傍法",
        "Opt2": "k-means法",
        "Opt3": "主成分分析",
        "Opt4": "自己組織化マップ",
        "Opt5": "アソシエーション分析",
        "Opt6": "t-SNE",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】k-means（k平均法）です。重心（セントロイド）を計算し、割り当てと更新を繰り返します。[図解：点の集まりの中にk個の重心が移動していくステップ]\""
    },
    {
        "ID": "ML-014",
        "Category": "3.機械学習手法",
        "Question": "k-means法において、適切なクラスタ数kを決定するために、クラスタ数ごとの「重心からの距離の合計」の変化を確認する手法は？",
        "Opt1": "シルエット法",
        "Opt2": "エルボー法",
        "Opt3": "クロスバリデーション",
        "Opt4": "カイ二乗検定",
        "Opt5": "ホールドアウト法",
        "Opt6": "グリッドサーチ",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】グラフが「肘（Elbow）」のように折れ曲がる箇所を最適なkとする手法です。\""
    },
    {
        "ID": "ML-015",
        "Category": "3.機械学習手法",
        "Question": "多次元のデータを、情報の損失を最小限に抑えつつ低次元（2〜3次元など）に圧縮する教師なし学習は？",
        "Opt1": "ロジスティック回帰",
        "Opt2": "主成分分析（PCA）",
        "Opt3": "線形判別分析",
        "Opt4": "サポートベクターマシン",
        "Opt5": "決定木",
        "Opt6": "ランダムフォレスト",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】PCA。分散が最大になる方向（第1主成分）を探します。データの可視化や前処理によく使われます。\""
    },
    {
        "ID": "ML-016",
        "Category": "3.機械学習手法",
        "Question": "主成分分析において、元のデータが持っていた情報（分散）を各主成分がどれくらい説明しているかを示す指標は？",
        "Opt1": "正解率",
        "Opt2": "適合率",
        "Opt3": "寄与率",
        "Opt4": "相関係数",
        "Opt5": "決定係数",
        "Opt6": "ジニ係数",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】各主成分の寄与率を足していったものを「累積寄与率」と呼び、次元をどこまで削るかの判断基準にします。\""
    },
    {
        "ID": "ML-017",
        "Category": "3.機械学習手法",
        "Question": "「ビールを買う人は紙おむつも買う」といった、データ間の相関ルールを見つけ出す教師なし学習手法は？",
        "Opt1": "クラスタリング",
        "Opt2": "アソシエーション分析（相関分析）",
        "Opt3": "次元圧縮",
        "Opt4": "回帰分析",
        "Opt5": "時系列分析",
        "Opt6": "強化学習",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】支持度（Support）、確信度（Confidence）、リフト値（Lift）という3つの指標を用いてルールを評価します。\""
    },
    {
        "ID": "ML-018",
        "Category": "3.機械学習手法",
        "Question": "混同行列において、実際に「正」であるデータのうち、正しく「正」と予測できた割合を何というか？",
        "Opt1": "適合率（Precision）",
        "Opt2": "再現率（Recall / 真陽性率）",
        "Opt3": "正解率（Accuracy）",
        "Opt4": "特異度（Specificity）",
        "Opt5": "F値",
        "Opt6": "負点的中率",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】再現率＝TP / (TP + FN)。「見逃しの少なさ」を重視する場合（病気の診断など）に重要です。\""
    },
    {
        "ID": "ML-019",
        "Category": "3.機械学習手法",
        "Question": "混同行列において、「正」と予測したデータのうち、実際に「正」であった割合を何というか？",
        "Opt1": "適合率（Precision）",
        "Opt2": "再現率（Recall）",
        "Opt3": "正解率",
        "Opt4": "F値",
        "Opt5": "AUC",
        "Opt6": "IOU",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】適合率＝TP / (TP + FP)。「間違いの少なさ（誤検知の低さ）」を重視する場合（スパム判定など）に重要です。\""
    },
    {
        "ID": "ML-020",
        "Category": "3.機械学習手法",
        "Question": "適合率と再現率がトレードオフの関係にある際、その2つの調和平均をとった指標を何というか？",
        "Opt1": "正解率",
        "Opt2": "F値（F-measure）",
        "Opt3": "決定係数",
        "Opt4": "MAE",
        "Opt5": "RMSE",
        "Opt6": "Log-Loss",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】F値 = 2 / (1/適合率 + 1/再現率)。両方のバランスを評価したい時に使います。\""
    },
    {
        "ID": "ML-021",
        "Category": "3.機械学習手法",
        "Question": "不均衡データ（陽性が極端に少ないデータなど）において、モデルの性能評価として「不適切な」指標はどれか？",
        "Opt1": "適合率",
        "Opt2": "再現率",
        "Opt3": "正解率",
        "Opt4": "F値",
        "Opt5": "AUC",
        "Opt6": "PR曲線",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】例えば99%が陰性のデータなら、常に「陰性」と答えるだけで正解率99%になってしまうため、正解率は不適切な指標となります。\""
    },
    {
        "ID": "ML-022",
        "Category": "3.機械学習手法",
        "Question": "分類モデルの閾値を変化させた時の「真陽性率」と「偽陽性率」の軌跡を描いた曲線を何というか？",
        "Opt1": "PR曲線",
        "Opt2": "学習曲線",
        "Opt3": "ROC曲線",
        "Opt4": "ベル曲線",
        "Opt5": "シグモイド曲線",
        "Opt6": "ソフトマックス曲線",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】Receiver Operating Characteristic曲線。この曲線の下側の面積をAUCと呼び、1に近いほど高性能です。[図解：縦軸(真陽性率)・横軸(偽陽性率)の右肩上がりの曲線]\""
    },
    {
        "ID": "ML-023",
        "Category": "3.機械学習手法",
        "Question": "データをk個に分割し、そのうち1つをテスト用、残りを学習用として、k回入れ替えて評価する手法は？",
        "Opt1": "ホールドアウト法",
        "Opt2": "k-分割交差検証（クロスバリデーション）",
        "Opt3": "Leave-one-out法",
        "Opt4": "ブートストラップ法",
        "Opt5": "グリッドサーチ",
        "Opt6": "ランダムサーチ",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】k-fold Cross Validation。データの偏りによる評価のブレを抑えることができます。\""
    },
    {
        "ID": "ML-024",
        "Category": "3.機械学習手法",
        "Question": "ハイパーパラメータの組み合わせを格子状にすべて試して、最適なものを見つける手法は？",
        "Opt1": "ランダムサーチ",
        "Opt2": "グリッドサーチ",
        "Opt3": "ベイズ最適化",
        "Opt4": "早期終了",
        "Opt5": "ドロップアウト",
        "Opt6": "正則化",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】網羅的に探すため確実ですが、パラメータ数が増えると計算時間が膨大になります。\""
    },
    {
        "ID": "ML-025",
        "Category": "3.機械学習手法",
        "Question": "強化学習において、エージェントが行動を選択した結果、環境から得られる値を何というか？",
        "Opt1": "状態",
        "Opt2": "報酬",
        "Opt3": "方策",
        "Opt4": "価値",
        "Opt5": "Q値",
        "Opt6": "エピソード",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】エージェントはこの「報酬（Reward）」の累積を最大化するように学習します。\""
    },
    {
        "ID": "ML-026",
        "Category": "3.機械学習手法",
        "Question": "強化学習において、「現在の状態においてどの行動をとるべきか」を決定するためのルール（戦略）を何というか？",
        "Opt1": "環境",
        "Opt2": "エージェント",
        "Opt3": "方策（ポリシー）",
        "Opt4": "価値関数",
        "Opt5": "遷移確率",
        "Opt6": "割引率",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】Policy。確率的に決めるものや、決定的に決めるものがあります。\""
    },
    {
        "ID": "ML-027",
        "Category": "3.機械学習手法",
        "Question": "将来得られる報酬を、現在の価値に換算するために用いる「0から1の間」のパラメータを何というか？",
        "Opt1": "学習率",
        "Opt2": "割引率",
        "Opt3": "探索率",
        "Opt4": "正則化係数",
        "Opt5": "モメンタム",
        "Opt6": "バイアス",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】割引率（γ：ガンマ）。「将来の100円」より「今の100円」をどれだけ重視するかを制御します。\""
    },
    {
        "ID": "ML-028",
        "Category": "3.機械学習手法",
        "Question": "強化学習において、既知の情報を利用するだけでなく、新しい行動を試して情報を集めることを何というか？",
        "Opt1": "活用（Exploitation）",
        "Opt2": "探索（Exploration）",
        "Opt3": "遷移（Transition）",
        "Opt4": "更新（Update）",
        "Opt5": "初期化（Initialization）",
        "Opt6": "収束（Convergence）",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】「探索と活用のトレードオフ」と呼ばれます。ずっと同じ店（活用）か、新しい店（探索）か、という問題です。\""
    },
    {
        "ID": "ML-029",
        "Category": "3.機械学習手法",
        "Question": "強化学習の代表的な手法で、ある状態で特定の行動をとった時の価値（Q値）をテーブル形式で更新していく手法は？",
        "Opt1": "Q学習（Q-Learning）",
        "Opt2": "モンテカルロ法",
        "Opt3": "SARSA",
        "Opt4": "方策勾配法",
        "Opt5": "DQN",
        "Opt6": "A3C",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】もっとも基本的な強化学習アルゴリズムの一つです。\""
    },
    {
        "ID": "ML-030",
        "Category": "3.機械学習手法",
        "Question": "クラスタリング手法において、個々のデータから始めて、似たもの同士を順に結合していく手法は？",
        "Opt1": "k-means法",
        "Opt2": "非階層的クラスタリング",
        "Opt3": "階層的クラスタリング",
        "Opt4": "主成分分析",
        "Opt5": "t-SNE",
        "Opt6": "DBSCAN",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】最終的に「デンドログラム（樹状図）」が作成されます。[図解：家系図のようなトーナメント形式の図]\""
    },
    {
        "ID": "ML-031",
        "Category": "3.機械学習手法",
        "Question": "階層的クラスタリングにおいて、クラスタ間の距離を定義する手法のうち、クラスタ内の分散の増加を最小にする手法は？",
        "Opt1": "最短距離法",
        "Opt2": "最長距離法",
        "Opt3": "群平均法",
        "Opt4": "ウォード法",
        "Opt5": "重心法",
        "Opt6": "メディアン法",
        "Answer_Idx": 3,
        "Explanation": "\"【解説】ウォード法（Ward's method）。計算量は多いですが、分類感度が高く、よく使われる手法です。\""
    },
    {
        "ID": "ML-032",
        "Category": "3.機械学習手法",
        "Question": "ベイズの定理に基づき、特徴量が互いに独立であると仮定して分類を行う単純なモデルは？",
        "Opt1": "ロジスティック回帰",
        "Opt2": "ナイーブベイズ（単純ベイズ）",
        "Opt3": "SVM",
        "Opt4": "決定木",
        "Opt5": "線形判別分析",
        "Opt6": "ニューラルネットワーク",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】「独立」という強い仮定を置くため、計算が非常に高速で、テキスト分類（スパム判定など）で強力です。\""
    },
    {
        "ID": "ML-033",
        "Category": "3.機械学習手法",
        "Question": "モデルの複雑さを評価する指標で、「損失（誤差）の小ささ」と「パラメータ数（複雑さ）」のバランスをとる指標は？",
        "Opt1": "平均二乗誤差",
        "Opt2": "決定係数",
        "Opt3": "AIC（赤池情報量基準）",
        "Opt4": "ジニ係数",
        "Opt5": "F値",
        "Opt6": "AUC",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】AIC = -2ln(L) + 2k。値が小さいほど「シンプルで良いモデル」と判断されます。\""
    },
    {
        "ID": "ML-034",
        "Category": "3.機械学習手法",
        "Question": "回帰モデルにおいて、予測がどれだけ実測値にフィットしているかを示す「0から1」の指標は？",
        "Opt1": "相関係数",
        "Opt2": "決定係数（R-squared）",
        "Opt3": "標準偏差",
        "Opt4": "分散",
        "Opt5": "MAE",
        "Opt6": "RMSE",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】1に近いほどモデルがデータをよく説明できていることを示します。\""
    },
    {
        "ID": "ML-035",
        "Category": "3.機械学習手法",
        "Question": "L1正則化とL2正則化を組み合わせて使用する回帰手法を何というか？",
        "Opt1": "ラッソ回帰",
        "Opt2": "リッジ回帰",
        "Opt3": "エラスティックネット（Elastic Net）",
        "Opt4": "多項式回帰",
        "Opt5": "分位点回帰",
        "Opt6": "ロジスティック回帰",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】両方のいいとこ取りをした手法です。\""
    },
    {
        "ID": "ML-036",
        "Category": "3.機械学習手法",
        "Question": "正則化項の強さを制御するハイパーパラメータ（λやα）の値を大きくすると、モデルはどう変化するか？",
        "Opt1": "より複雑になり、過学習しやすくなる",
        "Opt2": "よりシンプルになり、過学習が抑えられる",
        "Opt3": "学習速度が速くなる",
        "Opt4": "予測精度が必ず向上する",
        "Opt5": "パラメータの数が無限に増える",
        "Opt6": "バイアスが0になる",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】ペナルティが強くなるため、重みが小さくなり、モデルは滑らか（シンプル）になります。\""
    },
    {
        "ID": "ML-037",
        "Category": "3.機械学習手法",
        "Question": "SVMにおいて、一部のデータの誤分類を許容することで、汎化性能を高める考え方を何というか？",
        "Opt1": "ハードマージン",
        "Opt2": "ソフトマージン",
        "Opt3": "カーネルトリック",
        "Opt4": "次元の呪い",
        "Opt5": "オーバーフィッティング",
        "Opt6": "スラック変数",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】「多少のミスは目をつむって、綺麗な境界を引こう」という考え方。この許容範囲をスラック変数で調整します。\""
    },
    {
        "ID": "ML-038",
        "Category": "3.機械学習手法",
        "Question": "モデルが学習データに対して十分に学習できておらず、訓練エラーもテストエラーも高い状態を何というか？",
        "Opt1": "過学習（Overfitting）",
        "Opt2": "未学習（Underfitting）",
        "Opt3": "最適化",
        "Opt4": "収束",
        "Opt5": "汎化",
        "Opt6": "正規化",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】モデルが単純すぎる、または学習不足な状態です。\""
    },
    {
        "ID": "ML-039",
        "Category": "3.機械学習手法",
        "Question": "「特徴量の数が多すぎると、学習に必要なデータ量が指数関数的に増大する」という現象を何というか？",
        "Opt1": "ムーアの法則",
        "Opt2": "次元の呪い",
        "Opt3": "中心極限定理",
        "Opt4": "大数の法則",
        "Opt5": "ノーフリーランチ定理",
        "Opt6": "アムダールの法則",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】リチャード・ベルマンが提唱。次元が増えると、空間上の「スカスカ」な部分が増えて学習が困難になります。\""
    },
    {
        "ID": "ML-040",
        "Category": "3.機械学習手法",
        "Question": "データを学習用とテスト用に「一度だけ」分割して評価する、最もシンプルな手法は？",
        "Opt1": "k-分割交差検証",
        "Opt2": "ホールドアウト法",
        "Opt3": "留め置き法",
        "Opt4": "ブートストラップ法",
        "Opt5": "ジャックナイフ法",
        "Opt6": "アンサンブル法",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】計算コストは低いですが、分割の仕方によって結果が大きく変わるリスクがあります。\""
    },
    {
        "ID": "ML-041",
        "Category": "3.機械学習手法",
        "Question": "ランダムフォレストにおいて、各決定木を作成する際に用いられる、重複を許したランダムなサンプリング手法は？",
        "Opt1": "層化抽出",
        "Opt2": "系統抽出",
        "Opt3": "ブートストラップサンプリング",
        "Opt4": "クラスタサンプリング",
        "Opt5": "有意抽出",
        "Opt6": "全数調査",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】これにより、各決定木に異なるデータを見せて多様性を持たせます。\""
    },
    {
        "ID": "ML-042",
        "Category": "3.機械学習手法",
        "Question": "機械学習の前処理において、平均を0、分散を1にするようにデータを変換する操作を何というか？",
        "Opt1": "正規化（Normalization）",
        "Opt2": "標準化（Standardization）",
        "Opt3": "二値化",
        "Opt4": "白色化",
        "Opt5": "主成分分析",
        "Opt6": "平滑化",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】(x - 平均) / 標準偏差。SVMやPCAなど、距離を扱う手法では必須の処理です。\""
    },
    {
        "ID": "ML-043",
        "Category": "3.機械学習手法",
        "Question": "機械学習の前処理において、最小値を0、最大値を1にするようにデータを変換する操作を何というか？",
        "Opt1": "正規化（Normalization）",
        "Opt2": "標準化（Standardization）",
        "Opt3": "正則化",
        "Opt4": "離散化",
        "Opt5": "ダミー変数化",
        "Opt6": "対数変換",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】(x - min) / (max - min)。画像データの画素値（0-255）を0-1にする際などによく使われます。\""
    },
    {
        "ID": "ML-044",
        "Category": "3.機械学習手法",
        "Question": "カテゴリ変数（「赤」「青」「緑」など）を、0と1だけの複数の列に変換する処理を何というか？",
        "Opt1": "ラベルエンコーディング",
        "Opt2": "ワンホットエンコーディング（ダミー変数化）",
        "Opt3": "主成分分析",
        "Opt4": "正規化",
        "Opt5": "標準化",
        "Opt6": "次元削減",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】コンピュータが計算できるように、カテゴリを数値ベクトルに変換します。\""
    },
    {
        "ID": "ML-045",
        "Category": "3.機械学習手法",
        "Question": "強化学習において、エージェントが行動を決定するための「期待される報酬の合計」を表す関数は？",
        "Opt1": "損失関数",
        "Opt2": "活性化関数",
        "Opt3": "価値関数",
        "Opt4": "目的関数",
        "Opt5": "シグモイド関数",
        "Opt6": "恒等関数",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】状態価値関数（V）と行動価値関数（Q）があります。\""
    },
    {
        "ID": "ML-046",
        "Category": "3.機械学習手法",
        "Question": "回帰問題の評価指標で、予測値と実測値の差の「絶対値」の平均をとるものは？",
        "Opt1": "MSE（平均二乗誤差）",
        "Opt2": "MAE（平均絶対誤差）",
        "Opt3": "RMSE（平方根平均二乗誤差）",
        "Opt4": "決定係数",
        "Opt5": "ロジスティック損失",
        "Opt6": "ヒンジ損失",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】Mean Absolute Error。外れ値の影響を受けにくい評価指標です。\""
    },
    {
        "ID": "ML-047",
        "Category": "3.機械学習手法",
        "Question": "回帰問題の評価指標で、MSEの平方根をとることで単位を元のデータに合わせたものは？",
        "Opt1": "MAE",
        "Opt2": "MAPE",
        "Opt3": "RMSE",
        "Opt4": "AIC",
        "Opt5": "BIC",
        "Opt6": "F1-score",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】Root Mean Squared Error。誤差の大きさを直感的に理解しやすい指標です。\""
    },
    {
        "ID": "ML-048",
        "Category": "3.機械学習手法",
        "Question": "「どのような問題に対しても、常に他の手法より優れた性能を示すアルゴリズムは存在しない」という定理は？",
        "Opt1": "ピタゴラスの定理",
        "Opt2": "不完全性定理",
        "Opt3": "ノーフリーランチ定理",
        "Opt4": "中心極限定理",
        "Opt5": "ベイズの定理",
        "Opt6": "ケリーの公式",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】「タダ飯はない」。特定の問題には特定のアルゴリズムが合うので、色々試す必要があるという示唆です。\""
    },
    {
        "ID": "ML-049",
        "Category": "3.機械学習手法",
        "Question": "モデルの汎化性能を「バイアス」と「バリアンス」に分解した際、モデルを複雑にするほどどう変化するか？",
        "Opt1": "バイアスが増え、バリアンスが減る",
        "Opt2": "バイアスが減り、バリアンスが増える",
        "Opt3": "両方増える",
        "Opt4": "両方減る",
        "Opt5": "どちらも変化しない",
        "Opt6": "バイアスだけが0になる",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】複雑なモデル＝柔軟なので「偏り（バイアス）」は減りますが、データのノイズまで拾うので「変動（バリアンス）」が増えます。\""
    },
    {
        "ID": "ML-050",
        "Category": "3.機械学習手法",
        "Question": "時系列データの予測において、過去の自分自身の値を用いて回帰を行うモデルは？",
        "Opt1": "自己回帰モデル（ARモデル）",
        "Opt2": "移動平均モデル（MAモデル）",
        "Opt3": "SVM",
        "Opt4": "決定木",
        "Opt5": "k-means法",
        "Opt6": "ロジスティック回帰",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】AutoRegressiveモデル。過去の数値を説明変数として未来を予測します。\""
    },
    {
        "ID": "\"ML-051\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"学習曲線において、訓練データの誤差とテストデータの誤差がどちらも高い位置で収束している状態を何というか？\"",
        "Opt1": "\"高バイアス（未学習）\"",
        "Opt2": "\"高バリアンス（過学習）\"",
        "Opt3": "\"オーバーフィッティング\"",
        "Opt4": "\"モデルの収束\"",
        "Opt5": "\"次元の呪い\"",
        "Opt6": "\"局所最適解\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】両方の誤差が高いのはモデルが単純すぎる「未学習」の状態です。[図解：縦軸に誤差、横軸にデータ量。2本の線が高い位置で並行して並んでいる図]\""
    },
    {
        "ID": "\"ML-052\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"学習曲線において、訓練データの誤差は低いがテストデータの誤差との乖離（ギャップ）が大きい状態は？\"",
        "Opt1": "\"高バイアス\"",
        "Opt2": "\"高バリアンス（過学習）\"",
        "Opt3": "\"アンダーフィッティング\"",
        "Opt4": "\"大数の法則\"",
        "Opt5": "\"早期終了\"",
        "Opt6": "\"正規化\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】訓練データにのみ適応し、未知データに弱い「過学習」の状態です。[図解：訓練誤差は右下がりで低いが、テスト誤差が上方に離れている図]\""
    },
    {
        "ID": "\"ML-053\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"サポートベクターマシン（SVM）において、誤分類を許容する度合いを決めるパラメータ『C』を大きくした時の挙動は？\"",
        "Opt1": "\"マージンが広くなり、過学習しにくくなる\"",
        "Opt2": "\"マージンが狭くなり、過学習しやすくなる\"",
        "Opt3": "\"計算速度が速くなる\"",
        "Opt4": "\"カーネルトリックが無効になる\"",
        "Opt5": "\"バイアスが大きくなる\"",
        "Opt6": "\"全ての重みが0になる\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】Cはペナルティの強さです。大きくすると「誤分類を許さない」ため、境界が複雑になり過学習しやすくなります。\""
    },
    {
        "ID": "\"ML-054\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"決定木において、データの分割前後でどれだけ情報の混ざり具合が減少したかを表す指標を何というか？\"",
        "Opt1": "\"ジニ係数\"",
        "Opt2": "\"情報利得\"",
        "Opt3": "\"エントロピー\"",
        "Opt4": "\"決定係数\"",
        "Opt5": "\"相関係数\"",
        "Opt6": "\"累積寄与率\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】情報利得（Information Gain）＝分割前の不純度 － 分割後の不純度。これが最大になるように分割します。\""
    },
    {
        "ID": "\"ML-055\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"ランダムフォレストにおいて、学習に使わなかったデータ（残されたデータ）を用いてモデルを評価する手法を何というか？\"",
        "Opt1": "\"クロスバリデーション\"",
        "Opt2": "\"ホールドアウト評価\"",
        "Opt3": "\"OOB（Out-of-Bag）評価\"",
        "Opt4": "\"Leave-one-out法\"",
        "Opt5": "\"LOOCV\"",
        "Opt6": "\"グリッドサーチ\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】ブートストラップ抽出で選ばれなかった約36.8%のデータを用いて、テストデータなしで精度推定が可能です。\""
    },
    {
        "ID": "\"ML-056\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"勾配ブースティング（GBDT）において、前の木の誤差を学習する際に学習をゆっくり進めるために導入されるパラメータは？\"",
        "Opt1": "\"学習率（収縮）\"",
        "Opt2": "\"木の本数\"",
        "Opt3": "\"最大深さ\"",
        "Opt4": "\"正則化係数\"",
        "Opt5": "\"バッチサイズ\"",
        "Opt6": "\"モーメンタム\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】Learning Rate（またはShrinkage）。値を小さくすることで、過学習を防ぎつつ精度を高めることができます。\""
    },
    {
        "ID": "\"ML-057\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"LightGBMがXGBoostと比較して高速な理由の一つである、木の成長方式は何というか？\"",
        "Opt1": "\"Level-wise（層別）\"",
        "Opt2": "\"Leaf-wise（葉別）\"",
        "Opt3": "\"Depth-first（深さ優先）\"",
        "Opt4": "\"Breadth-first（幅優先）\"",
        "Opt5": "\"Random-walk\"",
        "Opt6": "\"Best-fit\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】XGBoostが層ごとに成長させるのに対し、LightGBMは最も誤差が減る葉を優先的に伸ばすため効率的です。[図解：非対称に伸びる木の構造イメージ]\""
    },
    {
        "ID": "\"ML-058\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"主成分分析（PCA）において、元データの情報の何％を保持できているかを示す「累積寄与率」の一般的な目安は？\"",
        "Opt1": "\"20〜30％\"",
        "Opt2": "\"50％\"",
        "Opt3": "\"70〜90％\"",
        "Opt4": "\"99.9％以上\"",
        "Opt5": "\"100％\"",
        "Opt6": "\"0％\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】分析の目的によりますが、概ね70%〜90%程度を維持できるように主成分の数を選択するのが一般的です。\""
    },
    {
        "ID": "\"ML-059\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"線形判別分析（LDA）と主成分分析（PCA）の最大の違いは何か？\"",
        "Opt1": "\"PCAは教師あり、LDAは教師なし学習である\"",
        "Opt2": "\"PCAは教師なし、LDAは教師あり学習である\"",
        "Opt3": "\"どちらも非線形な手法である\"",
        "Opt4": "\"LDAは回帰問題にしか使えない\"",
        "Opt5": "\"PCAは距離、LDAは角度を最大化する\"",
        "Opt6": "\"違いはない\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】PCAは「全体の分散」を最大化しますが、LDAは「クラス間の離れ具合」を最大化するように射影します。[図解：ラベルなしで広がりを探すPCA vs ラベルを分けて境界を探すLDA]\""
    },
    {
        "ID": "\"ML-060\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"高次元データの可視化（2次元化）によく使われ、データの局所的な構造を維持するのに長けた非線形手法は？\"",
        "Opt1": "\"PCA\"",
        "Opt2": "\"t-SNE\"",
        "Opt3": "\"LDA\"",
        "Opt4": "\"k-means\"",
        "Opt5": "\"決定木\"",
        "Opt6": "\"ロジスティック回帰\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】高次元の類似度を低次元でも維持するように配置します。クラスの固まりが見えやすいのが特徴です。\""
    },
    {
        "ID": "\"ML-061\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"クラスタリングにおいて、距離（類似度）ではなく「密度」に基づいてクラスタを形成し、外れ値を除去できる手法は？\"",
        "Opt1": "\"k-means法\"",
        "Opt2": "\"ウォード法\"",
        "Opt3": "\"DBSCAN\"",
        "Opt4": "\"自己組織化マップ\"",
        "Opt5": "\"主成分分析\"",
        "Opt6": "\"アソシエーション分析\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】半径ε（イプシロン）以内に点がいくつあるかで判断します。非凸状（三日月型など）のデータも分類可能です。\""
    },
    {
        "ID": "\"ML-062\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"アソシエーション分析において、「商品Aを買う人のうち商品Bも買う人の割合」を示す指標は？\"",
        "Opt1": "\"支持度（Support）\"",
        "Opt2": "\"確信度（Confidence）\"",
        "Opt3": "\"リフト値（Lift）\"",
        "Opt4": "\"正解率\"",
        "Opt5": "\"再現率\"",
        "Opt6": "\"F値\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】Confidence = P(B|A)。条件付き確率と同じ考え方です。\""
    },
    {
        "ID": "\"ML-063\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"アソシエーション分析において、「AとBを一緒に買う組み合わせが全データの中で出現する割合」は？\"",
        "Opt1": "\"支持度（Support）\"",
        "Opt2": "\"確信度\"",
        "Opt3": "\"リフト値\"",
        "Opt4": "\"寄与率\"",
        "Opt5": "\"決定係数\"",
        "Opt6": "\"相関係数\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】Support = (AとBを含む数) / (全データ数)。ルールとしての「重要度」を表します。\""
    },
    {
        "ID": "\"ML-064\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"リフト値が「1」である時の解釈として正しいものはどれか？\"",
        "Opt1": "\"Aを買う人は必ずBも買う\"",
        "Opt2": "\"AとBの購入は互いに独立（無関係）である\"",
        "Opt3": "\"Aを買う人はBを絶対に買わない\"",
        "Opt4": "\"Aが売れるとBの売上が下がる\"",
        "Opt5": "\"最強の相関ルールである\"",
        "Opt6": "\"計算エラーである\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】リフト値＝確信度 / (Bの出現率)。1なら「たまたまBが売れる確率」と同じであり、ルールに価値がありません。\""
    },
    {
        "ID": "\"ML-065\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"強化学習のマルコフ決定過程（MDP）において、エージェントが「状態s」で「行動a」をとった際に「状態s'」に遷移する確率を何というか？\"",
        "Opt1": "\"状態遷移確率\"",
        "Opt2": "\"報酬関数\"",
        "Opt3": "\"価値関数\"",
        "Opt4": "\"方策\"",
        "Opt5": "\"割引率\"",
        "Opt6": "\"収益\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】P(s' | s"
    },
    {
        "ID": "\"ML-066\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"強化学習における「ベルマン方程式」が表している概念は？\"",
        "Opt1": "\"現在の価値と、次の状態の価値の再帰的な関係\"",
        "Opt2": "\"報酬を最大化する最短経路\"",
        "Opt3": "\"ニューラルネットワークの誤差\"",
        "Opt4": "\"データの正規化の手順\"",
        "Opt5": "\"最適なクラスタ数\"",
        "Opt6": "\"勾配降下の方向\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】「今の価値 ＝ 得られる報酬 ＋ 割引かれた次の状態の価値」という関係を数式化したものです。\""
    },
    {
        "ID": "\"ML-067\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"強化学習において、価値関数の代わりに「行動そのものの確率」を直接学習する手法の総称は？\"",
        "Opt1": "\"Q学習\"",
        "Opt2": "\"SARSA\"",
        "Opt3": "\"方策勾配法\"",
        "Opt4": "\"DQN\"",
        "Opt5": "\"モンテカルロ法\"",
        "Opt6": "\"動的計画法\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】Policy Gradient。行動を確率的に出力するため、連続的な行動空間にも対応しやすいです。\""
    },
    {
        "ID": "\"ML-068\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"DQN（Deep Q-Network）において、学習の安定化のために「過去の経験（s",
        "Opt1": "a",
        "Opt2": "r",
        "Opt3": "s'）」をメモリに蓄積してランダムに使う手法は？\"",
        "Opt4": "\"Target Network\"",
        "Opt5": "\"Experience Replay\"",
        "Opt6": "\"Double DQN\"",
        "Answer_Idx": "\"Dueling Network\"",
        "Explanation": "\"Dropout\""
    },
    {
        "ID": "\"ML-069\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"混同行列において、病気でない人を誤って「病気」と判定してしまうエラーを統計学で何と呼ぶか？\"",
        "Opt1": "\"第1種の過誤（偽陽性）\"",
        "Opt2": "\"第2種の過誤（偽陰性）\"",
        "Opt3": "\"ハルシネーション\"",
        "Opt4": "\"オーバーフィッティング\"",
        "Opt5": "\"未学習\"",
        "Opt6": "\"欠損値\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】Type I error。本当は「偽」なのに「真」と判定してしまうミスです。\""
    },
    {
        "ID": "\"ML-070\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"適合率（Precision）が1.0、再現率（Recall）が0.5の時、F値（F1-score）の計算結果は？\"",
        "Opt1": "\"0.5\"",
        "Opt2": "\"0.66...\"",
        "Opt3": "\"0.75\"",
        "Opt4": "\"1.0\"",
        "Opt5": "\"0\"",
        "Opt6": "\"計算不能\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】F = 2*(1.0*0.5) / (1.0+0.5) = 1.0 / 1.5 = 2/3 = 0.666...\""
    },
    {
        "ID": "\"ML-071\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"モデルのパラメータを直接求める「解析解」が存在せず、繰り返し計算で最適値を探る手法の代表格は？\"",
        "Opt1": "\"最小二乗法\"",
        "Opt2": "\"勾配降下法\"",
        "Opt3": "\"主成分分析\"",
        "Opt4": "\"線形判別分析\"",
        "Opt5": "\"決定木\"",
        "Opt6": "\"k-近傍法\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】損失関数の傾き（勾配）に沿って少しずつ値を更新し、最小値（谷底）を探します。\""
    },
    {
        "ID": "\"ML-072\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"勾配降下法において、一度の更新に「全データ」ではなく「ランダムに選んだ1つ」のデータのみを使う手法は？\"",
        "Opt1": "\"バッチ勾配降下法\"",
        "Opt2": "\"ミニバッチ勾配降下法\"",
        "Opt3": "\"確率的勾配降下法（SGD）\"",
        "Opt4": "\"モメンタム\"",
        "Opt5": "\"Adam\"",
        "Opt6": "\"RMSprop\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】Stochastic Gradient Descent。計算が速く、局所解を脱出しやすい性質があります。\""
    },
    {
        "ID": "\"ML-073\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"正則化項として「重みの二乗和」を用いることで、重みが大きくなりすぎるのを防ぐ手法は？\"",
        "Opt1": "\"L1正則化（Lasso）\"",
        "Opt2": "\"L2正則化（Ridge）\"",
        "Opt3": "\"エラスティックネット\"",
        "Opt4": "\"ソフトマージン\"",
        "Opt5": "\"ハードマージン\"",
        "Opt6": "\"Dropout\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】L2正則化は重みを滑らかに小さくし、過学習を抑えます。特徴選択の効果はありません。\""
    },
    {
        "ID": "\"ML-074\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"時系列データにおいて、トレンドや季節性を除去して統計的性質（平均や分散）を一定にした状態を何というか？\"",
        "Opt1": "\"定常性\"",
        "Opt2": "\"ホワイトノイズ\"",
        "Opt3": "\"ランダムウォーク\"",
        "Opt4": "\"自己回帰\"",
        "Opt5": "\"ラグ\"",
        "Opt6": "\"移動平均\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】多くの時系列モデル（ARIMAなど）は、データが「定常」であることを前提としています。\""
    },
    {
        "ID": "\"ML-075\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"不均衡データ対策として、少ない方のクラスのデータを水増ししてバランスを整える手法（オーバーサンプリング）の代表例は？\"",
        "Opt1": "\"SMOTE\"",
        "Opt2": "\"アンダーサンプリング\"",
        "Opt3": "\"主成分分析\"",
        "Opt4": "\"正則化\"",
        "Opt5": "\"交叉検証\"",
        "Opt6": "\"早期終了\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】近傍の点を使って新しい合成データを作成します。\""
    },
    {
        "ID": "\"ML-076\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"ハイパーパラメータの探索において、過去の試行結果から次に試すべき有望な値を統計的に予測する手法は？\"",
        "Opt1": "\"グリッドサーチ\"",
        "Opt2": "\"ランダムサーチ\"",
        "Opt3": "\"ベイズ最適化\"",
        "Opt4": "\"ヒューリスティック探索\"",
        "Opt5": "\"幅優先探索\"",
        "Opt6": "\"深さ優先探索\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】ガウス過程などを用いて「効率よく」最適なパラメータを探し出す手法です。\""
    },
    {
        "ID": "\"ML-077\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"「特徴量Aの値が大きくなると、特徴量Bの値も大きくなる」という関係が強い時、これらの相関関係を何というか？\"",
        "Opt1": "\"正の相関\"",
        "Opt2": "\"負の相関\"",
        "Opt3": "\"無相関\"",
        "Opt4": "\"因果関係\"",
        "Opt5": "\"寄与率\"",
        "Opt6": "\"決定係数\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】相関係数が1に近い状態です。「因果関係」とは別物であることに注意。\""
    },
    {
        "ID": "\"ML-078\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"回帰問題において、外れ値の影響を抑えるために二乗誤差の代わりに「絶対値誤差」を最小化する手法の総称は？\"",
        "Opt1": "\"ロバスト回帰\"",
        "Opt2": "\"多項式回帰\"",
        "Opt3": "\"ステップワイズ法\"",
        "Opt4": "\"ラッソ回帰\"",
        "Opt5": "\"リッジ回帰\"",
        "Opt6": "\"ロジスティック回帰\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】Huber損失などを用い、外れ値に引きずられにくい強靭なモデルを作ります。\""
    },
    {
        "ID": "\"ML-079\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"ナイーブベイズが「単純（Naive）」と呼ばれる理由は何か？\"",
        "Opt1": "\"アルゴリズムが1行で書けるから\"",
        "Opt2": "\"各特徴量が互いに独立であると仮定しているから\"",
        "Opt3": "\"確率を使わないから\"",
        "Opt4": "\"線形分離しかできないから\"",
        "Opt5": "\"データが少なくても動くから\"",
        "Opt6": "\"古い手法だから\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】現実には特徴量同士に相関があっても「独立」とみなして計算を簡略化するため、そう呼ばれます。\""
    },
    {
        "ID": "\"ML-080\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"SVMにおいて、全てのデータを正しく分類する非常に厳しい境界を引く考え方を何というか？\"",
        "Opt1": "\"ソフトマージン\"",
        "Opt2": "\"ハードマージン\"",
        "Opt3": "\"カーネルトリック\"",
        "Opt4": "\"正則化\"",
        "Opt5": "\"スラック変数\"",
        "Opt6": "\"バイアス\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】外れ値が一つあるだけで境界が激変するため、実務ではあまり使われません。\""
    },
    {
        "ID": "\"ML-081\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"「k-means法」と「k-近傍法（k-NN）」の共通点は何か？\"",
        "Opt1": "\"どちらも教師あり学習である\"",
        "Opt2": "\"どちらも教師なし学習である\"",
        "Opt3": "\"距離（類似度）の概念を利用する\"",
        "Opt4": "\"どちらもクラスタリング手法である\"",
        "Opt5": "\"どちらも回帰問題用である\"",
        "Opt6": "\"共通点はない\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】どちらも「近いデータは似ている」という前提で距離計算（ユークリッド距離など）を行います。\""
    },
    {
        "ID": "\"ML-082\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"ランダムフォレストにおいて、個々の決定木を深く（複雑に）成長させた場合、モデル全体はどう変化するか？\"",
        "Opt1": "\"バリアンスが減り、バイアスが増える\"",
        "Opt2": "\"バイアスが減り、バリアンスが増える\"",
        "Opt3": "\"精度が必ず下がる\"",
        "Opt4": "\"学習速度が上がる\"",
        "Opt5": "\"変化しない\"",
        "Opt6": "\"モデルが1本になる\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】個々の木は過学習気味（低バイアス・高バリアンス）になりますが、平均化（バギング）することでバリアンスを抑えます。\""
    },
    {
        "ID": "\"ML-083\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"アンサンブル学習において、異なる種類のアルゴリズム（SVM、決定木、NNなど）の予測結果を、別のモデルの入力にして最終予測を出す手法は？\"",
        "Opt1": "\"バギング\"",
        "Opt2": "\"ブースティング\"",
        "Opt3": "\"スタッキング\"",
        "Opt4": "\"ホールドアウト\"",
        "Opt5": "\"アンダーサンプリング\"",
        "Opt6": "\"蒸留\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】Stacking。2段階（以上）のモデル構成で精度を追求します。\""
    },
    {
        "ID": "\"ML-084\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"強化学習において、現在の状態からエピソード終了まで得られる報酬の合計（割引あり）を何というか？\"",
        "Opt1": "\"収益（リターン）\"",
        "Opt2": "\"方策\"",
        "Opt3": "\"価値\"",
        "Opt4": "\"報酬\"",
        "Opt5": "\"損失\"",
        "Opt6": "\"Q値\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】Return。エージェントはこの期待値を最大化することを目指します。\""
    },
    {
        "ID": "\"ML-085\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"Q学習（Q-Learning）において、行動を選択する際に「1-ε」の確率でベストな行動を選び、「ε」の確率でランダムに選ぶ手法は？\"",
        "Opt1": "\"ε-greedy法\"",
        "Opt2": "\"ボルツマン選択\"",
        "Opt3": "\"ソフトマックス選択\"",
        "Opt4": "\"ルーレット選択\"",
        "Opt5": "\"トーナメント選択\"",
        "Opt6": "\"完全ランダム選択\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】「探索と活用のトレードオフ」を調整する最も一般的な手法です。\""
    },
    {
        "ID": "\"ML-086\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"機械学習プロジェクトにおいて、訓練データの中に「本来は予測時に知り得ないはずの情報（未来の情報など）」が含まれてしまう不備を何というか？\"",
        "Opt1": "\"データポイズニング\"",
        "Opt2": "\"データリーク（漏洩）\"",
        "Opt3": "\"オーバーフィッティング\"",
        "Opt4": "\"アンダーサンプリング\"",
        "Opt5": "\"バイアス\"",
        "Opt6": "\"次元の呪い\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】Data Leakage。カンニングしているような状態で、テスト時に精度が激減する原因になります。\""
    },
    {
        "ID": "\"ML-087\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"カテゴリ変数を数値化する際、「順序性（松・竹・梅など）」がある場合に適した処理は？\"",
        "Opt1": "\"ワンホットエンコーディング\"",
        "Opt2": "\"オーディナルエンコーディング\"",
        "Opt3": "\"ラベルエンコーディング\"",
        "Opt4": "\"標準化\"",
        "Opt5": "\"正規化\"",
        "Opt6": "\"主成分分析\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】Ordinal Encoding。1"
    },
    {
        "ID": "\"ML-088\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"データの分布が右に長く伸びている（歪んでいる）際、分布を正規分布に近づけるために行う代表的な変換は？\"",
        "Opt1": "\"対数変換\"",
        "Opt2": "\"標準化\"",
        "Opt3": "\"正規化\"",
        "Opt4": "\"二値化\"",
        "Opt5": "\"多項式変換\"",
        "Opt6": "\"反転\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】Log Transformation。大きな値を圧縮し、モデルが学習しやすい分布に整えます。\""
    },
    {
        "ID": "\"ML-089\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"時系列データで、1期前の自分自身の値とどれくらい似ているかを示す指標は？\"",
        "Opt1": "\"自己相関\"",
        "Opt2": "\"偏自己相関\"",
        "Opt3": "\"決定係数\"",
        "Opt4": "\"相関係数\"",
        "Opt5": "\"平均二乗誤差\"",
        "Opt6": "\"変動係数\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】Autocorrelation。これを確認してAR（自己回帰）モデルの次数などを決めます。\""
    },
    {
        "ID": "\"ML-090\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"強化学習において、環境のモデル（遷移確率など）が未知の状態で、実際に試行錯誤して学習する手法を何というか？\"",
        "Opt1": "\"モデルベース学習\"",
        "Opt2": "\"モデルフリー学習\"",
        "Opt3": "\"教師あり学習\"",
        "Opt4": "\"半教師あり学習\"",
        "Opt5": "\"バッチ学習\"",
        "Opt6": "\"オンライン学習\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】現在の主要な強化学習手法の多く（Q学習など）はモデルフリーに分類されます。\""
    },
    {
        "ID": "\"ML-091\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"SVMのカーネルトリックにおいて、実務で最も多用される「無限次元」への写像が可能なカーネルは？\"",
        "Opt1": "\"線形カーネル\"",
        "Opt2": "\"多項式カーネル\"",
        "Opt3": "\"RBFカーネル（ガウスカーネル）\"",
        "Opt4": "\"シグモイドカーネル\"",
        "Opt5": "\"ラプラスカーネル\"",
        "Opt6": "\"対数カーネル\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】Radial Basis Functionカーネル。非線形な境界を引く際の第一選択肢です。\""
    },
    {
        "ID": "\"ML-092\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"主成分分析（PCA）の前に必ず行うべき前処理はどれか？\"",
        "Opt1": "\"ワンホットエンコーディング\"",
        "Opt2": "\"標準化（スケーリング）\"",
        "Opt3": "\"対数変換\"",
        "Opt4": "\"二値化\"",
        "Opt5": "\"欠損値の0埋め\"",
        "Opt6": "\"何も行わなくてよい\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】PCAは分散を最大化するため、単位（スケール）が違う変数が混ざっていると正しい方向に射影できません。\""
    },
    {
        "ID": "\"ML-093\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"ROC曲線において、モデルが「ランダムな予測」をしている場合のAUC（曲線下の面積）の値は？\"",
        "Opt1": "\"0\"",
        "Opt2": "\"0.5\"",
        "Opt3": "\"1.0\"",
        "Opt4": "\"100\"",
        "Opt5": "\"-1\"",
        "Opt6": "\".99\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】対角線になるため面積は0.5になります。1.0なら完璧、0.5以下なら予測が逆転していることを示します。\""
    },
    {
        "ID": "\"ML-094\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"決定木において、分割を止める「最大深さ（max_depth）」を小さく設定するとモデルはどうなるか？\"",
        "Opt1": "\"過学習しやすくなる\"",
        "Opt2": "\"未学習になりやすくなる（単純になる）\"",
        "Opt3": "\"計算時間が無限になる\"",
        "Opt4": "\"予測精度が100%になる\"",
        "Opt5": "\"バイアスが0になる\"",
        "Opt6": "\"変化しない\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】制約を強める（正則化）ことになるため、モデルはシンプルになり汎化性能が上がることがあります。\""
    },
    {
        "ID": "\"ML-095\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"「k-means法」で、最初に選ぶ重心（シード）の場所によって結果が大きく変わる問題を改善した手法は？\"",
        "Opt1": "\"k-means++\"",
        "Opt2": "\"k-medoids\"",
        "Opt3": "\"エルボー法\"",
        "Opt4": "\"シルエット法\"",
        "Opt5": "\"DBSCAN\"",
        "Opt6": "\"階層的クラスタリング\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】最初の重心をできるだけ離して選ぶように工夫したアルゴリズムです。\""
    },
    {
        "ID": "\"ML-096\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"ロジスティック回帰の損失関数として一般的に用いられるのはどれか？\"",
        "Opt1": "\"二乗和誤差\"",
        "Opt2": "\"交差エントロピー誤差（対数損失）\"",
        "Opt3": "\"ヒンジ損失\"",
        "Opt4": "\"絶対値誤差\"",
        "Opt5": "\"ジニ係数\"",
        "Opt6": "\"ハルシネーション\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】Log Loss。確率のズレを評価するのに最適な関数です。\""
    },
    {
        "ID": "\"ML-097\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"回帰問題において、全ての実測値の「平均値」を予測し続けた場合の決定係数（R2）の値は？\"",
        "Opt1": "\"0\"",
        "Opt2": "\"1.0\"",
        "Opt3": "\"100\"",
        "Opt4": "\"-1\"",
        "Opt5": "\"無限\"",
        "Opt6": "\"0.5\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】決定係数とは「平均で予測するよりもどれだけマシか」を表す指標なので、平均と同じなら0となります。\""
    },
    {
        "ID": "\"ML-098\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"強化学習のSARSAとQ学習の最大の違いは？\"",
        "Opt1": "\"SARSAはオンポリティ、Q学習はオフポリティである\"",
        "Opt2": "\"SARSAは教師あり学習である\"",
        "Opt3": "\"Q学習はニューラルネットワークを使わない\"",
        "Opt4": "\"SARSAは囲碁にしか使えない\"",
        "Opt5": "\"違いはない\"",
        "Opt6": "\"Q学習は報酬を与えない\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】実際に選んだ行動に基づいて更新するのがSARSA、次はベストな行動をとると仮定して更新するのがQ学習です。\""
    },
    {
        "ID": "\"ML-099\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"特徴量の重要度（Feature Importance）を算出できる手法として「適切でない」ものは？\"",
        "Opt1": "\"決定木\"",
        "Opt2": "\"ランダムフォレスト\"",
        "Opt3": "\"勾配ブースティング\"",
        "Opt4": "\"k-近傍法（k-NN）\"",
        "Opt5": "\"Lasso回帰\"",
        "Opt6": "\"XGBoost\"",
        "Answer_Idx": 3,
        "Explanation": "\"【解説】k-NNは距離計算のみを行うため、どの特徴量が効いているかを直接算出する仕組みを持ちません。\""
    },
    {
        "ID": "\"ML-100\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"機械学習における「ノーフリーランチ定理」が示唆している内容は？\"",
        "Opt1": "\"高いモデルほど精度が良い\"",
        "Opt2": "\"全てのデータに万能な最強のアルゴリズムは存在しない\"",
        "Opt3": "\"学習データは多ければ多いほど良い\"",
        "Opt4": "\"昼食代を節約すれば計算資源が買える\"",
        "Opt5": "\"GPUは必須である\"",
        "Opt6": "\"ディープラーニングが常に正解である\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】特定の問題に特化した手法が他では通用しないこともあるため、目的合わせた手法選択が重要だという教訓です。\""
    },
    {
        "ID": "\"ML-101\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"多クラス分類において、各クラスごとにF値を算出し、それらの単純な算術平均をとる評価手法を何というか？\"",
        "Opt1": "\"マイクロ平均（Micro-average）\"",
        "Opt2": "\"マクロ平均（Macro-average）\"",
        "Opt3": "\"重み付き平均（Weighted-average）\"",
        "Opt4": "\"調和平均\"",
        "Opt5": "\"幾何平均\"",
        "Opt6": "\"移動平均\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】マクロ平均は「クラスごとの重要度」を均等に扱います。一方、マイクロ平均は全データ通算で算出するため、データ数の多いクラスに影響を受けます。\""
    },
    {
        "ID": "\"ML-102\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"不均衡データにおいて、各クラスのデータ数に比例した重みを付けて平均をとる評価手法は？\"",
        "Opt1": "\"マイクロ平均\"",
        "Opt2": "\"マクロ平均\"",
        "Opt3": "\"重み付き平均（Weighted-average）\"",
        "Opt4": "\"正解率\"",
        "Opt5": "\"特異度\"",
        "Opt6": "\"AUC\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】Weighted-average。各クラスのサンプルサイズを考慮するため、不均衡データでの全体的な性能把握に向いています。\""
    },
    {
        "ID": "\"ML-103\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"分類モデルの出力が「確率」として適切かどうかを評価する、予測確率と実際のラベルのズレを測る指標は？\"",
        "Opt1": "\"Log-Loss（対数損失）\"",
        "Opt2": "\"Brier Score（ブライアスコア）\"",
        "Opt3": "\"F値\"",
        "Opt4": "\"ジニ係数\"",
        "Opt5": "\"エルボー点\"",
        "Opt6": "\"決定係数\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】Brier Scoreは予測確率と実際の値（0 or 1）の二乗誤差の平均です。0に近いほど「確率の予測」が正確であることを示します。\""
    },
    {
        "ID": "\"ML-104\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"回帰モデルにおいて、モデルが全く意味のない予測（平均値を出し続ける）をした場合の決定係数（R2）の値は？\"",
        "Opt1": "\"1.0\"",
        "Opt2": "\"0.5\"",
        "Opt3": "\"0.0\"",
        "Opt4": "\"-1.0\"",
        "Opt5": "\"無限大\"",
        "Opt6": "\"計算不能\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】決定係数は「平均値での予測よりどれだけマシか」を表すため、平均値と同じ精度なら0になります。負の値になる場合は平均より悪いことを意味します。\""
    },
    {
        "ID": "\"ML-105\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"サポートベクターマシンにおいて、線形カーネルを使用した場合の「C」パラメータの役割として正しいものは？\"",
        "Opt1": "\"特徴量の数を制限する\"",
        "Opt2": "\"誤分類へのペナルティの強さを調節する\"",
        "Opt3": "\"高次元へ写像する次元数を決める\"",
        "Opt4": "\"学習率を決定する\"",
        "Opt5": "\"カーネルの広がり（ガンマ）を制御する\"",
        "Opt6": "\"バイアスを固定する\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】Cはハードマージン（厳格）とソフトマージン（寛容）のバランスを決めます。Cが大きいほど誤分類を許さず過学習しやすくなります。\""
    },
    {
        "ID": "\"ML-106\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"RBFカーネルを用いたSVMにおいて、1つのデータの影響範囲を制御するパラメータ『gamma』を大きくするとどうなるか？\"",
        "Opt1": "\"境界が直線に近づく\"",
        "Opt2": "\"境界が複雑になり、過学習しやすくなる\"",
        "Opt3": "\"マージンが広くなる\"",
        "Opt4": "\"計算速度が劇的に向上する\"",
        "Opt5": "\"全ての重みが0になる\"",
        "Opt6": "\"アンダーフィッティングが起きる\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】gamma（γ）が大きいと影響範囲が狭くなり、個々のデータに過剰に反応するため、複雑な境界線（島のような境界）になります。\""
    },
    {
        "ID": "\"ML-107\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"決定木の学習アルゴリズムのうち、多値分類や回帰にも対応し、不純度の指標にジニ係数を用いる代表的なものは？\"",
        "Opt1": "\"ID3\"",
        "Opt2": "\"C4.5\"",
        "Opt3": "\"CART\"",
        "Opt4": "\"CHAID\"",
        "Opt5": "\"XGBoost\"",
        "Opt6": "\"AdaBoost\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】Classification and Regression Trees。2分木（Yes/Noの2つに分ける）を繰り返すのが特徴で、Scikit-learnの標準実装もこれです。\""
    },
    {
        "ID": "\"ML-108\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"ランダムフォレストと比較して、勾配ブースティング（GBDT）が持つ性質として正しいものは？\"",
        "Opt1": "\"並列処理が容易である\"",
        "Opt2": "\"学習データが少ないと過学習しやすい\"",
        "Opt3": "\"決定木の深さを非常に深くする必要がある\"",
        "Opt4": "\"サンプリングに重複を許さない\"",
        "Opt5": "\"計算コストが常に低い\"",
        "Opt6": "\"予測精度が常に低い\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】GBDTは前の木のミスを次々と修正する「直列学習」のため、強力ですが過学習のリスクも高いです。そのため決定木は「浅く」作るのが一般的です。\""
    },
    {
        "ID": "\"ML-109\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"XGBoostが従来の勾配ブースティングに加えて導入した、計算速度と精度を向上させる工夫は？\"",
        "Opt1": "\"L1/L2正則化の導入\"",
        "Opt2": "\"Leaf-wiseの採用\"",
        "Opt3": "\"アンダーサンプリング\"",
        "Opt4": "\"決定木の並列学習（Bagging）\"",
        "Opt5": "\"特徴量抽出の自動化\"",
        "Opt6": "\"Dropoutの導入\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】XGBoostは損失関数に正則化項を含めることで過学習を抑制しました。また、欠損値の自動処理なども特徴です。\""
    },
    {
        "ID": "\"ML-110\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"LightGBMにおいて、ヒストグラムベースのアルゴリズムを利用して計算を高速化する手法の名称は？\"",
        "Opt1": "\"GOSS（Gradient-based One-Side Sampling）\"",
        "Opt2": "\"EFB（Exclusive Feature Bundling）\"",
        "Opt3": "\"経験再生\"",
        "Opt4": "\"ターゲットネットワーク\"",
        "Opt5": "\"ドロップアウト\"",
        "Opt6": "\"早期終了\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】勾配の小さい（学習が進んでいる）データを除外して計算量を減らす手法。EFBは疎な特徴量をまとめる手法です。\""
    },
    {
        "ID": "\"ML-111\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"CatBoost（Categorical Boosting）の最大の特徴はどれか？\"",
        "Opt1": "\"画像データを直接学習できる\"",
        "Opt2": "\"カテゴリ変数の変換（Target Encoding等）を効率的・自動的に行う\"",
        "Opt3": "\"GPUを必要としない\"",
        "Opt4": "\"ニューラルネットワークの一種である\"",
        "Opt5": "\"教師なし学習である\"",
        "Opt6": "\"欠損値を平均値で埋める\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】名前の通りCategoricalなデータに強く、データセット内の文字列などの扱いが非常に洗練されています。\""
    },
    {
        "ID": "\"ML-112\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"複数のモデル（Learner）を並列に学習させ、それらの予測結果をさらに別の「メタモデル」に学習させて最終出力を得る手法は？\"",
        "Opt1": "\"バギング\"",
        "Opt2": "\"ブースティング\"",
        "Opt3": "\"スタッキング（Stacking）\"",
        "Opt4": "\"蒸留\"",
        "Opt5": "\"転移学習\"",
        "Opt6": "\"アンダーサンプリング\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】Stackingは、個々のモデルの得意・不得意をメタモデルが学習するため、コンペティション等で頻用されます。\""
    },
    {
        "ID": "\"ML-113\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"k-means法において、初期値の選び方によって結果が変わってしまう問題を、距離の分散を考慮して初期値を決めることで改善した手法は？\"",
        "Opt1": "\"k-means++\"",
        "Opt2": "\"k-medoids\"",
        "Opt3": "\"エルボー法\"",
        "Opt4": "\"シルエット法\"",
        "Opt5": "\"DBSCAN\"",
        "Opt6": "\"平均値シフト法\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】最初の重心（セントロイド）同士ができるだけ離れるように選ぶ確率的な工夫をしています。\""
    },
    {
        "ID": "\"ML-114\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"クラスタリング手法において、円形ではない複雑な形状のクラスタ（三日月型など）を正しく検出できるのはどれか？\"",
        "Opt1": "\"k-means法\"",
        "Opt2": "\"エルボー法\"",
        "Opt3": "\"DBSCAN\"",
        "Opt4": "\"ウォード法\"",
        "Opt5": "\"主成分分析\"",
        "Opt6": "\"線形判別分析\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】DBSCANは「密度」で繋がっているかどうかを見るため、繋がっていればどんな形でも1つのクラスタとして認識できます。\""
    },
    {
        "ID": "\"ML-115\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"DBSCANのパラメータで、ある点から半径ε（イプシロン）以内に存在すべき最小のデータ点数を何というか？\"",
        "Opt1": "\"min_samples (または minPts)\"",
        "Opt2": "\"epsilon\"",
        "Opt3": "\"cluster_size\"",
        "Opt4": "\"density_threshold\"",
        "Opt5": "\"p-value\"",
        "Opt6": "\"k-value\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】この数以上の点が近傍にあれば、その点は「コア点」とみなされます。\""
    },
    {
        "ID": "\"ML-116\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"主成分分析（PCA）において、元データの情報の何％を保持できているかを示す「寄与率」を全主成分で合計したものを何というか？\"",
        "Opt1": "\"総寄与率\"",
        "Opt2": "\"累積寄与率\"",
        "Opt3": "\"平均寄与率\"",
        "Opt4": "\"決定係数\"",
        "Opt5": "\"相関係数\"",
        "Opt6": "\"F値\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】累積寄与率が80〜90%程度になるまでの主成分を採用するのが一般的です。\""
    },
    {
        "ID": "\"ML-117\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"行列分解の一種で、すべての要素が非負（0以上）であることを利用し、顔画像から「目」や「鼻」のような部品を抽出するのに適した手法は？\"",
        "Opt1": "\"PCA\"",
        "Opt2": "\"LDA\"",
        "Opt3": "\"NMF（非負行列因子分解）\"",
        "Opt4": "\"SVD（特異値分解）\"",
        "Opt5": "\"t-SNE\"",
        "Opt6": "\"k-means\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】Non-negative Matrix Factorization。足し算だけで構成されるため、部品の組み合わせ（パーツ分解）としての解釈が容易です。\""
    },
    {
        "ID": "\"ML-118\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"文書データ（単語-文書行列）に特異値分解を適用し、単語の潜在的な意味（トピック）を抽出する手法を何というか？\"",
        "Opt1": "\"LSA（潜在意味解析）\"",
        "Opt2": "\"LDA（潜在的ディリクレ配分法）\"",
        "Opt3": "\"Word2Vec\"",
        "Opt4": "\"BERT\"",
        "Opt5": "\"TF-IDF\"",
        "Opt6": "\"NMF\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】Latent Semantic Analysis。似た意味の単語を近くに配置できます。LDA（潜在的ディリクレ配分法）もトピックモデルですが、確率モデルです。\""
    },
    {
        "ID": "\"ML-119\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"高次元データの可視化手法「t-SNE」の欠点として正しいものはどれか？\"",
        "Opt1": "\"非線形なデータに対応できない\"",
        "Opt2": "\"計算コストが非常に高く、大規模データに時間がかかる\"",
        "Opt3": "\"次元を増やすことしかできない\"",
        "Opt4": "\"必ず直線的な境界を引く\"",
        "Opt5": "\"出力が常に0か1になる\"",
        "Opt6": "\"教師あり学習でしか使えない\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】t-SNEは計算が重く、またパラメータ（Perplexity）によって結果が大きく変わるため注意が必要です。\""
    },
    {
        "ID": "\"ML-120\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"アソシエーション分析において、アイテムAを含む全取引のうち、アイテムBも含む割合を算出した際、分母となるのはどれか？\"",
        "Opt1": "\"全取引数\"",
        "Opt2": "\"アイテムAを含む取引数\"",
        "Opt3": "\"アイテムBを含む取引数\"",
        "Opt4": "\"アイテムAとBの両方を含む取引数\"",
        "Opt5": "\"アイテムAもBも含まない取引数\"",
        "Opt6": "\"計算不能\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】確信度（Confidence）の計算です。P(B|A) = N(A∩B) / N(A) となります。\""
    },
    {
        "ID": "\"ML-121\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"強化学習において、エージェントが「環境のルール（状態遷移確率など）」を完全に知っている状態で計画を立てる手法を何というか？\"",
        "Opt1": "\"モデルフリー学習\"",
        "Opt2": "\"モデルベース学習\"",
        "Opt3": "\"オンライン学習\"",
        "Opt4": "\"オフライン学習\"",
        "Opt5": "\"教師あり学習\"",
        "Opt6": "\"能動学習\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】環境のシミュレーター（モデル）を自前で持っている状態。ダイナミックプログラミング（動的計画法）などが該当します。\""
    },
    {
        "ID": "\"ML-122\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"強化学習のSARSAにおいて、Q値の更新に使う「次の行動」はどのように決まるか？\"",
        "Opt1": "\"常にQ値が最大となる行動（ベスト）を選ぶ\"",
        "Opt2": "\"実際に次にとった（方策に従った）行動を使う\"",
        "Opt3": "\"ランダムに選ぶ\"",
        "Opt4": "\"人間が指示した行動を使う\"",
        "Opt5": "\"前のステップと同じ行動を使う\"",
        "Opt6": "\"何もしない\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】SARSAはOn-policy（オンポリティ）。実際に自分が選んだ行動の報酬で更新します。一方、Q学習は「次もベストを尽くす」と仮定して更新します（Off-policy）。\""
    },
    {
        "ID": "\"ML-123\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"DQN（Deep Q-Network）において、学習を安定させるために「Q値の算出用」と「更新の目標値（教師）用」の2つのネットワークを使い分ける手法は？\"",
        "Opt1": "\"Experience Replay\"",
        "Opt2": "\"Target Network\"",
        "Opt3": "\"Double DQN\"",
        "Opt4": "\"Dueling Network\"",
        "Opt5": "\"Dropout\"",
        "Opt6": "\"Softmax\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】同じネットワークで目標値を追いかけると「自分のしっぽを追いかける犬」のように不安定になるため、目標値をしばらく固定するTarget Networkが導入されました。\""
    },
    {
        "ID": "\"ML-124\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"強化学習において、行動を直接出力する「Actor（行動手）」と、その行動を評価する「Critic（批評家）」を組み合わせた手法の総称は？\"",
        "Opt1": "\"Q学習\"",
        "Opt2": "\"モンテカルロ法\"",
        "Opt3": "\"Actor-Critic\"",
        "Opt4": "\"SARSA\"",
        "Opt5": "\"GAIL\"",
        "Opt6": "\"DQN\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】Actorが方策を改善し、Criticが価値関数を推定します。A2CやA3C、PPOなどがこの枠組みに基づいています。\""
    },
    {
        "ID": "\"ML-125\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"未知のデータに対する予測ミスを「バイアス」と「バリアンス」に分けたとき、モデルを簡略化（正則化を強化）するとどうなるか？\"",
        "Opt1": "\"バイアスが減り、バリアンスが増える\"",
        "Opt2": "\"バイアスが増え、バリアンスが減る\"",
        "Opt3": "\"両方増える\"",
        "Opt4": "\"両方減る\"",
        "Opt5": "\"どちらも変化しない\"",
        "Opt6": "\"計算不能になる\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】モデルを「頑固（単純）」にすると、個別のデータに惑わされなくなる（低バリアンス）一方で、真の構造からズレやすくなる（高バイアス）傾向があります。\""
    },
    {
        "ID": "\"ML-126\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"線形回帰において、特徴量間に強い相関がある（多重共線性：マルチコ）場合に、最小二乗法で起きる問題は？\"",
        "Opt1": "\"予測が常に0になる\"",
        "Opt2": "\"解が不安定になり、係数が極端に大きな値になる\"",
        "Opt3": "\"計算が速すぎて停止する\"",
        "Opt4": "\"過学習が絶対に起きなくなる\"",
        "Opt5": "\"バイアスが0になる\"",
        "Opt6": "\"常に正解率100%になる\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】マルチコが起きると行列の逆行列が不安定になり、回帰係数が爆発します。これを防ぐのがL2正則化（Ridge回帰）の役割の一つです。\""
    },
    {
        "ID": "\"ML-127\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"L1正則化（Lasso）が一部の回帰係数を完全に「0」にする数学的な理由は、L1ノルムの制約領域がどのような形をしているからか？\"",
        "Opt1": "\"円形\"",
        "Opt2": "\"正方形（ダイヤモンド型）\"",
        "Opt3": "\"三角形\"",
        "Opt4": "\"楕円形\"",
        "Opt5": "\"球体\"",
        "Opt6": "\"無限に広がる形\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】ダイヤモンド型の角（軸上）で損失関数の等高線と接しやすいため、係数が0になりやすい（スパース性）という特徴があります。[図解：座標軸に頂点がある菱形イメージ]\""
    },
    {
        "ID": "\"ML-128\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"ハイパーパラメータの最適化において、学習の途中で検証データの精度が悪化し始めたら学習を打ち切る手法は？\"",
        "Opt1": "\"早期終了（Early Stopping）\"",
        "Opt2": "\"ドロップアウト\"",
        "Opt3": "\"正則化\"",
        "Opt4": "\"バッチ正規化\"",
        "Opt5": "\"勾配クリッピング\"",
        "Opt6": "\"データ拡張\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】過学習が始まる直前で止める、最も実戦的でシンプルなテクニックの一つです。\""
    },
    {
        "ID": "\"ML-129\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"特徴量の重要度を評価する際、特定の列の値をシャッフル（ランダムに入れ替え）して、精度の低下具合を測る手法は？\"",
        "Opt1": "\"ジニ重要度\"",
        "Opt2": "\"Permutation Importance\"",
        "Opt3": "\"LIME\"",
        "Opt4": "\"SHAP\"",
        "Opt5": "\"Grad-CAM\"",
        "Opt6": "\"主成分分析\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】「その列をめちゃくちゃにしたらどれだけ困るか」で重要度を測る、モデルを選ばない強力な手法です。\""
    },
    {
        "ID": "\"ML-130\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"ベイズ最適化において、次に探索すべきパラメータ地点を決めるための指標を何というか？\"",
        "Opt1": "\"損失関数\"",
        "Opt2": "\"獲得関数（Acquisition Function）\"",
        "Opt3": "\"活性化関数\"",
        "Opt4": "\"価値関数\"",
        "Opt5": "\"尤度関数\"",
        "Opt6": "\"ポテンシャル関数\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】「まだ見ていない未知の領域（探索）」と「良さそうな既知の領域（活用）」のバランスをとって次を決めます。\""
    },
    {
        "ID": "\"ML-131\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"時系列分析において、現在の値が過去の「誤差（ホワイトノイズ）」の影響を受けると仮定するモデルは？\"",
        "Opt1": "\"ARモデル（自己回帰）\"",
        "Opt2": "\"MAモデル（移動平均）\"",
        "Opt3": "\"ARMAモデル\"",
        "Opt4": "\"ARIMAモデル\"",
        "Opt5": "\"SARIMAモデル\"",
        "Opt6": "\"指数平滑法\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】Moving Averageモデル。過去の「値」ではなく「予測誤差（ノイズ）」の重み付け和で表します。\""
    },
    {
        "ID": "\"ML-132\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"ARIMAモデルの『I』は何を意味し、どのようなデータに対して適用されるか？\"",
        "Opt1": "\"Integration（和分）：非定常データ（トレンドがあるデータ）\"",
        "Opt2": "\"Intelligence（知能）：AIデータ\"",
        "Opt3": "\"Isolation（孤立）：外れ値データ\"",
        "Opt4": "\"Identity（恒等）：変化のないデータ\"",
        "Opt5": "\"Increase（増加）：右肩上がりのデータ\"",
        "Opt6": "\"Inverse（逆）：反転データ\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】差分をとることでトレンドを除去し、定常データに変換する処理を指します。\""
    },
    {
        "ID": "\"ML-133\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"混合ガウスモデル（GMM）のパラメータ（平均や分散）を推定するために用いられる、期待値計算と最大化を繰り返すアルゴリズムは？\"",
        "Opt1": "\"誤差逆伝播法\"",
        "Opt2": "\"勾配降下法\"",
        "Opt3": "\"EMアルゴリズム\"",
        "Opt4": "\"k-means法\"",
        "Opt5": "\"モンテカルロ法\"",
        "Opt6": "\"ビタビアルゴリズム\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】Expectation-Maximization。隠れ変数（どのデータがどのガウス分布から出たか）がある場合の最尤推定に使います。\""
    },
    {
        "ID": "\"ML-134\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"複数の決定木を組み合わせるランダムフォレストにおいて、個々の木が「相関の低い（多様な）」ものになる理由は？\"",
        "Opt1": "\"データの重複を許さないから\"",
        "Opt2": "\"各木で使う特徴量をランダムに制限するから\"",
        "Opt3": "\"全ての木で同じデータを使うから\"",
        "Opt4": "\"木を深くしないから\"",
        "Opt5": "\"ブースティングを使っているから\"",
        "Opt6": "\"学習率を小さくするから\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】「ブートストラップサンプリング（データ選択）」と「特徴量のランダム選択」の2つの工夫で多様性を出します。\""
    },
    {
        "ID": "\"ML-135\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"SVMにおいて、データが線形分離できない場合に「マージンの外側や境界の反対側」に点があることを許容するための変数は？\"",
        "Opt1": "\"スラック変数（ξ：クサイ）\"",
        "Opt2": "\"ダミー変数\"",
        "Opt3": "\"隠れ変数\"",
        "Opt4": "\"ラグランジュ未定乗数\"",
        "Opt5": "\"重み係数\"",
        "Opt6": "\"バイアス項\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】この変数の和が一定以下になるように制限しながらマージンを最大化します。\""
    },
    {
        "ID": "\"ML-136\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"ナイーブベイズ（単純ベイズ）が適用される代表的なタスクはどれか？\"",
        "Opt1": "\"株価の精密な数値予測\"",
        "Opt2": "\"スパムメールの判定（テキスト分類）\"",
        "Opt3": "\"画像のセグメンテーション\"",
        "Opt4": "\"強化学習の報酬設計\"",
        "Opt5": "\"データの欠損値補完\"",
        "Opt6": "\"リアルタイムの物体検出\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】単語の出現頻度という単純な情報から確率を計算するため、高速かつ強力に機能します。\""
    },
    {
        "ID": "\"ML-137\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"「k-近傍法（k-NN）」の学習（モデル構築）コストについて正しい説明は？\"",
        "Opt1": "\"非常に高い\"",
        "Opt2": "\"中程度\"",
        "Opt3": "\"実質的にゼロ（怠惰学習）\"",
        "Opt4": "\"データの二乗に比例する\"",
        "Opt5": "\"パラメータ数に依存する\"",
        "Opt6": "\"学習データがなくても予測できる\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】k-NNはデータを保存しておくだけで、予測時に初めて計算を行う「Lazy Learning（怠惰学習）」です。\""
    },
    {
        "ID": "\"ML-138\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"不均衡データの評価で、ROC曲線よりも「陽性クラス（少ない方）」の予測精度をシビアに評価できる指標は？\"",
        "Opt1": "\"正解率\"",
        "Opt2": "\"PR曲線（適合率-再現率曲線）\"",
        "Opt3": "\"エルボー曲線\"",
        "Opt4": "\"正規分布\"",
        "Opt5": "\"学習曲線\"",
        "Opt6": "\"決定係数\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】ROC曲線は陰性が膨大だと「偽陽性率」が小さくなり、性能が過大評価されがちです。PR曲線は陽性にフォーカスします。\""
    },
    {
        "ID": "\"ML-139\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"決定木において、分割しても不純度が下がらない（あるいはサンプル数が少なすぎる）場合に分割を止めることを何というか？\"",
        "Opt1": "\"事前剪定（プルーニング）\"",
        "Opt2": "\"事後剪定\"",
        "Opt3": "\"スケーリング\"",
        "Opt4": "\"正規化\"",
        "Opt5": "\"正則化\"",
        "Opt6": "\"早期終了\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】木が成長しすぎるのをあらかじめ止める手法。成長しきった後に削るのが事後剪定です。\""
    },
    {
        "ID": "\"ML-140\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"勾配ブースティングにおいて、1つ前の木の「勾配（残差）」ではなく「2次微分」まで利用して最適化を高速化しているライブラリは？\"",
        "Opt1": "\"XGBoost\"",
        "Opt2": "\"AdaBoost\"",
        "Opt3": "\"Random Forest\"",
        "Opt4": "\"k-NN\"",
        "Opt5": "\"SVM\"",
        "Opt6": "\"Linear Regression\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】テイラー展開による2次近似を用いて、より効率的に損失を最小化する方向を探ります。\""
    },
    {
        "ID": "\"ML-141\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"「次元の呪い」の対策として不適切なものはどれか？\"",
        "Opt1": "\"特徴量選択（不要な列を消す）\"",
        "Opt2": "\"主成分分析（次元圧縮）\"",
        "Opt3": "\"L1正則化（係数を0にする）\"",
        "Opt4": "\"学習データ（サンプル数）を増やす\"",
        "Opt5": "\"特徴量の数をさらに増やす\"",
        "Opt6": "\"ドメイン知識による変数絞り込み\"",
        "Answer_Idx": 4,
        "Explanation": "\"【解説】次元が増えるほどデータ密度がスカスカになるため、さらに次元を増やすのは逆効果です。\""
    },
    {
        "ID": "\"ML-142\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"SVMで「カーネルトリック」が計算上有利な理由は？\"",
        "Opt1": "\"データを実際に高次元に変換しなくてよいから\"",
        "Opt2": "\"全てのデータを保存しなくてよいから\"",
        "Opt3": "\"学習率を気にする必要がないから\"",
        "Opt4": "\"欠損値があっても動くから\"",
        "Opt5": "\"次元を減らす技術だから\"",
        "Opt6": "\"線形問題にしか使えないから\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】高次元空間での「内積」を、元の空間の計算だけで求められる関数（カーネル関数）を使うため、次元がいくら高くても計算負荷が上がりません。\""
    },
    {
        "ID": "\"ML-143\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"教師なし学習「自己組織化マップ（SOM）」が脳の仕組みからヒントを得ている点として正しいのは？\"",
        "Opt1": "\"誤差を後ろから伝える（逆伝播）\"",
        "Opt2": "\"隣り合うユニットが似た情報を学習する（トポロジー保存）\"",
        "Opt3": "\"全てのユニットが同期して発火する\"",
        "Opt4": "\"データを順番に忘れていく\"",
        "Opt5": "\"入力層と出力層が同じである\"",
        "Opt6": "\"活性化関数にシグモイドを使う\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】高次元データを2次元のグリッドに写像し、似たデータが近くに集まるように配置される「地図」のような手法です。\""
    },
    {
        "ID": "\"ML-144\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"アソシエーション分析のアルゴリズム「Apriori」が効率的な理由は？\"",
        "Opt1": "\"全ての組み合わせを計算するから\"",
        "Opt2": "\"『頻出しないアイテムを含むセットは、頻出しない』という性質を利用するから\"",
        "Opt3": "\"ニューラルネットワークを使っているから\"",
        "Opt4": "\"データを100次元に圧縮するから\"",
        "Opt5": "\"ランダムに計算を打ち切るから\"",
        "Opt6": "\"GPUを多用するから\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】「単独で売れていないものは、セットでも売れない」という枝刈りルール（Apriori原理）により、計算爆発を防ぎます。\""
    },
    {
        "ID": "\"ML-145\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"強化学習の「価値関数」において、状態sから方策πに従った時の期待収益を表すのは？\"",
        "Opt1": "\"状態価値関数 Vπ(s)\"",
        "Opt2": "\"行動価値関数 Qπ(s",
        "Opt3": "a)\"",
        "Opt4": "\"即時報酬 r\"",
        "Opt5": "\"遷移確率 P\"",
        "Opt6": "\"割引率 γ\"",
        "Answer_Idx": "\"アドバンテージ関数\"",
        "Explanation": 0
    },
    {
        "ID": "\"ML-146\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"強化学習のモンテカルロ法の特徴として正しいものは？\"",
        "Opt1": "\"エピソードが終了するまで報酬が確定しない\"",
        "Opt2": "\"1ステップごとに学習を行う\"",
        "Opt3": "\"環境のモデルが必要である\"",
        "Opt4": "\"無限に高い報酬を許容する\"",
        "Opt5": "\"Q値を使わない\"",
        "Opt6": "\"ニューラルネットワークが必須である\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】最後までやってみて（エピソード完了）、得られた収益を過去の各状態に分配する手法です。\""
    },
    {
        "ID": "\"ML-147\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"DQNなどの深層強化学習で、強化学習特有の「データ間の時間的相関」を解消するために用いられる技術は？\"",
        "Opt1": "\"Batch Normalization\"",
        "Opt2": "\"Experience Replay\"",
        "Opt3": "\"Softmax\"",
        "Opt4": "\"ReLU\"",
        "Opt5": "\"Max Pooling\"",
        "Opt6": "\"Dropout\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】過去の経験（s"
    },
    {
        "ID": "\"ML-148\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"クラスタリングの結果を評価する「シルエット係数」において、最も良い分類ができている時の値は？\"",
        "Opt1": "\"1\"",
        "Opt2": "\"-1\"",
        "Opt3": "\"0\"",
        "Opt4": "\"100\"",
        "Opt5": "\"無限大\"",
        "Opt6": "\"0.5\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】1に近いほど「自分のクラスタの点と近く、隣のクラスタの点と遠い」状態を示します。\""
    },
    {
        "ID": "\"ML-149\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"「バイアス-バリアンス分解」において、ノイズが含まれるデータに対してモデルを極限まで柔軟（高バリアンス）にした際、合計誤差はどうなるか？\"",
        "Opt1": "\"0になる\"",
        "Opt2": "\"ノイズの分散（不可避誤差）以下にはならない\"",
        "Opt3": "\"無限に大きくなる\"",
        "Opt4": "\"バイアスと同じ値になる\"",
        "Opt5": "\"負の値になる\"",
        "Opt6": "\"不変である\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】どれだけ完璧に学習しても、データそのものが持つノイズ（ベイズ誤差）分は予測誤差として残ります。\""
    },
    {
        "ID": "\"ML-150\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"モデルの評価手法「ホールドアウト法」が持つ大きな欠点はどれか？\"",
        "Opt1": "\"計算時間が非常にかかること\"",
        "Opt2": "\"データ分割の仕方に結果が依存し、評価が不安定になること\"",
        "Opt3": "\"数学的に複雑すぎること\"",
        "Opt4": "\"小規模データでは使えないこと\"",
        "Opt5": "\"必ず過学習が起きること\"",
        "Opt6": "\"パラメータチューニングができないこと\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】たまたま学習データに難しい問題、テストデータに簡単な問題が混ざると精度が高く見えてしまう等のブレが生じます。これを防ぐのが「交差検証（クロスバリデーション）」です。\""
    },
    {
        "ID": "\"ML-151\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"回帰分析において、説明変数同士に強い相関がある場合に係数の推定が不安定になる現象を何というか？\"",
        "Opt1": "\"自己相関\"",
        "Opt2": "\"多重共線性（マルチコ）\"",
        "Opt3": "\"次元の呪い\"",
        "Opt4": "\"過学習\"",
        "Opt5": "\"不均一分散\"",
        "Opt6": "\"バイアス・バリアンス・トレードオフ\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】マルチコリニアリティ（Multicollinearity）と呼ばれます。これを解決するためにL2正則化（Ridge回帰）や主成分回帰が用いられます。\""
    },
    {
        "ID": "\"ML-152\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"重回帰分析において、説明変数を増やせば増やすほど値が改善（上昇）してしまうため、変数の数で調整を行う指標は？\"",
        "Opt1": "\"決定係数\"",
        "Opt2": "\"自由度調整済み決定係数\"",
        "Opt3": "\"相関係数\"",
        "Opt4": "\"F値\"",
        "Opt5": "\"AIC（赤池情報量基準）\"",
        "Opt6": "\"MAE\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】通常の決定係数は意味のない変数を入れても上昇するため、適切な変数選択のために自由度調整済み決定係数を用います。\""
    },
    {
        "ID": "\"ML-153\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"ロジスティック回帰において、パラメータ推定の際に最大化を目指す関数はどれか？\"",
        "Opt1": "\"残差二乗和\"",
        "Opt2": "\"尤度関数（ゆうどかんすう）\"",
        "Opt3": "\"ジニ係数\"",
        "Opt4": "\"情報利得\"",
        "Opt5": "\"ヒンジ損失\"",
        "Opt6": "\"シルエット係数\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】ロジスティック回帰では「最尤推定法」を用い、手元のデータが得られる確率である尤度（ゆうど）を最大化するように学習します。\""
    },
    {
        "ID": "\"ML-154\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"SVMにおいて、全てのデータがマージンの外側に正しく分類されることを要求する「ハードマージン」の弱点は？\"",
        "Opt1": "\"計算が極めて速すぎること\"",
        "Opt2": "\"外れ値に対して非常に弱く、解が存在しない場合があること\"",
        "Opt3": "\"必ず直線的な境界になること\"",
        "Opt4": "\"次元圧縮が必要なこと\"",
        "Opt5": "\"特徴量選択ができないこと\"",
        "Opt6": "\"バイアスが大きすぎること\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】ハードマージンは1つでも外れ値があると成立しなくなるため、実務では誤分類を許容する「ソフトマージン」が一般的です。\""
    },
    {
        "ID": "\"ML-155\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"決定木の学習において、分割後の各ノードに含まれるサンプルのクラスが均一であればあるほど「低くなる」指標はどれか？\"",
        "Opt1": "\"正解率\"",
        "Opt2": "\"情報利得\"",
        "Opt3": "\"不純度（ジニ係数・エントロピー）\"",
        "Opt4": "\"再現率\"",
        "Opt5": "\"累積寄与率\"",
        "Opt6": "\"決定係数\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】不純度は「混ざり具合」を示すため、均一（純粋）になると0になります。\""
    },
    {
        "ID": "\"ML-156\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"アンサンブル学習の「バギング」において、個々の弱学習器として最も一般的に用いられるアルゴリズムは？\"",
        "Opt1": "\"ロジスティック回帰\"",
        "Opt2": "\"決定木\"",
        "Opt3": "\"SVM\"",
        "Opt4": "\"k-近傍法\"",
        "Opt5": "\"ナイーブベイズ\"",
        "Opt6": "\"主成分分析\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】決定木はバリアンスが高いため、バギング（平均化）による精度向上の恩恵を最も受けやすく、ランダムフォレストの基礎となっています。\""
    },
    {
        "ID": "\"ML-157\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"ランダムフォレストにおいて、個々の決定木を構築する際に「使わない特徴量」をあえて作る理由は？\"",
        "Opt1": "\"メモリを節約するため\"",
        "Opt2": "\"計算速度を極限まで高めるため\"",
        "Opt3": "\"各木の間の相関を下げ、多様性を持たせるため\"",
        "Opt4": "\"特徴量選択を人間が行うため\"",
        "Opt5": "\"欠損値を無視するため\"",
        "Opt6": "\"線形分離をしやすくするため\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】全ての木が同じ強い特徴量ばかり使うと似たような木ばかりになり、アンサンブルの効果が薄れるのを防ぎます。\""
    },
    {
        "ID": "\"ML-158\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"勾配ブースティング（GBDT）が、ランダムフォレストと比較して優れている傾向にある点は？\"",
        "Opt1": "\"並列計算のしやすさ\"",
        "Opt2": "\"外れ値への頑健性\"",
        "Opt3": "\"少ない試行錯誤での収束\"",
        "Opt4": "\"一般的に高い予測精度\"",
        "Opt5": "\"過学習の起きにくさ\"",
        "Opt6": "\"モデルの解釈性\"",
        "Answer_Idx": 3,
        "Explanation": "\"【解説】GBDTは逐次的に誤差を修正するため、適切にチューニングすればランダムフォレストより高い精度を出すことが多いです。\""
    },
    {
        "ID": "\"ML-159\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"k-means法で「クラスタの数 k」を事前に決める必要があるが、kを1から増やしていき損失の変化が緩やかになる点を探す手法は？\"",
        "Opt1": "\"シルエット法\"",
        "Opt2": "\"エルボー法\"",
        "Opt3": "\"ホールドアウト法\"",
        "Opt4": "\"グリッドサーチ\"",
        "Opt5": "\"EMアルゴリズム\"",
        "Opt6": "\"主成分分析\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】グラフの形が「肘（Elbow）」のように折れ曲がる箇所を最適なkとする手法です。\""
    },
    {
        "ID": "\"ML-160\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"主成分分析（PCA）において、元のデータが持っていた分散を最も大きく保持している軸を何というか？\"",
        "Opt1": "\"第1主成分\"",
        "Opt2": "\"第2主成分\"",
        "Opt3": "\"固有値\"",
        "Opt4": "\"固有ベクトル\"",
        "Opt5": "\"寄与率\"",
        "Opt6": "\"負荷量\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】第1主成分が最大の情報（分散）を持ち、第2、第3となるにつれて保持する情報は減っていきます。\""
    },
    {
        "ID": "\"ML-161\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"階層的クラスタリングの「最短距離法」で発生しやすい、クラスタが鎖状に繋がってしまう現象を何というか？\"",
        "Opt1": "\"鎖効果（チェイニング効果）\"",
        "Opt2": "\"次元の呪い\"",
        "Opt3": "\"過学習\"",
        "Opt4": "\"マルチコ\"",
        "Opt5": "\"勾配消失\"",
        "Opt6": "\"ハルシネーション\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】近いもの同士を繋いでいく性質上、細長いクラスタが形成されやすくなる傾向があります。\""
    },
    {
        "ID": "\"ML-162\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"アソシエーション分析において、リフト値が『1より大きい』場合の解釈として正しいものは？\"",
        "Opt1": "\"AとBは全く無関係である\"",
        "Opt2": "\"Aを買う人はBを買わない傾向がある\"",
        "Opt3": "\"Aを買う人はBも買う傾向（正の相関）がある\"",
        "Opt4": "\"Aが売れるとBの在庫が増える\"",
        "Opt5": "\"計算ミスである\"",
        "Opt6": "\"Bを買う人はAを絶対に買わない\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】リフト値 > 1 は、Aの購入がBの購入確率を押し上げていることを示します。\""
    },
    {
        "ID": "\"ML-163\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"強化学習において、エージェントが「将来得られる報酬の期待値」に基づいて行動を決める際、即時報酬を重視するか将来報酬を重視するか決める係数は？\"",
        "Opt1": "\"学習率（α）\"",
        "Opt2": "\"割引率（γ）\"",
        "Opt3": "\"探索率（ε）\"",
        "Opt4": "\"正則化係数（λ）\"",
        "Opt5": "\"モーメンタム\"",
        "Opt6": "\"慣性項\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】割引率が0に近いほど「今すぐもらえる報酬」を重視し、1に近いほど「将来の報酬」を重視します。\""
    },
    {
        "ID": "\"ML-164\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"Q学習（Q-Learning）において、Q値の更新式に含まれる「max Q」という項は何を意味しているか？\"",
        "Opt1": "\"次の状態でとれる行動のうち、最も報酬が低いもの\"",
        "Opt2": "\"次の状態でとれる行動のうち、最もQ値が高いもの（最善手）\"",
        "Opt3": "\"現在とった行動の報酬\"",
        "Opt4": "\"ランダムな行動の結果\"",
        "Opt5": "\"エピソードの終了判定\"",
        "Opt6": "\"過去の平均報酬\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】Q学習は「次も自分は最善を尽くす」という前提（オフポリティ）で、現在の行動価値を更新します。\""
    },
    {
        "ID": "\"ML-165\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"強化学習の「多腕バンディット問題」が焦点を当てている主な課題は？\"",
        "Opt1": "\"計算資源の不足\"",
        "Opt2": "\"状態遷移の複雑さ\"",
        "Opt3": "\"探索と活用のトレードオフ\"",
        "Opt4": "\"画像認識の精度\"",
        "Opt5": "\"報酬の遅延\"",
        "Opt6": "\"マルチエージェントの衝突\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】限られた試行回数の中で、既存の「当たり台（活用）」を打つか、未開拓の「新台（探索）」を試すかのバランスを問う問題です。\""
    },
    {
        "ID": "\"ML-166\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"混同行列において、実際に「負」であるデータのうち、正しく「負」と予測できた割合を何というか？\"",
        "Opt1": "\"再現率\"",
        "Opt2": "\"適合率\"",
        "Opt3": "\"特異度（Specificity）\"",
        "Opt4": "\"F値\"",
        "Opt5": "\"正解率\"",
        "Opt6": "\"AUC\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】特異度は「健康な人を正しく健康と診断できる力」に相当します。\""
    },
    {
        "ID": "\"ML-167\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"適合率（Precision）と再現率（Recall）がトレードオフになる主な理由は？\"",
        "Opt1": "\"どちらも計算式が同じだから\"",
        "Opt2": "\"判定の閾値（境界線）を動かすと、一方が上がればもう一方が下がる傾向にあるから\"",
        "Opt3": "\"学習データが足りないから\"",
        "Opt4": "\"正則化が強すぎるから\"",
        "Opt5": "\"モデルが複雑すぎるから\"",
        "Opt6": "\"カテゴリ変数が多すぎるから\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】例えばスパム判定を厳しくすると、誤検知（Precision向上）は減りますが、本物のスパムの見逃し（Recall低下）が増えるためです。\""
    },
    {
        "ID": "\"ML-168\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"「実際に陽性であるケースが1%しかない」ような不均衡データで、モデルの評価に正解率（Accuracy）を使うのが危険な理由は？\"",
        "Opt1": "\"計算に時間がかかるから\"",
        "Opt2": "\"全てを「陰性」と予測するだけで正解率99%になってしまい、モデルの有用性が測れないから\"",
        "Opt3": "\"F値が常に0になるから\"",
        "Opt4": "\"正解率は1を超えてしまうことがあるから\"",
        "Opt5": "\"数学的に定義できないから\"",
        "Opt6": "\"ROC曲線が描けないから\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】多数派のクラスに引きずられ、本当に見つけたい少数派（病気など）を無視したモデルが高評価されてしまうためです。\""
    },
    {
        "ID": "\"ML-169\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"学習曲線において、訓練データと検証データの誤差がどちらも収束せず非常に高い場合、最初に行うべき対策は？\"",
        "Opt1": "\"データを減らす\"",
        "Opt2": "\"モデルの複雑さを上げる（表現力を高める）\"",
        "Opt3": "\"正則化をさらに強くする\"",
        "Opt4": "\"学習を途中で打ち切る\"",
        "Opt5": "\"特徴量を削る\"",
        "Opt6": "\"次元を圧縮する\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】これは「未学習（アンダーフィッティング）」の状態なので、モデルをより複雑にする、あるいは特徴量を増やす必要があります。\""
    },
    {
        "ID": "\"ML-170\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"ハイパーパラメータの「ランダムサーチ」が「グリッドサーチ」より優れているとされる主な理由は？\"",
        "Opt1": "\"必ず最適な値が見つかるから\"",
        "Opt2": "\"重要なパラメータを集中的に探索できる可能性が高く、計算効率が良いから\"",
        "Opt3": "\"計算が直列で行われるから\"",
        "Opt4": "\"人間が値を指定しなくて良いから\"",
        "Opt5": "\"深層学習にはランダムサーチしか使えないから\"",
        "Opt6": "\"結果の再現性が高いから\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】多くのパラメータの中で「本当に効いている変数」に効率よく当たる確率が高いため、大規模な探索で推奨されます。\""
    },
    {
        "ID": "\"ML-171\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"時系列データの「移動平均（MA）モデル」において、現在の値に影響を与える要素は？\"",
        "Opt1": "\"過去の自分自身の値\"",
        "Opt2": "\"過去の予測誤差（ホワイトノイズ）の重み付き和\"",
        "Opt3": "\"データの季節性成分のみ\"",
        "Opt4": "\"未来の予測値\"",
        "Opt5": "\"ランダムウォークの累積\"",
        "Opt6": "\"外部変数の回帰係数\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】AR（自己回帰）が「過去の値」を使うのに対し、MAは「過去のノイズ（誤差）」の余韻を使います。\""
    },
    {
        "ID": "\"ML-172\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"「k-means法」と「混合ガウスモデル（GMM）」の関係として正しいものは？\"",
        "Opt1": "\"全く無関係である\"",
        "Opt2": "\"GMMはk-meansを確率的に拡張した手法（ソフトクラスタリング）といえる\"",
        "Opt3": "\"k-meansの方が複雑な形状を分類できる\"",
        "Opt4": "\"どちらも教師あり学習である\"",
        "Opt5": "\"k-meansはEMアルゴリズムを使わない\"",
        "Opt6": "\"GMMは重心計算を行わない\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】k-meansは「どれか1つのクラスタに100%所属」させますが、GMMは「Aに70%、Bに30%」という確率的な所属を許容します。\""
    },
    {
        "ID": "\"ML-173\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"SVMの「RBFカーネル」使用時、gammaを極端に小さく設定するとモデルはどうなるか？\"",
        "Opt1": "\"境界が非常に複雑になる\"",
        "Opt2": "\"境界が直線的（単純）になり、未学習に近づく\"",
        "Opt3": "\"マージンが0になる\"",
        "Opt4": "\"サポートベクトルが1つだけになる\"",
        "Opt5": "\"計算が停止する\"",
        "Opt6": "\"正解率が必ず100%になる\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】gammaが小さい＝1つの点の影響範囲が広い＝大まかな（滑らかな）境界線になります。逆に大きいと過学習しやすくなります。\""
    },
    {
        "ID": "\"ML-174\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"「決定木」を弱学習器とする「ブースティング」において、新しい木が重点的に学習するデータは？\"",
        "Opt1": "\"ランダムに選ばれたデータ\"",
        "Opt2": "\"前の木が正解したデータ\"",
        "Opt3": "\"前の木が予測を外した（誤差が大きい）データ\"",
        "Opt4": "\"全ての学習データ\"",
        "Opt5": "\"特徴量が少ないデータ\"",
        "Opt6": "\"ラベルが0のデータのみ\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】前のモデルの失敗を次で補うのがブースティングの基本原理です。\""
    },
    {
        "ID": "\"ML-175\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"主成分分析（PCA）で、ある主成分が持つ情報の割合を示す「寄与率」の算出に使われる数学的な値は？\"",
        "Opt1": "\"平均値\"",
        "Opt2": "\"標準偏差\"",
        "Opt3": "\"固有値\"",
        "Opt4": "\"行列のランク\"",
        "Opt5": "\"決定係数\"",
        "Opt6": "\"相関係数\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】（特定の主成分の固有値）/（固有値の総和）で寄与率が求められます。\""
    },
    {
        "ID": "\"ML-176\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"データの前処理で「外れ値」に対して最も頑健（影響を受けにくい）なスケーリング手法は？\"",
        "Opt1": "\"正規化（Min-Max）\"",
        "Opt2": "\"標準化（Standardization）\"",
        "Opt3": "\"ロバストスケーリング（中央値と四分位範囲を使用）\"",
        "Opt4": "\"対数変換\"",
        "Opt5": "\"二値化\"",
        "Opt6": "\"ワンホットエンコーディング\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】平均や最大最小は外れ値に引きずられますが、中央値や四分位範囲（IQR）を使うRobustScalerは影響を抑えられます。\""
    },
    {
        "ID": "\"ML-177\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"強化学習の「SARSA」が「Q学習」よりも慎重（安全重視）な行動をとる傾向がある理由は？\"",
        "Opt1": "\"割引率が低いから\"",
        "Opt2": "\"実際に選択した（失敗する可能性もある）行動の報酬に基づいて学習するから\"",
        "Opt3": "\"常にランダムに行動するから\"",
        "Opt4": "\"ニューラルネットワークを使わないから\"",
        "Opt5": "\"報酬を与えないから\"",
        "Opt6": "\"エピソードを途中で止めるから\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】SARSAはOn-policy（オンポリティ）であり、「危ない橋を渡って失敗した」経験をそのまま評価に反映するため、崖っぷちを歩くような行動を避けるようになります。\""
    },
    {
        "ID": "\"ML-178\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"ナイーブベイズにおいて、ある単語が一度も訓練データに出てこなかった場合に確率が0になってしまうのを防ぐ手法は？\"",
        "Opt1": "\"正規化\"",
        "Opt2": "\"ラプラススムージング（加算スムージング）\"",
        "Opt3": "\"ドロップアウト\"",
        "Opt4": "\"早期終了\"",
        "Opt5": "\"主成分分析\"",
        "Opt6": "\"アンダーサンプリング\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】全てのカウントに小さな値（1など）を足すことで、未知のデータに対しても計算が破綻しないようにします。\""
    },
    {
        "ID": "\"ML-179\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"「L1正則化（Lasso）」と「L2正則化（Ridge）」を比較した際、L1のみが持つ顕著な特徴は？\"",
        "Opt1": "\"計算が非常に安定すること\"",
        "Opt2": "\"一部の重みを完全に0にすることで、特徴量選択の効果を持つこと\"",
        "Opt3": "\"重みを大きくすること\"",
        "Opt4": "\"外れ値を無視すること\"",
        "Opt5": "\"深層学習でしか使えないこと\"",
        "Opt6": "\"バイアスを最小化すること\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】L1の「角張った」制約条件により、不要な変数の係数が0になりやすく、モデルの解釈性が高まります。\""
    },
    {
        "ID": "\"ML-180\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"「k-近傍法（k-NN）」において、kの値を大きくしすぎるとモデルはどうなるか？\"",
        "Opt1": "\"過学習（高バリアンス）になる\"",
        "Opt2": "\"未学習（高バイアス）になり、多数派のクラスに引きずられる\"",
        "Opt3": "\"計算速度が無限に速くなる\"",
        "Opt4": "\"予測精度が必ず100%になる\"",
        "Opt5": "\"境界線がギザギザになる\"",
        "Opt6": "\"データが削除される\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】kを大きくする＝近所の人をたくさん呼ぶ＝クラスの平均的な意見しか聞かなくなるため、境界が滑らかになりすぎて個別の特徴を無視してしまいます。\""
    },
    {
        "ID": "\"ML-181\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"「次元圧縮」の目的として適切でないものはどれか？\"",
        "Opt1": "\"計算コストの削減\"",
        "Opt2": "\"データの可視化\"",
        "Opt3": "\"過学習の抑制（次元の呪いの回避）\"",
        "Opt4": "\"特徴量間の相関（マルチコ）の解消\"",
        "Opt5": "\"学習データそのものを増やすこと\"",
        "Opt6": "\"情報のノイズ除去\"",
        "Answer_Idx": 4,
        "Explanation": "\"【解説】次元圧縮は情報を削ったり変換したりする技術であり、データ（サンプル数）を増やす効果はありません。\""
    },
    {
        "ID": "\"ML-182\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"強化学習の「DQN」において、行動価値関数Q(s",
        "Opt1": "a)を近似するために用いられるのは？\"",
        "Opt2": "\"決定木\"",
        "Opt3": "\"サポートベクターマシン\"",
        "Opt4": "\"深層ニューラルネットワーク\"",
        "Opt5": "\"k-means\"",
        "Opt6": "\"ロジスティック回帰\"",
        "Answer_Idx": "\"線形回帰\"",
        "Explanation": 2
    },
    {
        "ID": "\"ML-183\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"決定木において、特定の変数で分割した時に得られる「不純度の減少量」の合計を何というか？\"",
        "Opt1": "\"情報利得\"",
        "Opt2": "\"特徴量の重要度（Feature Importance）\"",
        "Opt3": "\"累積寄与率\"",
        "Opt4": "\"F値\"",
        "Opt5": "\"決定係数\"",
        "Opt6": "\"偏回帰係数\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】これを見ることで、どの変数が分類に最も貢献しているかを判断できます。\""
    },
    {
        "ID": "\"ML-184\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"分類問題の評価指標「AUC（Area Under the Curve）」の最大値と最小値は？\"",
        "Opt1": "\"最大1.0 / 最小-1.0\"",
        "Opt2": "\"最大100 / 最小0\"",
        "Opt3": "\"最大1.0 / 最小0\"",
        "Opt4": "\"最大無限 / 最小0\"",
        "Opt5": "\"最大1.0 / 最小0.5（ランダム時）\"",
        "Opt6": "\"最大0.5 / 最小0\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】AUCは0から1の範囲をとり、1に近いほど高性能、0.5がランダム予測、0は予測が全て逆であることを意味します。\""
    },
    {
        "ID": "\"ML-185\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"強化学習の「方策勾配法」のメリットとして、Q学習と比較して優れている点は？\"",
        "Opt1": "\"計算が常に速いこと\"",
        "Opt2": "\"連続的な行動（力の入れ具合など）を直接扱えること\"",
        "Opt3": "\"過去のデータを再利用しやすいこと\"",
        "Opt4": "\"報酬がなくても学習できること\"",
        "Opt5": "\"環境のモデルが必須なこと\"",
        "Opt6": "\"過学習が起きないこと\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】Q学習などの価値ベース手法は「離散的な選択」が得意ですが、方策勾配法は「どの方向にどれくらい」という連続値を確率分布として扱えます。\""
    },
    {
        "ID": "\"ML-186\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"不均衡データにおいて、少ない方のクラスを「コピー」して増やす手法を何というか？\"",
        "Opt1": "\"ランダムオーバーサンプリング\"",
        "Opt2": "\"ランダムアンダーサンプリング\"",
        "Opt3": "\"SMOTE\"",
        "Opt4": "\"主成分分析\"",
        "Opt5": "\"正則化\"",
        "Opt6": "\"ブートストラップ\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】単純に増やすのがオーバーサンプリング、近傍データから合成して増やすのがSMOTEです。\""
    },
    {
        "ID": "\"ML-187\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"主成分分析（PCA）において、各主成分軸が「元の変数」とどれくらい相関しているかを示す指標は？\"",
        "Opt1": "\"固有値\"",
        "Opt2": "\"主成分負荷量（ファクターローディング）\"",
        "Opt3": "\"寄与率\"",
        "Opt4": "\"バイアス\"",
        "Opt5": "\"決定係数\"",
        "Opt6": "\"共分散\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】これを見ることで、「第1主成分は『価格』と『広さ』を強く反映している」といった解釈が可能になります。\""
    },
    {
        "ID": "\"ML-188\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"時系列分析において、平均や分散が時間によって変化しない（一定である）性質を何というか？\"",
        "Opt1": "\"定常性\"",
        "Opt2": "\"ホワイトノイズ\"",
        "Opt3": "\"季節性\"",
        "Opt4": "\"トレンド\"",
        "Opt5": "\"自己回帰性\"",
        "Opt6": "\"ランダムウォーク\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】多くの時系列モデルは、データに「定常性」があることを前提として計算を行います。\""
    },
    {
        "ID": "\"ML-189\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"アソシエーション分析の「確信度（Confidence）」について、A→Bの確信度とB→Aの確信度はどうなるか？\"",
        "Opt1": "\"常に一致する\"",
        "Opt2": "\"一般的に一致しない\"",
        "Opt3": "\"足すと1になる\"",
        "Opt4": "\"掛け合わせるとリフト値になる\"",
        "Opt5": "\"どちらかが0になる\"",
        "Opt6": "\"常に0.5になる\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】「おむつを買う人がビールを買う確率」と「ビールを買う人がおむつを買う確率」は分母（単独の購入数）が違うため、通常は異なります。\""
    },
    {
        "ID": "\"ML-190\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"ロジスティック回帰の出力値を0から1の範囲（確率）に変換するために用いられる関数は？\"",
        "Opt1": "\"ReLU関数\"",
        "Opt2": "\"シグモイド関数\"",
        "Opt3": "\"ソフトマックス関数\"",
        "Opt4": "\"サイン関数\"",
        "Opt5": "\"ステップ関数\"",
        "Opt6": "\"恒等関数\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】シグモイド関数により、実数全体を(0"
    },
    {
        "ID": "[Image of Sigmoid function curve]",
        "Category": "",
        "Question": "",
        "Opt1": "",
        "Opt2": "",
        "Opt3": "",
        "Opt4": "",
        "Opt5": "",
        "Opt6": "",
        "Answer_Idx": "",
        "Explanation": ""
    },
    {
        "ID": "\"",
        "Category": "",
        "Question": "",
        "Opt1": "",
        "Opt2": "",
        "Opt3": "",
        "Opt4": "",
        "Opt5": "",
        "Opt6": "",
        "Answer_Idx": "",
        "Explanation": ""
    },
    {
        "ID": "\"ML-191\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"アンサンブル学習において、複数の弱学習器が「互いに独立（間違い方がバラバラ）」であるほど、全体の誤差はどうなるか？\"",
        "Opt1": "\"大きくなる\"",
        "Opt2": "\"小さくなる\"",
        "Opt3": "\"変わらない\"",
        "Opt4": "\"0になる\"",
        "Opt5": "\"計算不能になる\"",
        "Opt6": "\"1になる\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】みんなが同じところで間違えると意味がありません。多様性があるほど、多数決や平均の効果でエラーが打ち消し合います。\""
    },
    {
        "ID": "\"ML-192\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"SVMにおいて、カーネルトリックを使わずに「線形カーネル」のみを使用する場合、それは実質的にどのモデルに近いか？\"",
        "Opt1": "\"決定木\"",
        "Opt2": "\"単純な線形判別分析（マージン最大化付き）\"",
        "Opt3": "\"k-means\"",
        "Opt4": "\"ナイーブベイズ\"",
        "Opt5": "\"RNN\"",
        "Opt6": "\"主成分分析\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】次元を上げずに直線を引くため、基本的な線形分類器として動作します。\""
    },
    {
        "ID": "\"ML-193\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"「ノーフリーランチ定理」が機械学習エンジニアに示唆する実務上の教訓は？\"",
        "Opt1": "\"特定の最強アルゴリズムを信じ、それだけを使えば良い\"",
        "Opt2": "\"問題に応じて複数の手法を試し、最適なものを選択すべきである\"",
        "Opt3": "\"データさえ多ければアルゴリズムは何でも良い\"",
        "Opt4": "\"無料のライブラリは使わない方が良い\"",
        "Opt5": "\"ディープラーニングが常に最強である\"",
        "Opt6": "\"理論よりも計算機の性能が重要である\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】あらゆる問題に万能な手法はないため、試行錯誤（実験）の重要性を説いています。\""
    },
    {
        "ID": "\"ML-194\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"「k-means法」において、クラスタの重心（セントロイド）が移動しなくなる状態を何というか？\"",
        "Opt1": "\"過学習\"",
        "Opt2": "\"収束\"",
        "Opt3": "\"次元の呪い\"",
        "Opt4": "\"剪定\"",
        "Opt5": "\"勾配消失\"",
        "Opt6": "\"早期終了\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】割り当てと更新を繰り返し、変化がなくなった時点でアルゴリズムは終了します。\""
    },
    {
        "ID": "\"ML-195\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"多クラス分類において、あるクラスに対する「適合率」を計算する際の分母はどれか？\"",
        "Opt1": "\"全データ数\"",
        "Opt2": "\"そのクラスと予測したデータの総数\"",
        "Opt3": "\"実際にそのクラスであるデータの総数\"",
        "Opt4": "\"正解したデータの総数\"",
        "Opt5": "\"間違えたデータの総数\"",
        "Opt6": "\"他のクラスと予測したデータの総数\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】適合率 ＝ 本当にそうだった数 / 「そうだ」と予測した数 です。\""
    },
    {
        "ID": "\"ML-196\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"モデルの評価指標「F値」が、単純な平均ではなく「調和平均」を用いる理由は？\"",
        "Opt1": "\"計算が簡単だから\"",
        "Opt2": "\"適合率と再現率のどちらか一方が極端に低い場合に、評価を厳しく（値を小さく）するため\"",
        "Opt3": "\"値を1以上に大きくするため\"",
        "Opt4": "\"外れ値を無視するため\"",
        "Opt5": "\"データ数に依存させないため\"",
        "Opt6": "\"バイアスを測るため\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】例えば適合率1.0、再現率0.01の場合、算術平均だと約0.5になりますが、調和平均（F値）だとほぼ0になり、モデルの欠陥を正しく示せます。\""
    },
    {
        "ID": "\"ML-197\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"強化学習において、エージェントが「環境」に対して行う働きかけを何というか？\"",
        "Opt1": "\"状態\"",
        "Opt2": "\"報酬\"",
        "Opt3": "\"行動（アクション）\"",
        "Opt4": "\"方策\"",
        "Opt5": "\"価値\"",
        "Opt6": "\"割引率\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】エージェントが行動し、それに対して環境が「新しい状態」と「報酬」を返します。\""
    },
    {
        "ID": "\"ML-198\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"「Lasso回帰（L1）」において、正則化の強さを決めるパラメータを極限まで大きくすると、回帰係数はどうなるか？\"",
        "Opt1": "\"無限に大きくなる\"",
        "Opt2": "\"全て0になる\"",
        "Opt3": "\"平均値に収束する\"",
        "Opt4": "\"1になる\"",
        "Opt5": "\"ランダムな値になる\"",
        "Opt6": "\"変化しない\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】ペナルティが強すぎて、全ての変数の影響を排除（係数を0に）しようとします。\""
    },
    {
        "ID": "\"ML-199\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"決定木において、分割を途中で止める「最大深さ（max_depth）」の制限は、どのような効果があるか？\"",
        "Opt1": "\"学習を速くするが、精度は必ず下がる\"",
        "Opt2": "\"過学習を抑制し、汎化性能を高める（正則化の効果）\"",
        "Opt3": "\"未学習を促進するだけである\"",
        "Opt4": "\"次元を増やす効果がある\"",
        "Opt5": "\"欠損値を補完する\"",
        "Opt6": "\"バイアスを0にする\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】木が細かくなりすぎる（個別のデータに特化しすぎる）のを防ぎ、シンプルなモデルに保ちます。\""
    },
    {
        "ID": "\"ML-200\"",
        "Category": "\"3.機械学習手法\"",
        "Question": "\"機械学習のプロジェクトにおいて、テストデータを使って何度もハイパーパラメータを調整してしまうことで起きる問題は？\"",
        "Opt1": "\"データの不足\"",
        "Opt2": "\"テストデータへの過学習（情報のリーク）\"",
        "Opt3": "\"モデルの未学習\"",
        "Opt4": "\"計算コストの増大\"",
        "Opt5": "\"正解率の低下\"",
        "Opt6": "\"手法の陳腐化\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】テストデータは「最後の最後」まで見てはいけません。調整に使うと、そのテストデータにだけ強いモデルになり、本当の未知データに通用しなくなります。\""
    },
    {
        "ID": "\"DL-001\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"1950年代に提案された、入力層と出力層のみからなり線形分離可能な問題しか解けないモデルを何というか？\"",
        "Opt1": "\"単純パーセプトロン\"",
        "Opt2": "\"マルチレイヤーパーセプトロン\"",
        "Opt3": "\"シグモイドニューロン\"",
        "Opt4": "\"ロジスティック回帰\"",
        "Opt5": "\"ボルツマンマシン\"",
        "Opt6": "\"畳み込みニューラルネットワーク\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】フランク・ローゼンブラットが提案。XOR問題のような線形非分離な問題を解けないことがマービン・ミンスキーらによって指摘されました。\""
    },
    {
        "ID": "\"DL-002\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"1960年代、単純パーセプトロンがXOR（排他的論理和）問題を解けないことを数学的に証明し、第一次AIブームを終焉させた人物は？\"",
        "Opt1": "\"ヤン・ルカン\"",
        "Opt2": "\"ジェフリー・ヒントン\"",
        "Opt3": "\"マービン・ミンスキー\"",
        "Opt4": "\"フランク・ローゼンブラット\"",
        "Opt5": "\"アンドリュー・ン\"",
        "Opt6": "\"ヨシュア・ベンジオ\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】ミンスキーとパパートが著書『パーセプトロン』で指摘。これにより冬の時代が始まりました。\""
    },
    {
        "ID": "\"DL-003\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"多層パーセプトロンにおいて、誤差を出力層から入力層に向かって逆方向に伝播させることで重みを更新するアルゴリズムは？\"",
        "Opt1": "\"勾配降下法\"",
        "Opt2": "\"誤差逆伝播法（バックプロパゲーション）\"",
        "Opt3": "\"順伝播\"",
        "Opt4": "\"ヘブ則\"",
        "Opt5": "\"Q学習\"",
        "Opt6": "\"畳み込み\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】1986年にデビッド・ラメルハートらによって広められました。連鎖律（チェインルール）を利用して各層の勾配を計算します。\""
    },
    {
        "ID": "\"DL-004\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"ディープラーニングにおいて、出力値を次の層に渡す際に非線形な変換を行う関数を総称して何というか？\"",
        "Opt1": "\"損失関数\"",
        "Opt2": "\"目的関数\"",
        "Opt3": "\"活性化関数\"",
        "Opt4": "\"ステップ関数\"",
        "Opt5": "\"シグモイド関数\"",
        "Opt6": "\"恒等関数\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】Activation Function。これが非線形であることで、ニューラルネットワークは複雑な関数を近似できるようになります。\""
    },
    {
        "ID": "\"DL-005\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"シグモイド関数を活性化関数に用いた際、層が深くなるにつれて勾配が0に近づき、学習が進まなくなる現象を何というか？\"",
        "Opt1": "\"勾配爆発\"",
        "Opt2": "\"勾配消失問題\"",
        "Opt3": "\"オーバーフィッティング\"",
        "Opt4": "\"アンダーフィッティング\"",
        "Opt5": "\"局所最適解\"",
        "Opt6": "\"鞍点\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】シグモイド関数の微分値の最大値が0.25であるため、層を重ねるごとに掛け算の結果が0に収束してしまいます。\""
    },
    {
        "ID": "\"DL-006\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"勾配消失問題の解決策として、現在のディープラーニングで最も一般的に中間層で利用される活性化関数は？\"",
        "Opt1": "\"シグモイド関数\"",
        "Opt2": "\"ReLU（Rectified Linear Unit）\"",
        "Opt3": "\"tanh（ハイパボリックタンジェント）\"",
        "Opt4": "\"ソフトマックス関数\"",
        "Opt5": "\"ステップ関数\"",
        "Opt6": "\"恒等関数\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】入力が0より大きければ勾配が1のまま伝わるため、勾配消失が起きにくく、計算も高速です。[図解：x>0で傾き45度の直線、x<0で0になるグラフ]\""
    },
    {
        "ID": "\"DL-007\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"多クラス分類問題の出力層において、各クラスの予測確率の合計が1になるように変換する関数は？\"",
        "Opt1": "\"ReLU関数\"",
        "Opt2": "\"シグモイド関数\"",
        "Opt3": "\"ソフトマックス関数\"",
        "Opt4": "\"tanh関数\"",
        "Opt5": "\"ステップ関数\"",
        "Opt6": "\"符号関数\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】Softmax関数。各出力の指数（exp）をとり、その合計で割ることで「確率」として解釈可能な形式にします。\""
    },
    {
        "ID": "\"DL-008\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"2値分類（Yes/No）の出力層において、出力を0から1の範囲に押し込めるために用いられる関数は？\"",
        "Opt1": "\"ReLU関数\"",
        "Opt2": "\"シグモイド関数\"",
        "Opt3": "\"ソフトマックス関数\"",
        "Opt4": "\"恒等関数\"",
        "Opt5": "\"ステップ関数\"",
        "Opt6": "\"tanh関数\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】Sigmoid関数。0.5を閾値として分類を行うのが一般的です。[図解：S字型のカーブ]\""
    },
    {
        "ID": "\"DL-009\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"誤差逆伝播法の数理的な基礎となる、合成関数の微分を求めるための法則を何というか？\"",
        "Opt1": "\"ベイズの法則\"",
        "Opt2": "\"連鎖律（チェインルール）\"",
        "Opt3": "\"大数の法則\"",
        "Opt4": "\"中心極限定理\"",
        "Opt5": "\"ラプラスの悪魔\"",
        "Opt6": "\"ムーアの法則\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】y=f(g(x))の微分をdy/dg * dg/dxとして計算する手法。これが多層ネットワークの勾配計算を可能にします。\""
    },
    {
        "ID": "\"DL-1010\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"ニューラルネットワークが学習する際に最小化しようとする、予測値と正解値のズレを定義する関数は？\"",
        "Opt1": "\"活性化関数\"",
        "Opt2": "\"損失関数（誤差関数）\"",
        "Opt3": "\"獲得関数\"",
        "Opt4": "\"価値関数\"",
        "Opt5": "\"ステップ関数\"",
        "Opt6": "\"固有関数\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】Loss Function。回帰なら二乗誤差、分類なら交差エントロピーがよく使われます。\""
    },
    {
        "ID": "\"DL-011\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"回帰問題において、一般的に損失関数として用いられるのはどれか？\"",
        "Opt1": "\"平均二乗誤差（MSE）\"",
        "Opt2": "\"交差エントロピー誤差\"",
        "Opt3": "\"ヒンジ損失\"",
        "Opt4": "\"ジニ不純度\"",
        "Opt5": "\"L1正則化項\"",
        "Opt6": "\"対数尤度\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】予測値と正解の差を二乗して平均したもの。誤差が大きいほど大きなペナルティを与えます。\""
    },
    {
        "ID": "\"DL-012\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"分類問題において、一般的に損失関数として用いられるのはどれか？\"",
        "Opt1": "\"平均二乗誤差\"",
        "Opt2": "\"交差エントロピー誤差\"",
        "Opt3": "\"絶対値誤差（MAE）\"",
        "Opt4": "\"決定係数\"",
        "Opt5": "\"相関係数\"",
        "Opt6": "\"ハブア損失\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】Cross Entropy Loss。予測確率分布と正解ラベルの分布の「距離」を測る指標です。\""
    },
    {
        "ID": "\"DL-013\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"勾配降下法において、一度の更新に「全学習データ」を使用して勾配を計算する手法を何というか？\"",
        "Opt1": "\"確率的勾配降下法（SGD）\"",
        "Opt2": "\"ミニバッチ勾配降下法\"",
        "Opt3": "\"バッチ勾配降下法\"",
        "Opt4": "\"オンライン学習\"",
        "Opt5": "\"Adam\"",
        "Opt6": "\"Momentum\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】計算が安定しますが、メモリ消費が大きく、局所解（ローカルミニマム）に陥りやすい欠点があります。\""
    },
    {
        "ID": "\"DL-014\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"学習データをいくつかの小さな塊に分け、その塊ごとに勾配を計算して更新を行う、現代の主流な手法は？\"",
        "Opt1": "\"バッチ勾配降下法\"",
        "Opt2": "\"ミニバッチ勾配降下法\"",
        "Opt3": "\"フルバッチ学習\"",
        "Opt4": "\"逐次学習\"",
        "Opt5": "\"アンサンブル学習\"",
        "Opt6": "\"強化学習\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】計算効率と学習の安定性のバランスが良く、GPUの並列計算能力を活かせるため最も多用されます。\""
    },
    {
        "ID": "\"DL-015\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"確率的勾配降下法（SGD）において、勾配の更新に「慣性（前回の更新方向）」を加えることで振動を抑える手法は？\"",
        "Opt1": "\"AdaGrad\"",
        "Opt2": "\"RMSprop\"",
        "Opt3": "\"Momentum（慣性）\"",
        "Opt4": "\"Adam\"",
        "Opt5": "\"L2正則化\"",
        "Opt6": "\"ドロップアウト\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】ボールが坂を転がり落ちるように、過去の勾配の方向を維持して加速・安定化させます。\""
    },
    {
        "ID": "\"DL-016\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"学習が進むにつれて、頻繁に更新される重みの学習率を自動的に下げ、あまり更新されない重みの学習率を維持する手法は？\"",
        "Opt1": "\"Momentum\"",
        "Opt2": "\"AdaGrad\"",
        "Opt3": "\"SGD\"",
        "Opt4": "\"初期化\"",
        "Opt5": "\"正則化\"",
        "Opt6": "\"バッチ正規化\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】パラメータごとに学習率を調整する「適応的学習率」の先駆けです。後半に学習率が下がりすぎて止まる欠点があります。\""
    },
    {
        "ID": "\"DL-017\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"Momentumの「慣性」とAdaGrad/RMSpropの「学習率の適応的調整」を組み合わせた、現在最も人気の高い最適化アルゴリズムは？\"",
        "Opt1": "\"SGD\"",
        "Opt2": "\"Adam\"",
        "Opt3": "\"Nesterov\"",
        "Opt4": "\"AdaDelta\"",
        "Opt5": "\"SMOTE\"",
        "Opt6": "\"L-BFGS\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】Adaptive Moment Estimation。ハイパーパラメータがデフォルトのままでも良好な結果が出やすいため、第一選択肢となります。\""
    },
    {
        "ID": "\"DL-018\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"中間層の各層において、活性化関数を適用する直前（または直後）に値を「平均0、分散1」に近い形に整える技術は？\"",
        "Opt1": "\"レイヤー正規化\"",
        "Opt2": "\"バッチ正規化（Batch Normalization）\"",
        "Opt3": "\"インスタンス正規化\"",
        "Opt4": "\"重みの初期化\"",
        "Opt5": "\"ドロップアウト\"",
        "Opt6": "\"早期終了\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】学習速度の向上、勾配消失の抑制、初期値への依存度低減など多くのメリットがあります。\""
    },
    {
        "ID": "\"DL-019\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"過学習を防ぐために、学習の際にランダムに一定割合のニューロンを無効化（不活性化）する手法を何というか？\"",
        "Opt1": "\"L1正則化\"",
        "Opt2": "\"L2正則化\"",
        "Opt3": "\"ドロップアウト（Dropout）\"",
        "Opt4": "\"重み減衰\"",
        "Opt5": "\"早期終了\"",
        "Opt6": "\"データ拡張\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】特定のニューロン間の過度な「共適応」を防ぎ、アンサンブル学習に近い効果を得られます。\""
    },
    {
        "ID": "\"DL-020\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"損失関数に「重みの絶対値の和」を加えることで、不要な重みを0にし、特徴選択のような効果を持たせる正則化は？\"",
        "Opt1": "\"L1正則化（Lasso）\"",
        "Opt2": "\"L2正則化（Ridge）\"",
        "Opt3": "\"エラスティックネット\"",
        "Opt4": "\"重み減衰\"",
        "Opt5": "\"バッチ正規化\"",
        "Opt6": "\"ドロップアウト\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】重みが「スパース（疎）」になります。ダイヤモンド型の制約領域を持つのが特徴です。\""
    },
    {
        "ID": "\"DL-021\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"損失関数に「重みの二乗和」を加えることで、重みが極端に大きな値になるのを防ぎ、滑らかなモデルにする正則化は？\"",
        "Opt1": "\"L1正則化\"",
        "Opt2": "\"L2正則化（Weight Decay）\"",
        "Opt3": "\"エラスティックネット\"",
        "Opt4": "\"早期終了\"",
        "Opt5": "\"勾配クリッピング\"",
        "Opt6": "\"データ拡張\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】ディープラーニングで「重み減衰（Weight Decay）」といえば通常このL2正則化を指します。\""
    },
    {
        "ID": "\"DL-022\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"検証データ（Validation set）の誤差を監視し、誤差が改善されなくなった時点で学習を強制終了させる手法は？\"",
        "Opt1": "\"早期終了（Early Stopping）\"",
        "Opt2": "\"ドロップアウト\"",
        "Opt3": "\"バッチ正規化\"",
        "Opt4": "\"勾配降下法\"",
        "Opt5": "\"バックプロパゲーション\"",
        "Opt6": "\"過学習\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】エポック数をハイパーパラメータとして最適化する手間を省き、過学習を未然に防ぎます。\""
    },
    {
        "ID": "\"DL-023\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"ニューラルネットワークの初期値として、シグモイド関数やtanh関数に適しているとされる「層のノード数」を考慮した初期化法は？\"",
        "Opt1": "\"Heの初期化\"",
        "Opt2": "\"Xavier（ザビエル）の初期化\"",
        "Opt3": "\"ゼロ初期化\"",
        "Opt4": "\"ランダム初期化（平均0",
        "Opt5": "分散1）\"",
        "Opt6": "\"定数初期化\"",
        "Answer_Idx": "\"直交初期化\"",
        "Explanation": 1
    },
    {
        "ID": "\"DL-024\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"活性化関数にReLUを用いる際に最も適している、分散を「2/前層のノード数」とする初期化法は？\"",
        "Opt1": "\"Xavierの初期化\"",
        "Opt2": "\"Heの初期化\"",
        "Opt3": "\"ガウス分布初期化\"",
        "Opt4": "\"一様分布初期化\"",
        "Opt5": "\"ゼロ初期化\"",
        "Opt6": "\"単位行列初期化\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】ReLUでは負の入力が0になるため、Xavierより大きな分散が必要となります。\""
    },
    {
        "ID": "\"DL-025\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"全結合層のみからなるネットワークにおいて、入力データの形状（28x28の画像など）をどう処理する必要があるか？\"",
        "Opt1": "\"そのまま入力する\"",
        "Opt2": "\"1次元のベクトル（784）に平滑化（Flatten）する\"",
        "Opt3": "\"3次元テンソルのまま扱う\"",
        "Opt4": "\"RGBに分解して別々に学習する\"",
        "Opt5": "\"主成分分析で10次元に減らす\"",
        "Opt6": "\"反転させて入力する\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】全結合層（Affine層）は空間的な位置関係を保持できないため、1列に並べ直す必要があります。\""
    },
    {
        "ID": "\"DL-026\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"勾配降下法において、重みの更新量を決定する、勾配にかける小さな定数を何というか？\"",
        "Opt1": "\"バッチサイズ\"",
        "Opt2": "\"学習率（ラーニングレート）\"",
        "Opt3": "\"エポック数\"",
        "Opt4": "\"モメンタム係数\"",
        "Opt5": "\"減衰率\"",
        "Opt6": "\"バイアス\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】これが大きすぎると発散し、小さすぎると学習がいつまでも終わりません。\""
    },
    {
        "ID": "\"DL-027\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"学習データを全て1回ずつ使い切る学習単位を何というか？\"",
        "Opt1": "\"ステップ\"",
        "Opt2": "\"イテレーション\"",
        "Opt3": "\"エポック\"",
        "Opt4": "\"バッチ\"",
        "Opt5": "\"チャンク\"",
        "Opt6": "\"フレーム\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】通常、ディープラーニングでは数十〜数百エポック学習を繰り返します。\""
    },
    {
        "ID": "\"DL-028\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"「鞍点（あんてん：Saddle point）」とはどのような状態を指すか？\"",
        "Opt1": "\"全ての方向で極小となっている点\"",
        "Opt2": "\"ある方向からは極大、別の方向からは極小となっている点\"",
        "Opt3": "\"微分が不可能な点\"",
        "Opt4": "\"勾配が無限大になる点\"",
        "Opt5": "\"損失が0になる理想的な点\"",
        "Opt6": "\"データが1つもない領域\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】高次元空間では局所最適解よりも鞍点に捕まることの方が学習の停滞原因として多いと言われています。\""
    },
    {
        "ID": "\"DL-029\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"勾配消失問題とは逆に、勾配が非常に大きな値になり、重みが爆発的に増大してしまう現象の対策として有効なのは？\"",
        "Opt1": "\"ドロップアウト\"",
        "Opt2": "\"勾配クリッピング\"",
        "Opt3": "\"L1正則化\"",
        "Opt4": "\"ReLUの使用\"",
        "Opt5": "\"ゼロ初期化\"",
        "Opt6": "\"データ拡張\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】Gradient Clipping。勾配がある閾値を超えたら、その閾値内に収まるように縮小させます。RNNなどで重要です。\""
    },
    {
        "ID": "\"DL-030\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"画像認識において、画像を左右反転させたり、一部を切り抜いたり（クロップ）してデータを増やす手法を何というか？\"",
        "Opt1": "\"データ拡張（Data Augmentation）\"",
        "Opt2": "\"正規化\"",
        "Opt3": "\"標準化\"",
        "Opt4": "\"アンダーサンプリング\"",
        "Opt5": "\"オーバーサンプリング\"",
        "Opt6": "\"ドロップアウト\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】モデルの汎化性能を高めるために非常に有効です。見かけ上のデータ数を数倍〜数十倍に増やせます。\""
    },
    {
        "ID": "\"DL-031\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"ディープラーニングが既存の機械学習手法と一線を画す最大の利点は何か？\"",
        "Opt1": "\"計算速度が圧倒的に速い\"",
        "Opt2": "\"特徴量エンジニアリング（特徴抽出）を自動で行える\"",
        "Opt3": "\"少量のデータでも高い精度が出る\"",
        "Opt4": "\"数学的な理論が完璧に解明されている\"",
        "Opt5": "\"CPUだけで学習が可能である\"",
        "Opt6": "\"必ず正解率100%が出る\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】「表現学習」とも呼ばれ、人間が特徴を指定しなくてもデータから重要な要素を自ら見つけ出します。\""
    },
    {
        "ID": "\"DL-032\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"2012年の画像認識コンペILSVRCで圧勝し、ディープラーニングブームの火付け役となったモデルは？\"",
        "Opt1": "\"LeNet\"",
        "Opt2": "\"AlexNet\"",
        "Opt3": "\"ResNet\"",
        "Opt4": "\"VGG16\"",
        "Opt5": "\"GoogLeNet\"",
        "Opt6": "\"RNN\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】ジェフリー・ヒントン教授らのチームが開発。GPUの使用やドロップアウト、ReLUを採用していました。\""
    },
    {
        "ID": "\"DL-033\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"ニューラルネットワークの各ニューロンが持つ「重み」以外に、入力の総和に加算される定数項を何というか？\"",
        "Opt1": "\"係数\"",
        "Opt2": "\"バイアス\"",
        "Opt3": "\"切片\"",
        "Opt4": "\"スラック変数\"",
        "Opt5": "\"誤差\"",
        "Opt6": "\"ウェイト\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】活性化関数が発火する「しきい値」を調整する役割を持ちます。\""
    },
    {
        "ID": "\"DL-034\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"多層ネットワークにおいて、活性化関数がない（すべて恒等関数である）場合、どうなるか？\"",
        "Opt1": "\"学習が非常に速くなる\"",
        "Opt2": "\"何層重ねても単層の線形モデルと同じ表現力しか持たない\"",
        "Opt3": "\"非線形な境界が引けるようになる\"",
        "Opt4": "\"勾配消失が絶対に起きない\"",
        "Opt5": "\"精度が100%になる\"",
        "Opt6": "\"重みが0に収束する\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】線形変換を何度繰り返しても1回の線形変換（行列計算）に集約できてしまうため、層を重ねる意味がなくなります。\""
    },
    {
        "ID": "\"DL-035\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"ReLU関数の微分値（入力x > 0のとき）はいくらか？\"",
        "Opt1": "\"x\"",
        "Opt2": "\"x^2\"",
        "Opt3": "\"0\"",
        "Opt4": "\"1\"",
        "Opt5": "\"0.5\"",
        "Opt6": "\"e^x\"",
        "Answer_Idx": 3,
        "Explanation": "\"【解説】微分値が1であるため、多層になっても勾配が減衰せず、後ろの層まで情報が伝わります。\""
    },
    {
        "ID": "\"DL-036\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"中間層の活性化関数としてtanh（ハイパボリックタンジェント）がシグモイド関数より優れている点は？\"",
        "Opt1": "\"計算が単純である\"",
        "Opt2": "\"出力が「0中心」であるため、学習が効率的になりやすい\"",
        "Opt3": "\"微分値が常に1である\"",
        "Opt4": "\"過学習を防ぐ効果がある\"",
        "Opt5": "\"負の値をすべて0にする\"",
        "Opt6": "\"確率として解釈できる\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】シグモイドは0〜1ですが、tanhは-1〜1を出力するため、各層の出力が平均0に近くなり、勾配の更新が安定します。\""
    },
    {
        "ID": "\"DL-037\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"パラメータの更新において、最新の勾配を重視し、過去の勾配の影響を指数関数的に減衰させて移動平均をとる手法は？\"",
        "Opt1": "\"AdaGrad\"",
        "Opt2": "\"RMSprop\"",
        "Opt3": "\"SGD\"",
        "Opt4": "\"Momentum\"",
        "Opt5": "\"バッチ正規化\"",
        "Opt6": "\"初期化\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】ジェフリー・ヒントンが講義で紹介した手法。AdaGradで学習率が下がりすぎる問題を解決しました。\""
    },
    {
        "ID": "\"DL-038\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"重みの初期値をすべて「0」に設定して学習を開始するとどうなるか？\"",
        "Opt1": "\"理想的な学習が始まる\"",
        "Opt2": "\"全てのニューロンが同じ更新をされ、多層にする意味がなくなる\"",
        "Opt3": "\"計算機がフリーズする\"",
        "Opt4": "\"勾配が無限大になる\"",
        "Opt5": "\"過学習が起きる\"",
        "Opt6": "\"精度がランダム（0.5）で固定される\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】「対称性の崩壊」が起きず、全ての重みが同じ値のまま変化するため、多様な特徴を学習できません。\""
    },
    {
        "ID": "\"DL-039\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"ディープラーニングの「汎化性能（はんかせいのう）」とは何か？\"",
        "Opt1": "\"学習データに対してどれだけ高い精度を出せるか\"",
        "Opt2": "\"未知のデータ（テストデータ）に対してどれだけ正しく予測できるか\"",
        "Opt3": "\"計算速度がいかに速いか\"",
        "Opt4": "\"モデルの層がいかに深いか\"",
        "Opt5": "\"GPUをどれだけ効率よく使えるか\"",
        "Opt6": "\"学習がどれだけ早く収束するか\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】「訓練データにだけ強い（過学習）」のではなく、実社会のデータに通用する能力のことです。\""
    },
    {
        "ID": "\"DL-040\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"ニューラルネットワークのパラメータ（重みやバイアス）ではなく、人間があらかじめ設定する「学習率」や「層の数」などを何というか？\"",
        "Opt1": "\"内部パラメータ\"",
        "Opt2": "\"ハイパーパラメータ\"",
        "Opt3": "\"定数\"",
        "Opt4": "\"メタデータ\"",
        "Opt5": "\"ダミー変数\"",
        "Opt6": "\"説明変数\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】これらを最適化するために、グリッドサーチやベイズ最適化などの手法が使われます。\""
    },
    {
        "ID": "\"DL-041\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"ディープラーニングの学習において、1つのバッチの勾配計算に複数のGPUを使い、計算を分担させる手法を何というか？\"",
        "Opt1": "\"モデル並列\"",
        "Opt2": "\"データ並列\"",
        "Opt3": "\"垂直並列\"",
        "Opt4": "\"パイプライン並列\"",
        "Opt5": "\"アンサンブル\"",
        "Opt6": "\"蒸留\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】データを分割して各GPUで計算し、最後に勾配を同期させる手法。大規模学習では必須です。\""
    },
    {
        "ID": "\"DL-042\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"1つの巨大なモデルを複数のGPUに分割して配置し、各パーツを計算させる手法を何というか？\"",
        "Opt1": "\"データ並列\"",
        "Opt2": "\"モデル並列\"",
        "Opt3": "\"タスク並列\"",
        "Opt4": "\"分散学習\"",
        "Opt5": "\"転移学習\"",
        "Opt6": "\"ファインチューニング\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】モデルが1つのGPUのメモリに乗り切らないほど巨大な場合（LLMなど）に用いられます。\""
    },
    {
        "ID": "\"DL-043\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"ドロップアウトは「学習時」と「推論（テスト）時」で挙動が異なる。推論時の正しい挙動は？\"",
        "Opt1": "\"ランダムにニューロンを無効化する\"",
        "Opt2": "\"全てのニューロンを有効にするが、出力を学習時の生存率で調整する\"",
        "Opt3": "\"全てのニューロンを無効化する\"",
        "Opt4": "\"学習時と同じ割合で無効化する\"",
        "Opt5": "\"学習率を2倍にする\"",
        "Opt6": "\"何もしない（そのまま出力する）\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】推論時はアンサンブル平均をとるような効果を狙い、全ての接続を使いますが、出力値の大きさを合わせるために調整が必要です。\""
    },
    {
        "ID": "\"DL-044\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"バッチ正規化（Batch Norm）において、平均と分散を計算する単位は？\"",
        "Opt1": "\"全学習データ\"",
        "Opt2": "\"ミニバッチ内のサンプル\"",
        "Opt3": "\"1つのサンプル内のチャンネル\"",
        "Opt4": "\"全ネットワークの全重み\"",
        "Opt5": "\"層ごとのパラメータ数\"",
        "Opt6": "\"時間軸方向\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】ミニバッチごとに統計量をとることで、層間の「内部共変量シフト」を抑制します。\""
    },
    {
        "ID": "\"DL-045\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"「普遍性定理（Universal Approximation Theorem）」が示す内容はどれか？\"",
        "Opt1": "\"どんなに深い層でも勾配は消失しない\"",
        "Opt2": "\"1つ以上の中間層を持つニューラルネットワークは、任意の連続関数を近似できる\"",
        "Opt3": "\"ディープラーニングは全ての計算を代替できる\"",
        "Opt4": "\"データが多いほど精度は無限に上がる\"",
        "Opt5": "\"学習率は0.01が最適である\"",
        "Opt6": "\"GPUは学習を100倍速くする\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】理論上、十分に広い中間層があればどんな複雑な関数も再現できるという証明です（深さではなく広さに関する定理）。\""
    },
    {
        "ID": "\"DL-046\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"中間層のノード数を入力層よりも極端に少なくすることで、データの重要な情報を抽出（圧縮）するネットワークを何というか？\"",
        "Opt1": "\"CNN\"",
        "Opt2": "\"RNN\"",
        "Opt3": "\"自己符号化器（オートエンコーダ）\"",
        "Opt4": "\"GAN\"",
        "Opt5": "\"トランスフォーマー\"",
        "Opt6": "\"ResNet\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】入力と同じものを出力するように学習させることで、中間層（ボトルネック）に濃縮された特徴が得られます。\""
    },
    {
        "ID": "\"DL-047\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"損失関数の形状が「谷底のように平らで広い（Flat Minimum）」場合、どのようなメリットがあると言われているか？\"",
        "Opt1": "\"学習が高速になる\"",
        "Opt2": "\"汎化性能が高く、データの僅かな変化に強い（堅牢である）\"",
        "Opt3": "\"過学習しやすくなる\"",
        "Opt4": "\"計算エラーが起きにくい\"",
        "Opt5": "\"重みが全て0になる\"",
        "Opt6": "\"活性化関数が不要になる\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】谷が鋭いと、テストデータが少しズレただけで誤差が激増しますが、平らな谷はズレを許容します。\""
    },
    {
        "ID": "\"DL-048\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"誤差逆伝播法の計算過程で、あるノードの出力を、そのノードの入力で微分した値を何というか？\"",
        "Opt1": "\"局所的な勾配（ローカルグラディエント）\"",
        "Opt2": "\"最終的な勾配\"",
        "Opt3": "\"損失\"",
        "Opt4": "\"活性化レベル\"",
        "Opt5": "\"バイアス勾配\"",
        "Opt6": "\"学習係数\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】この局所勾配に、後ろの層から流れてきた勾配を掛け合わせることで、さらに前の層へ勾配を伝えます。\""
    },
    {
        "ID": "\"DL-049\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"1943年に「形式ニューロン」を提案し、脳の神経細胞をモデル化した最初の人物（二人）は？\"",
        "Opt1": "\"ミンスキーとパパート\"",
        "Opt2": "\"マカロックとピッツ\"",
        "Opt3": "\"ヒントンとルカン\"",
        "Opt4": "\"ローゼンブラットとラメルハート\"",
        "Opt5": "\"チューリングとノイマン\"",
        "Opt6": "\"ショックレーとバーディーン\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】Warren McCullochとWalter Pitts。デジタルな（0か1か）処理として脳を捉え、現代のAIの原点となりました。\""
    },
    {
        "ID": "\"DL-050\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"ディープラーニングにおいて、入力が画像の場合、全結合層で起きる「位置情報の欠落」を解決する層は？\"",
        "Opt1": "\"プーリング層\"",
        "Opt2": "\"畳み込み層\"",
        "Opt3": "\"回帰層\"",
        "Opt4": "\"再帰層\"",
        "Opt5": "\"注意層（アテンション）\"",
        "Opt6": "\"正則化層\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】Convolution層。フィルタをスライドさせることで、画像の局所的な特徴（エッジなど）と位置関係を保ったまま抽出できます。\""
    },
    {
        "ID": "\"DL-051\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"シグモイド関数の微分値の最大値は0.25である。この事実が勾配消失問題にどう寄与するか？\"",
        "Opt1": "\"1より小さいため、層を重ねるごとに勾配が指数関数的に減少する\"",
        "Opt2": "\"1より小さいため、勾配が爆発しやすくなる\"",
        "Opt3": "\"計算が複雑になり、GPUのメモリを圧迫する\"",
        "Opt4": "\"出力が0中心にならないため、学習が不安定になる\"",
        "Opt5": "\"バイアスが大きくなりすぎる\"",
        "Opt6": "\"正解率が0.25で頭打ちになる\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】1層通過するごとに勾配が最大でも4分の1になるため、4層で256分の1、10層で約100万分の1となり、入力層付近の重みが全く更新されなくなります。\""
    },
    {
        "ID": "\"DL-052\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"活性化関数にReLUを用いる際、Heの初期化（He Normal）が推奨される理由は？\"",
        "Opt1": "\"負の領域が0になる性質上、分散を半分（1/2）にする必要があるから\"",
        "Opt2": "\"負の領域が0になる性質上、分散を2倍にする必要があるから\"",
        "Opt3": "\"計算速度を向上させるため\"",
        "Opt4": "\"重みをすべて0にするため\"",
        "Opt5": "\"バイアスを1に固定するため\"",
        "Opt6": "\"シグモイド関数と同じ勾配を得るため\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】ReLUは入力の半分を0にするため、出力の分散を維持するにはXavier（1/n）よりも大きな分散（2/n）が必要になります。\""
    },
    {
        "ID": "\"DL-053\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"バッチ正規化（Batch Norm）が解決するとされる、学習中に各層の入力分布が変化してしまう現象を何というか？\"",
        "Opt1": "\"内部共変量シフト\"",
        "Opt2": "\"次元の呪い\"",
        "Opt3": "\"オーバーフィッティング\"",
        "Opt4": "\"局所最適解\"",
        "Opt5": "\"勾配爆発\"",
        "Opt6": "\"アンダーフィッティング\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】Internal Covariate Shift。前層のパラメータ更新によって次層への入力分布がコロコロ変わると学習が非効率になります。Batch Normはこれを正規化して安定させます。\""
    },
    {
        "ID": "\"DL-054\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"「学習率（Learning Rate）」を時間の経過とともに小さくしていく手法を総称して何というか？\"",
        "Opt1": "\"学習率減衰（Learning Rate Decay）\"",
        "Opt2": "\"正則化\"",
        "Opt3": "\"バッチ正規化\"",
        "Opt4": "\"モーメンタム\"",
        "Opt5": "\"バックプロパゲーション\"",
        "Opt6": "\"早期終了\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】学習初期は大きく動いて探索し、終盤は細かく動いて最適解に収束させるためのテクニックです。\""
    },
    {
        "ID": "\"DL-055\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"重みのL2正則化が「Weight Decay（重み減衰）」と呼ばれる理由は？\"",
        "Opt1": "\"重みの更新式において、現在の重みに1未満の定数を掛けて値を小さくする操作と等価だから\"",
        "Opt2": "\"重みの数（次元）を減らすから\"",
        "Opt3": "\"学習を途中で止めるから\"",
        "Opt4": "\"勾配を0にするから\"",
        "Opt5": "\"バイアスを減らすから\"",
        "Opt6": "\"メモリ消費を減らすから\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】L2正則化の勾配を計算すると、重みそのものに比例した値を引く形になるため、更新のたびに重みが減衰していきます。\""
    },
    {
        "ID": "\"DL-056\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"ディープニューラルネットワークにおいて、重みを「すべて1」で初期化した場合の最大の問題は？\"",
        "Opt1": "\"勾配が爆発する\"",
        "Opt2": "\"全てのニューロンが同じ計算を行い、多層構造の利点が失われる\"",
        "Opt3": "\"メモリ不足になる\"",
        "Opt4": "\"学習が速すぎて過学習する\"",
        "Opt5": "\"シグモイド関数が動かなくなる\"",
        "Opt6": "\"バイアスが負の値になる\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】「対称性の崩壊」が起きません。各ニューロンが同じ信号を受け取り同じ更新をするため、単一のニューロンがあるのと変わらない状態になります。\""
    },
    {
        "ID": "\"DL-057\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"誤差逆伝播法において、出力層側の勾配をΔとすると、1つ前の層の勾配を求めるために必要な演算は？\"",
        "Opt1": "\"勾配の足し算\"",
        "Opt2": "\"行列の転置と行列積（および活性化関数の微分との積）\"",
        "Opt3": "\"勾配の平均化\"",
        "Opt4": "\"二乗和の平方根\"",
        "Opt5": "\"ランダムなサンプリング\"",
        "Opt6": "\"定数倍\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】連鎖律に基づき、後ろから来た勾配に行列の重みを掛け合わせることで、前の層の誤差への寄与度を算出します。\""
    },
    {
        "ID": "\"DL-058\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"Adam（アダム）において、学習初期に指数移動平均が0に偏ってしまうのを防ぐための処理は？\"",
        "Opt1": "\"バイアス補正\"",
        "Opt2": "\"データ拡張\"",
        "Opt3": "\"L1正則化\"",
        "Opt4": "\"ドロップアウト\"",
        "Opt5": "\"重み初期化\"",
        "Opt6": "\"活性化関数の変更\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】Bias correction。初期段階では移動平均が0に近い値から始まるため、これを補正して立ち上がりをスムーズにしています。\""
    },
    {
        "ID": "\"DL-059\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"検証データ（Validation set）が必要な主な理由は？\"",
        "Opt1": "\"テストデータで学習してしまうのを防ぐため\"",
        "Opt2": "\"ハイパーパラメータの調整に使い、汎化性能を評価するため\"",
        "Opt3": "\"学習データを増やすため\"",
        "Opt4": "\"精度を100%にするため\"",
        "Opt5": "\"GPUの負荷を減らすため\"",
        "Opt6": "\"不正解データを削除するため\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】「学習用」「調整用（検証）」「最終評価用（テスト）」に分けるのが基本です。調整にテストデータを使うのは「禁じ手」です。\""
    },
    {
        "ID": "\"DL-060\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"多層ニューラルネットワークが「ブラックボックス」と言われる主な理由は？\"",
        "Opt1": "\"コードが暗号化されているから\"",
        "Opt2": "\"内部パラメータが膨大で、どの重みがどう予測に寄与したか人間が解釈しにくいから\"",
        "Opt3": "\"数学的な根拠が一切ないから\"",
        "Opt4": "\"コンピュータにしか計算できないから\"",
        "Opt5": "\"結果が毎回ランダムだから\"",
        "Opt6": "\"商用利用が禁止されているから\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】説明可能性（Explainability）の課題。近年、LIMEやSHAP、Grad-CAMなどの手法で可視化が試みられています。\""
    },
    {
        "ID": "\"DL-061\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"「普遍性定理」において、中間層が1層だけでも近似可能な関数はどのようなものか？\"",
        "Opt1": "\"全ての離散関数\"",
        "Opt2": "\"任意の連続関数\"",
        "Opt3": "\"一次関数のみ\"",
        "Opt4": "\"指数関数のみ\"",
        "Opt5": "\"計算不能な関数\"",
        "Opt6": "\"3次元以上の関数のみ\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】理論上、十分に多いノード数があればどんな連続関数も再現できますが、効率や学習のしやすさから「深さ」が重要視されます。\""
    },
    {
        "ID": "\"DL-062\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"ドロップアウトを使用する際、学習時（Training）と推論時（Inference）の挙動として正しい組み合わせは？\"",
        "Opt1": "\"学習時：間引く / 推論時：間引かない（出力を調整する）\"",
        "Opt2": "\"学習時：間引かない / 推論時：間引く\"",
        "Opt3": "\"学習時：間引く / 推論時：間引く\"",
        "Opt4": "\"学習時：重みを0にする / 推論時：重みを1にする\"",
        "Opt5": "\"学習時：ランダム / 推論時：常に固定\"",
        "Opt6": "\"学習時：間引く / 推論時：全層削除\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】推論時はすべてのユニットを使い、学習時の期待値と合わせるために出力をスケーリングします。\""
    },
    {
        "ID": "\"DL-063\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"学習の進捗を確認する際、訓練誤差は下がっているが検証誤差が上がっている状態を何というか？\"",
        "Opt1": "\"未学習（アンダーフィッティング）\"",
        "Opt2": "\"過学習（オーバーフィッティング）\"",
        "Opt3": "\"収束\"",
        "Opt4": "\"勾配消失\"",
        "Opt5": "\"正常な学習\"",
        "Opt6": "\"バイアス不足\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】モデルが訓練データの「ノイズ」まで覚えてしまい、未知のデータに対応できなくなっている典型的な兆候です。\""
    },
    {
        "ID": "\"DL-064\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"「データリーク（Data Leakage）」とはどのような現象か？\"",
        "Opt1": "\"テストデータの情報が学習プロセスに入り込んでしまい、精度が不当に高く見えること\"",
        "Opt2": "\"サーバーからデータが盗まれること\"",
        "Opt3": "\"メモリ不足でデータが消えること\"",
        "Opt4": "\"学習中にデータが重複すること\"",
        "Opt5": "\"入力データに欠損値があること\"",
        "Opt6": "\"GPUからCPUへデータが移ること\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】カンニングと同じ状態です。例えば、未来の情報が訓練データに含まれている場合などが該当します。\""
    },
    {
        "ID": "\"DL-065\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"RMSpropがAdaGradの欠点を克服するために導入した計算手法は？\"",
        "Opt1": "\"指数移動平均\"",
        "Opt2": "\"単純算術平均\"",
        "Opt3": "\"移動中央値\"",
        "Opt4": "\"最大値の保持\"",
        "Opt5": "\"ランダムウォーク\"",
        "Opt6": "\"定数加算\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】AdaGradは過去の勾配を全て蓄積するため学習率が下がり続けますが、RMSpropは新しい勾配を重視して減衰させるため、学習が止まりません。\""
    },
    {
        "ID": "\"DL-066\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"バッチサイズを極端に「1」にした場合の勾配降下法を特に何というか？\"",
        "Opt1": "\"バッチ勾配降下法\"",
        "Opt2": "\"ミニバッチ勾配降下法\"",
        "Opt3": "\"確率的勾配降下法（純粋なSGD）\"",
        "Opt4": "\"アンサンブル学習\"",
        "Opt5": "\"強化学習\"",
        "Opt6": "\"オンライン学習\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】1データごとに更新するため動きが激しく（ノイズが多く）、局所解を抜け出しやすいメリットがありますが、並列計算効率は悪いです。\""
    },
    {
        "ID": "\"DL-067\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"ディープラーニングのモデルを学習させる際、GPU（Graphics Processing Unit）がCPUより圧倒的に有利な理由は？\"",
        "Opt1": "\"メモリ容量が無限だから\"",
        "Opt2": "\"並列演算ユニットが数千個あり、行列計算を同時に行えるから\"",
        "Opt3": "\"OSを介さずに動作するから\"",
        "Opt4": "\"壊れにくいから\"",
        "Opt5": "\"データの読み込みが速いから\"",
        "Opt6": "\"プログラムが単純になるから\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】ディープラーニングの本体は巨大な行列演算（積和演算）の塊であるため、並列処理が得意なGPUが最適です。\""
    },
    {
        "ID": "\"DL-068\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"「勾配爆発（Exploding Gradients）」を防ぐために、勾配のノルムが一定値を超えたら強制的に縮小する手法は？\"",
        "Opt1": "\"勾配クリッピング\"",
        "Opt2": "\"ドロップアウト\"",
        "Opt3": "\"L2正則化\"",
        "Opt4": "\"早期終了\"",
        "Opt5": "\"バッチ正規化\"",
        "Opt6": "\"Heの初期化\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】Gradient Clipping。特にRNNのような時系列モデルで勾配が急激に大きくなるのを防ぐために多用されます。\""
    },
    {
        "ID": "\"DL-069\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"重みの初期化において、一様分布から $ [- \\sqrt{6/(n_{in}+n_{out})}",
        "Opt1": "\\sqrt{6/(n_{in}+n_{out})}] $ の範囲でサンプリングする手法は？\"",
        "Opt2": "\"Heの初期化\"",
        "Opt3": "\"Xavierの初期化（Glorot初期化）\"",
        "Opt4": "\"ゼロ初期化\"",
        "Opt5": "\"ガウス分布初期化\"",
        "Opt6": "\"単位行列初期化\"",
        "Answer_Idx": "\"バイアス初期化\"",
        "Explanation": 1
    },
    {
        "ID": "\"DL-070\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"「レイヤー正規化（Layer Normalization）」が、バッチ正規化と異なる点はどれか？\"",
        "Opt1": "\"バッチサイズに依存せず、1つのサンプル内の各層の全ユニット間で正規化を行う\"",
        "Opt2": "\"全ての学習データを使う\"",
        "Opt3": "\"正規化を行わない\"",
        "Opt4": "\"GPUを使わない\"",
        "Opt5": "\"学習率を変えない\"",
        "Opt6": "\"常に0をかける\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】ミニバッチの統計量を使わないため、バッチサイズが1の場合や、系列長が異なるRNNなどで特に有効です。\""
    },
    {
        "ID": "\"DL-071\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"「重み減衰（Weight Decay）」を導入した際の損失関数の変化として正しいものは？\"",
        "Opt1": "\"損失関数 = 元の損失 + 正則化項\"",
        "Opt2": "\"損失関数 = 元の損失 - 正則化項\"",
        "Opt3": "\"損失関数 = 元の損失 × 正則化項\"",
        "Opt4": "\"損失関数 = 正則化項のみ\"",
        "Opt5": "\"損失関数 = 0\"",
        "Opt6": "\"変化しない\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】正則化項（重みの大きさ）をペナルティとして加えることで、損失を最小化しつつ「重みも小さく保つ」よう促します。\""
    },
    {
        "ID": "\"DL-072\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"ディープラーニングにおいて、入力データの各特徴量のスケールを揃える（0〜1にする、または平均0・分散1にする）工程を何というか？\"",
        "Opt1": "\"特徴量抽出\"",
        "Opt2": "\"データの正規化・標準化\"",
        "Opt3": "\"データ拡張\"",
        "Opt4": "\"次元圧縮\"",
        "Opt5": "\"平滑化\"",
        "Opt6": "\"量子化\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】スケールがバラバラだと、特定の変数が勾配に過剰な影響を与え、学習が収束しにくくなります。\""
    },
    {
        "ID": "\"DL-073\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"活性化関数「Leaky ReLU」の特徴として正しいものは？\"",
        "Opt1": "\"入力が負の場合、0ではなく小さな値（0.01xなど）を出力する\"",
        "Opt2": "\"入力が負の場合、常に-1を出力する\"",
        "Opt3": "\"最大値が1で頭打ちになる\"",
        "Opt4": "\"微分値が常に一定ではない\"",
        "Opt5": "\"シグモイド関数と同じである\"",
        "Opt6": "\"計算に指数関数を用いる\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】ReLUで発生しうる「Dead ReLU問題（一度0になると二度と更新されない）」を緩和するために提案されました。\""
    },
    {
        "ID": "\"DL-074\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"誤差逆伝播法をプログラムで実装する際、順伝播時の出力を保存しておく必要がある理由は？\"",
        "Opt1": "\"メモリを無駄に使うため\"",
        "Opt2": "\"逆伝播（微分の計算）の際に、その値が必要になるから\"",
        "Opt3": "\"結果を可視化するため\"",
        "Opt4": "\"学習率を計算するため\"",
        "Opt5": "\"バイアスを初期化するため\"",
        "Opt6": "\"推論を速くするため\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】例えば $ y=x^2 $ の微分 $ 2x $ を計算するには、順伝播時の $ x $ の値を知っている必要があります。\""
    },
    {
        "ID": "\"DL-075\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"ディープラーニングのモデルサイズを削減するために、重みの精度を32bit浮動小数点から16bitや8bitに落とす手法は？\"",
        "Opt1": "\"蒸留\"",
        "Opt2": "\"量子化（Quantization）\"",
        "Opt3": "\"剪定（プルーニング）\"",
        "Opt4": "\"平滑化\"",
        "Opt5": "\"正則化\"",
        "Opt6": "\"バッチ正規化\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】計算速度の向上とメモリ削減が可能で、特にモバイルデバイスやエッジAIでの実行に重要です。\""
    },
    {
        "ID": "\"DL-076\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"精度の高い巨大なモデル（教師モデル）の知識を、軽量なモデル（生徒モデル）に継承させる手法を何というか？\"",
        "Opt1": "\"転移学習\"",
        "Opt2": "\"知識蒸留（Knowledge Distillation）\"",
        "Opt3": "\"ファインチューニング\"",
        "Opt4": "\"データ拡張\"",
        "Opt5": "\"アンサンブル\"",
        "Opt6": "\"正則化\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】生徒モデルに「教師モデルの出力確率分布」を学習させることで、小さなモデルでも高い精度を実現します。\""
    },
    {
        "ID": "\"DL-077\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"ニューラルネットワークにおいて、ある層の全ユニットが次の層の全ユニットと接続されている構造を何というか？\"",
        "Opt1": "\"畳み込み層\"",
        "Opt2": "\"全結合層（Affine層 / Dense層）\"",
        "Opt3": "\"再帰層\"",
        "Opt4": "\"プーリング層\"",
        "Opt5": "\"注意層\"",
        "Opt6": "\"ドロップアウト層\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】Fully Connected Layer。画像認識では位置情報が失われるため、後にCNNに取って代わられました。\""
    },
    {
        "ID": "\"DL-078\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"「万能近似定理」を証明するために必要な条件として適切なのはどれか？\"",
        "Opt1": "\"活性化関数が非線形であること\"",
        "Opt2": "\"層が100層以上あること\"",
        "Opt3": "\"GPUを使用すること\"",
        "Opt4": "\"学習率が1.0であること\"",
        "Opt5": "\"データが画像であること\"",
        "Opt6": "\"バッチ正規化を使うこと\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】活性化関数が線形だと、何層重ねても線形な表現しかできず、複雑な関数を近似できません。\""
    },
    {
        "ID": "\"DL-079\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"モデルが学習データに「完全にフィット」し、未知のデータに対して誤差が大きくなる現象を避けるための工夫ではないものはどれか？\"",
        "Opt1": "\"ドロップアウトの導入\"",
        "Opt2": "\"L2正則化の適用\"",
        "Opt3": "\"データ拡張\"",
        "Opt4": "\"モデルのパラメータ数を大幅に増やす\"",
        "Opt5": "\"早期終了\"",
        "Opt6": "\"バッチ正規化\"",
        "Answer_Idx": 3,
        "Explanation": "\"【解説】パラメータ数を増やすとモデルの表現力が上がりすぎて、むしろ過学習しやすくなる傾向があります。\""
    },
    {
        "ID": "\"DL-080\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"最適化アルゴリズムにおいて、パラメータごとに異なる学習率を適用するものを総称して何というか？\"",
        "Opt1": "\"適応的学習率アルゴリズム（Adaptive Learning Rate）\"",
        "Opt2": "\"固定学習率アルゴリズム\"",
        "Opt3": "\"確率的アルゴリズム\"",
        "Opt4": "\"教師あり学習\"",
        "Opt5": "\"非線形最適化\"",
        "Opt6": "\"オンライン最適化\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】AdaGrad"
    },
    {
        "ID": "\"DL-081\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"「データ並列」において、各GPUが計算した勾配を統合して重みを更新する際、同期をとって待機する方式を何というか？\"",
        "Opt1": "\"非同期分散学習\"",
        "Opt2": "\"同期分散学習\"",
        "Opt3": "\"逐次学習\"",
        "Opt4": "\"モデル並列\"",
        "Opt5": "\"パイプライン並列\"",
        "Opt6": "\"アンサンブル\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】全員の計算が終わるのを待って平均をとるため、計算は正確ですが、遅いGPU（ボトルネック）に引きずられます。\""
    },
    {
        "ID": "\"DL-082\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"過学習の抑制手法として、あらかじめ用意した大量の重みのうち、値の小さい接続を削除（0に）してモデルを軽量化する手法は？\"",
        "Opt1": "\"量子化\"",
        "Opt2": "\"剪定（プルーニング：Pruning）\"",
        "Opt3": "\"蒸留\"",
        "Opt4": "\"データ拡張\"",
        "Opt5": "\"早期終了\"",
        "Opt6": "\"バッチ正規化\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】木の枝を払うように不要な接続を切る手法。精度を維持したまま計算量を大幅に減らせる場合があります。\""
    },
    {
        "ID": "\"DL-083\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"学習が進むにつれて「損失関数の谷」が非常に細長くなっている場合、単純なSGDで起きる問題は？\"",
        "Opt1": "\"学習が爆発する\"",
        "Opt2": "\"最適解へ向かわず、谷の壁の間を激しく振動して収束が遅くなる\"",
        "Opt3": "\"学習が即座に終わる\"",
        "Opt4": "\"重みがすべて1になる\"",
        "Opt5": "\"活性化関数が消える\"",
        "Opt6": "\"メモリが不足する\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】これを解決するために、過去の方向を維持するMomentumや、軸ごとに学習率を変えるRMSpropなどが有効です。\""
    },
    {
        "ID": "\"DL-084\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"多層ニューラルネットワークにおいて、逆伝播時に勾配が「1より大きな値」の掛け算により無限大に発散することを何というか？\"",
        "Opt1": "\"勾配消失\"",
        "Opt2": "\"勾配爆発\"",
        "Opt3": "\"勾配収束\"",
        "Opt4": "\"勾配飽和\"",
        "Opt5": "\"勾配消滅\"",
        "Opt6": "\"勾配分散\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】深い層（特に入力層側）で重みの更新が制御不能になり、NaN（非数）が発生する原因になります。\""
    },
    {
        "ID": "\"DL-085\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"「全結合ニューラルネットワーク」で、入力画像が $(28 \\times 28 \\times 3)$ のカラー画像である場合、最初の層に入力されるユニット数はいくつか？\"",
        "Opt1": "\"28\"",
        "Opt2": "\"84\"",
        "Opt3": "\"784\"",
        "Opt4": "\"2352\"",
        "Opt5": "\"3\"",
        "Opt6": "\"1\"",
        "Answer_Idx": 3,
        "Explanation": "\"【解説】$ 28 \\times 28 \\times 3 = 2352 $ です。全結合層では多次元のデータを1次元に平滑化して入力します。\""
    },
    {
        "ID": "\"DL-086\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"「勾配降下法」の停止条件として不適切なものはどれか？\"",
        "Opt1": "\"あらかじめ決めた最大エポック数に達した時\"",
        "Opt2": "\"損失関数の減少が一定値以下になった時\"",
        "Opt3": "\"正解率が100%になった時\"",
        "Opt4": "\"検証データの誤差が上がり始めた時（早期終了）\"",
        "Opt5": "\"勾配が0になった時\"",
        "Opt6": "\"計算機が熱くなった時\"",
        "Answer_Idx": 5,
        "Explanation": "\"【解説】ハードウェアの温度は学習アルゴリズムの停止条件には含まれません。\""
    },
    {
        "ID": "\"DL-087\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"損失関数 $ L $ に対して、重み $ w $ を $ w \\leftarrow w - \\eta \\frac{\\partial L}{\\partial w} $ と更新する手法の名称は？（ $\\eta$ は学習率）\"",
        "Opt1": "\"ニュートン法\"",
        "Opt2": "\"最急降下法（勾配降下法）\"",
        "Opt3": "\"最小二乗法\"",
        "Opt4": "\"ベイズ推定\"",
        "Opt5": "\"ラグランジュ未定乗数法\"",
        "Opt6": "\"シンプレックス法\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】関数の傾き（勾配）とは逆の方向にパラメータを動かして、最小値を探る最も基本的な手法です。\""
    },
    {
        "ID": "\"DL-088\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"ディープラーニングのモデルが「局所最適解（ローカルミニマム）」に陥っても、実用上の精度がそれほど悪くないと言われる理由は？\"",
        "Opt1": "\"高次元空間では、ほとんどの極小値が全域最適解に近い値を持つから\"",
        "Opt2": "\"コンピュータが自動で修正するから\"",
        "Opt3": "\"学習データに正解があるから\"",
        "Opt4": "\"活性化関数がシグモイドだから\"",
        "Opt5": "\"過学習しているから\"",
        "Opt6": "\"誤差逆伝播法が完璧だから\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】近年の研究では、高次元空間における極小値はどれも十分に「良い」解であり、真の最適解と遜色ないことが多いとされています。\""
    },
    {
        "ID": "\"DL-089\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"活性化関数「ELU（Exponential Linear Unit）」がReLUより優れている点は？\"",
        "Opt1": "\"入力が負の場合に滑らかな負の値を出力し、出力の平均を0に近づけられる\"",
        "Opt2": "\"計算がReLUより高速である\"",
        "Opt3": "\"常に1を出力する\"",
        "Opt4": "\"微分が不要である\"",
        "Opt5": "\"重みを0にする\"",
        "Opt6": "\"量子化に適している\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】負の値を $ \\alpha(e^x - 1) $ で扱うことで、ReLUの利点を保ちつつ、学習をより安定させます。\""
    },
    {
        "ID": "\"DL-090\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"「アンサンブル学習」をディープラーニングで行う際の課題は？\"",
        "Opt1": "\"精度が下がること\"",
        "Opt2": "\"複数の巨大なモデルを学習・保持するための計算コストとメモリ消費が非常に大きいこと\"",
        "Opt3": "\"並列化できないこと\"",
        "Opt4": "\"データが不足すること\"",
        "Opt5": "\"活性化関数が使えないこと\"",
        "Opt6": "\"理論が未解明であること\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】非常に重い学習を何度も行う必要があるため、実務ではドロップアウト（擬似的なアンサンブル）などで代用されることが多いです。\""
    },
    {
        "ID": "\"DL-091\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"学習データに対する損失が下がっているのに、検証データに対する損失が「全く下がらない（最初から高い）」場合の主な原因は？\"",
        "Opt1": "\"学習不足\"",
        "Opt2": "\"学習データと検証データの分布が大きく異なっている（ドメインシフト）\"",
        "Opt3": "\"過学習\"",
        "Opt4": "\"学習率が小さすぎる\"",
        "Opt5": "\"データ拡張が足りない\"",
        "Opt6": "\"バッチ正規化のしすぎ\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】モデルの性能以前に、用意したデータの性質が「学習用」と「評価用」で噛み合っていない可能性があります。\""
    },
    {
        "ID": "\"DL-092\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"ディープラーニングにおいて、入力データの平均を0、分散を1に変換する「標準化」を行う主な目的は？\"",
        "Opt1": "\"学習を安定させ、収束を速めるため\"",
        "Opt2": "\"データの容量を減らすため\"",
        "Opt3": "\"画像を鮮明にするため\"",
        "Opt4": "\"重みを初期化するため\"",
        "Opt5": "\"正解率を100%にするため\"",
        "Opt6": "\"バイアスを消去するため\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】入力のスケールを揃えることで、勾配の計算が安定し、適切な学習率を共有しやすくなります。\""
    },
    {
        "ID": "\"DL-093\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"「ハイパーパラメータ」の探索手法において、あらかじめ決めた範囲を等間隔に調べる手法を何というか？\"",
        "Opt1": "\"ランダムサーチ\"",
        "Opt2": "\"グリッドサーチ\"",
        "Opt3": "\"ベイズ最適化\"",
        "Opt4": "\"遺伝的アルゴリズム\"",
        "Opt5": "\"早期終了\"",
        "Opt6": "\"交差検証\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】Grid Search。確実ですが、パラメータ数が増えると組み合わせが爆発的に増える欠点があります。\""
    },
    {
        "ID": "\"DL-094\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"「勾配の飽和（Saturated Gradient）」とは、どのような状態で起きるか？\"",
        "Opt1": "\"シグモイド関数の出力が0または1に近づき、微分値がほぼ0になる状態\"",
        "Opt2": "\"ReLUの出力が無限大になる状態\"",
        "Opt3": "\"バッチサイズが1の時\"",
        "Opt4": "\"学習率が1.0の時\"",
        "Opt5": "\"重みがすべて同じ時\"",
        "Opt6": "\"データが足りない時\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】関数の「平らな部分」に値が入り込んでしまい、勾配が流れなくなる現象を指します。\""
    },
    {
        "ID": "\"DL-095\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"L1正則化とL2正則化を組み合わせて使用する手法を何というか？\"",
        "Opt1": "\"モーメンタム\"",
        "Opt2": "\"エラスティックネット（Elastic Net）\"",
        "Opt3": "\"バッチ正規化\"",
        "Opt4": "\"ドロップアウト\"",
        "Opt5": "\"アンサンブル\"",
        "Opt6": "\"知識蒸留\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】両方の利点（特徴選択と学習の安定性）を取り入れるために、係数を調整してハイブリッドに使用します。\""
    },
    {
        "ID": "\"DL-096\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"「勾配降下法」において、1回も更新を行っていない初期状態での予測精度は一般的にどうなるか？\"",
        "Opt1": "\"100%\"",
        "Opt2": "\"0%\"",
        "Opt3": "\"ランダム（10クラス分類なら約10%）\"",
        "Opt4": "\"計算不能\"",
        "Opt5": "\"負の値\"",
        "Opt6": "\"50%\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】初期化された重みによるランダムな出力となるため、期待値はランダムな推測と同じになります。\""
    },
    {
        "ID": "\"DL-097\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"「交差エントロピー誤差」の計算式に含まれる $ \\log $ 関数の役割は？\"",
        "Opt1": "\"正解クラスの予測確率が低いほど、損失（ペナルティ）を急激に大きくするため\"",
        "Opt2": "\"数値を小さくするため\"",
        "Opt3": "\"確率を1以上にするため\"",
        "Opt4": "\"負の値を消すため\"",
        "Opt5": "\"微分を簡単にするため\"",
        "Opt6": "\"バイアスを加えるため\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】$ \\log(p) $ は $ p $ が1（正解）に近いと0に、0（不正解）に近いと無限大に発散するため、誤りに対して強い警告を与えられます。\""
    },
    {
        "ID": "\"DL-098\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"活性化関数に「シグモイド関数」を中間層で使ってはいけないと言われる決定的な理由は？\"",
        "Opt1": "\"計算が遅いから\"",
        "Opt2": "\"層が深くなると勾配消失が必ず起きて学習が止まるから\"",
        "Opt3": "\"出力がマイナスにならないから\"",
        "Opt4": "\"微分が定義できないから\"",
        "Opt5": "\"メモリを消費するから\"",
        "Opt6": "\"人気がないから\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】歴史的には使われていましたが、現在はReLUなどの勾配消失が起きにくい関数に取って代わられました。\""
    },
    {
        "ID": "\"DL-099\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"「ミニバッチサイズ」を大きくすることによる直接的なデメリットは？\"",
        "Opt1": "\"計算が不安定になること\"",
        "Opt2": "\"1エポックあたりの重みの更新回数が減り、学習の進みが遅く見えること\"",
        "Opt3": "\"精度が必ず下がること\"",
        "Opt4": "\"過学習しやすくなること\"",
        "Opt5": "\"CPUしか使えなくなること\"",
        "Opt6": "\"勾配が0になること\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】バッチサイズが大きいと並列計算は速いですが、1エポック内の更新回数が減るため、収束までに必要なエポック数が増える場合があります。\""
    },
    {
        "ID": "\"DL-100\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"ディープラーニングにおいて、入力、重み、勾配などの情報を保持するために使われる多次元配列を何というか？\"",
        "Opt1": "\"ベクトル\"",
        "Opt2": "\"行列\"",
        "Opt3": "\"テンソル（Tensor）\"",
        "Opt4": "\"スカラ\"",
        "Opt5": "\"リスト\"",
        "Opt6": "\"辞書\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】0次元がスカラ、1次元がベクトル、2次元が行列、それ以上の多次元をテンソルと呼びます。\""
    },
    {
        "ID": "\"DL-101\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"ニューラルネットワークの「層の深さ」を増やすことによる主なメリットは？\"",
        "Opt1": "\"計算が単純になること\"",
        "Opt2": "\"より抽象的で複雑な特徴を効率よく表現できるようになること\"",
        "Opt3": "\"過学習が絶対に起きなくなること\"",
        "Opt4": "\"学習データが少なくて済むこと\"",
        "Opt5": "\"重みの初期化が不要になること\"",
        "Opt6": "\"GPUが不要になること\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】浅い層でエッジや色を、深い層で顔や物体といった複雑な概念を段階的に抽出できるようになります。\""
    },
    {
        "ID": "\"DL-102\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"Googleが開発した、ディープラーニングの高速演算に特化した専用プロセッサ（ASIC）を何というか？\"",
        "Opt1": "\"GPU\"",
        "Opt2": "\"FPGA\"",
        "Opt3": "\"TPU（Tensor Processing Unit）\"",
        "Opt4": "\"CPU\"",
        "Opt5": "\"DSP\"",
        "Opt6": "\"APU\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】行列演算に最適化されており、クラウド（Google Cloud）経由で利用されることが多いです。\""
    },
    {
        "ID": "\"DL-103\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"「データ拡張」において、画像認識モデルを頑健にするために行われる適切な処理は？\"",
        "Opt1": "\"ラベルをランダムに付け替える\"",
        "Opt2": "\"画像にノイズを加えたり、角度を変えたりする\"",
        "Opt3": "\"全ての画像を黒塗りにする\"",
        "Opt4": "\"画像のサイズを1ピクセルにする\"",
        "Opt5": "\"学習データを半分に減らす\"",
        "Opt6": "\"同じ画像を100万回コピーする\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】データのバリエーションを意図的に増やすことで、モデルが本質的な特徴を捉えやすくなります。\""
    },
    {
        "ID": "\"DL-104\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"「全結合層」を計算式で表すと $ Y = \\sigma(WX + B) $ となる。ここで $ B $ が表すものは？\"",
        "Opt1": "\"重み行列\"",
        "Opt2": "\"入力ベクトル\"",
        "Opt3": "\"バイアスベクトル\"",
        "Opt4": "\"活性化関数\"",
        "Opt5": "\"学習率\"",
        "Opt6": "\"誤差\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】Bias。各ニューロンの出力に加算される定数で、発火のしやすさを制御します。\""
    },
    {
        "ID": "\"DL-105\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"「過学習」を防ぐためのアプローチとして間違っているものはどれか？\"",
        "Opt1": "\"モデルをより深く複雑にする\"",
        "Opt2": "\"ドロップアウトを入れる\"",
        "Opt3": "\"L1/L2正則化を加える\"",
        "Opt4": "\"学習データを増やす\"",
        "Opt5": "\"早期終了を行う\"",
        "Opt6": "\"バッチ正規化を行う\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】モデルを複雑にしすぎると、訓練データの些細な特徴まで覚えてしまい、むしろ過学習が悪化します。\""
    },
    {
        "ID": "\"DL-106\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"「推論（Inference）」フェーズにおいて、不要な計算はどれか？\"",
        "Opt1": "\"行列演算\"",
        "Opt2": "\"活性化関数の適用\"",
        "Opt3": "\"誤差逆伝播（重みの更新）\"",
        "Opt4": "\"順伝播\"",
        "Opt5": "\"Softmaxの適用\"",
        "Opt6": "\"データの入力\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】推論は「予測するだけ」なので、重みを書き換える逆伝播の工程は必要ありません。\""
    },
    {
        "ID": "\"DL-107\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"「誤差逆伝播法」は、1980年代のどのAIブームの時期に確立されたか？\"",
        "Opt1": "\"第1次AIブーム\"",
        "Opt2": "\"第2次AIブーム\"",
        "Opt3": "\"第3次AIブーム\"",
        "Opt4": "\"第4次AIブーム\"",
        "Opt5": "\"ブームではない\"",
        "Opt6": "\"石器時代\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】1986年のラメルハートらによる論文で広まり、第2次AIブームの核心技術となりました。\""
    },
    {
        "ID": "\"DL-108\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"「勾配降下法」のハイパーパラメータ調整において、最も重要とされる項目はどれか？\"",
        "Opt1": "\"学習率\"",
        "Opt2": "\"バッチサイズ\"",
        "Opt3": "\"エポック数\"",
        "Opt4": "\"初期値の分散\"",
        "Opt5": "\"活性化関数の種類\"",
        "Opt6": "\"バイアスの初期値\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】学習率は「モデルが収束するか、発散するか」を左右する最もクリティカルなパラメータです。\""
    },
    {
        "ID": "\"DL-109\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"活性化関数の出力が「常に正の値」であるシグモイド関数やReLUで起きやすい、重みの更新方向が制限される問題を何というか？\"",
        "Opt1": "\"ジグザグ現象（不規則な更新）\"",
        "Opt2": "\"勾配消失\"",
        "Opt3": "\"勾配爆発\"",
        "Opt4": "\"オーバーフィッティング\"",
        "Opt5": "\"デッドロック\"",
        "Opt6": "\"バイアスシフト\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】入力がすべて正だと、勾配の符号が全パラメータで同じになり、更新ベクトルが特定の方向に制限されるため、収束が非効率になります。\""
    },
    {
        "ID": "\"DL-110\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"「学習率（$\\eta$）」が大きすぎると学習はどうなるか？\"",
        "Opt1": "\"瞬時に収束する\"",
        "Opt2": "\"損失関数が最小値付近で振動、または発散して学習が失敗する\"",
        "Opt3": "\"過学習が起きる\"",
        "Opt4": "\"メモリが不足する\"",
        "Opt5": "\"何も起きない\"",
        "Opt6": "\"バイアスが0になる\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】谷底を飛び越えて反対側の斜面に着地し、どんどん高いところへ逃げていくイメージです。\""
    },
    {
        "ID": "\"DL-111\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"「L2正則化」におけるペナルティ項は、重み $ w $ を用いてどのように表されるか？（ $\\lambda$ は正則化係数）\"",
        "Opt1": "\"$\\lambda \\sum |w|$\"",
        "Opt2": "\"$\\lambda \\sum w^2$\"",
        "Opt3": "\"$\\lambda \\sum \\sqrt{w}$\"",
        "Opt4": "\"$\\lambda \\max(w)$\"",
        "Opt5": "\"$\\lambda \\log(w)$\"",
        "Opt6": "\"$\\lambda / w$\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】重みの二乗和を損失に加えることで、重みが大きくなることを強く抑制します。\""
    },
    {
        "ID": "\"DL-112\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"バッチ正規化において、学習時に計算した「平均」と「分散」の値を、推論時にどう扱うか？\"",
        "Opt1": "\"推論時の1データから計算し直す\"",
        "Opt2": "\"学習時に記録した「移動平均・移動分散」を固定値として使う\"",
        "Opt3": "\"推論時は正規化をしない\"",
        "Opt4": "\"ランダムな値を使う\"",
        "Opt5": "\"常に0と1にする\"",
        "Opt6": "\"学習時の最終バッチの値のみ使う\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】推論時はデータが1つずつ来ることもあるため、学習全体を通じて推定した平均と分散を再利用します。\""
    },
    {
        "ID": "\"DL-113\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"「AdaGrad」が抱える致命的な弱点は？\"",
        "Opt1": "\"計算が非常に遅いこと\"",
        "Opt2": "\"学習が進むにつれて学習率が0に近づき、最適解に達する前に学習が止まってしまうこと\"",
        "Opt3": "\"メモリを大量に消費すること\"",
        "Opt4": "\"過学習しやすいこと\"",
        "Opt5": "\"GPUが使えないこと\"",
        "Opt6": "\"重みが発散すること\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】分母に過去の勾配の累積を加算し続けるため、後半になると学習率が極小になってしまいます。\""
    },
    {
        "ID": "\"DL-114\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"ニューラルネットワークの「全結合」を「行列演算」で書くメリットは？\"",
        "Opt1": "\"数学的に綺麗だから\"",
        "Opt2": "\"GPUやCPUのベクトル演算命令（SIMD）を活用して高速に計算できるから\"",
        "Opt3": "\"プログラムの行数が増えるから\"",
        "Opt4": "\"メモリを節約できるから\"",
        "Opt5": "\"誤差が出ないから\"",
        "Opt6": "\"活性化関数が不要になるから\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】1つずつ個別に計算するより、まとめて行列として計算する方がハードウェアの性能を引き出せます。\""
    },
    {
        "ID": "\"DL-115\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"「ファインチューニング（Fine-tuning）」とはどのような手法か？\"",
        "Opt1": "\"既存の学習済みモデルを、別の目的のデータで「少しだけ」追加学習させて微調整すること\"",
        "Opt2": "\"モデルを一から設計すること\"",
        "Opt3": "\"重みをすべて0にすること\"",
        "Opt4": "\"データを10倍に増やすこと\"",
        "Opt5": "\"画像を白黒にすること\"",
        "Opt6": "\"学習率を固定すること\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】「転移学習」の一種で、学習済みモデルの重みを初期値として使い、新しいタスクに適応させます。\""
    },
    {
        "ID": "\"DL-116\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"「中間層（隠れ層）」のユニット数を増やすことで、一般的にモデルの何が高まるか？\"",
        "Opt1": "\"表現力（複雑な境界線を引く能力）\"",
        "Opt2": "\"学習速度\"",
        "Opt3": "\"汎化性能\"",
        "Opt4": "\"データの信頼性\"",
        "Opt5": "\"消費電力の削減効率\"",
        "Opt6": "\"バイアス\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】ユニット数（モデルの幅）が増えると、より細かな特徴の組み合わせを記憶できるようになります。\""
    },
    {
        "ID": "\"DL-117\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"ディープラーニングにおいて、入力データの順序をランダムに入れ替えて（シャッフルして）学習させる主な目的は？\"",
        "Opt1": "\"計算を速くするため\"",
        "Opt2": "\"データの順序に依存した偏った学習を防ぐため\"",
        "Opt3": "\"メモリを節約するため\"",
        "Opt4": "\"ラベルを隠すため\"",
        "Opt5": "\"重複データを消すため\"",
        "Opt6": "\"バイアスを増やすため\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】順序に意味がある（例：前半がすべて犬、後半がすべて猫）と、モデルが偏った更新を繰り返してしまい、学習が不安定になります。\""
    },
    {
        "ID": "\"DL-118\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"「損失関数」の値が0に極めて近いとき、モデルの状態として推測されるのは？\"",
        "Opt1": "\"学習が完璧である、または過学習している\"",
        "Opt2": "\"学習が始まっていない\"",
        "Opt3": "\"勾配が爆発している\"",
        "Opt4": "\"データが間違っている\"",
        "Opt5": "\"計算機が故障している\"",
        "Opt6": "\"活性化関数がReLUである\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】損失が0ということは、予測が正解とほぼ完全に一致していることを意味します。実務では過学習を疑うサインでもあります。\""
    },
    {
        "ID": "\"DL-119\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"「誤差逆伝播法」の計算効率について正しい記述は？\"",
        "Opt1": "\"総当たりの数値微分よりも圧倒的に高速である\"",
        "Opt2": "\"数値微分と同じ速度である\"",
        "Opt3": "\"数値微分より遅い\"",
        "Opt4": "\"計算は人間が手書きで行う必要がある\"",
        "Opt5": "\"GPUでは計算できない\"",
        "Opt6": "\"学習データが多いと計算不能になる\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】数値微分は1パラメータごとに計算が必要ですが、逆伝播は1回の計算で全パラメータの勾配を求められます。\""
    },
    {
        "ID": "\"DL-120\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"画像認識コンペILSVRC2015で優勝し、層の深さを152層まで増やした画期的なモデルは？\"",
        "Opt1": "\"AlexNet\"",
        "Opt2": "\"VGGNet\"",
        "Opt3": "\"ResNet\"",
        "Opt4": "\"GoogLeNet\"",
        "Opt5": "\"LeNet\"",
        "Opt6": "\"MobileNet\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】「スキップ接続（残差接続）」を導入することで、超多層ネットワークでも勾配消失を防ぎ、学習を可能にしました。\""
    },
    {
        "ID": "\"DL-121\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"「スキップ接続（残差接続）」の主な役割は？\"",
        "Opt1": "\"層を飛ばして勾配を直接伝えることで、勾配消失を防ぐ\"",
        "Opt2": "\"データを削除する\"",
        "Opt3": "\"計算を複雑にする\"",
        "Opt4": "\"学習率を上げる\"",
        "Opt5": "\"メモリを解放する\"",
        "Opt6": "\"バイアスを0にする\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】$ F(x) + x $ という形で入力をそのまま後ろに伝えることで、深い層でも学習が安定します。\""
    },
    {
        "ID": "\"DL-122\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"「ハイパーパラメータ」ではないものはどれか？\"",
        "Opt1": "\"学習率\"",
        "Opt2": "\"中間層のユニット数\"",
        "Opt3": "\"重みの初期値の分散\"",
        "Opt4": "\"バッチサイズ\"",
        "Opt5": "\"各ニューロンの「重み」の具体的な値\"",
        "Opt6": "\"活性化関数の種類\"",
        "Answer_Idx": 4,
        "Explanation": "\"【解説】重みやバイアスは学習によって「自動で更新される」パラメータであり、人間が事前に決めるハイパーパラメータとは区別されます。\""
    },
    {
        "ID": "\"DL-123\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"「勾配降下法」において、常に一定の方向にだけ急な勾配がある場合に有効な最適化手法は？\"",
        "Opt1": "\"Momentum（モーメンタム）\"",
        "Opt2": "\"バッチ正規化\"",
        "Opt3": "\"ゼロ初期化\"",
        "Opt4": "\"ドロップアウト\"",
        "Opt5": "\"早期終了\"",
        "Opt6": "\"L1正則化\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】過去の慣性を利用することで、振動を抑えつつ一定方向（谷の底に沿った方向）へ加速できます。\""
    },
    {
        "ID": "\"DL-124\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"「正則化係数（$\\lambda$）」を大きくしすぎると、モデルはどうなるか？\"",
        "Opt1": "\"過学習する\"",
        "Opt2": "\"未学習（アンダーフィッティング）になる\"",
        "Opt3": "\"計算が速くなる\"",
        "Opt4": "\"精度が100%になる\"",
        "Opt5": "\"重みが無限大になる\"",
        "Opt6": "\"何も変わらない\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】ペナルティが強すぎて重みが極端に小さくなり、データから何も学習できない（単純すぎる）モデルになってしまいます。\""
    },
    {
        "ID": "\"DL-125\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"ニューラルネットワークにおいて「表現学習（Representation Learning）」が意味することは？\"",
        "Opt1": "\"人間が特徴量を設計すること\"",
        "Opt2": "\"データから自動的に適切な特徴量を抽出・獲得すること\"",
        "Opt3": "\"モデルを絵で描くこと\"",
        "Opt4": "\"正解ラベルを予想すること\"",
        "Opt5": "\"GPUで計算すること\"",
        "Opt6": "\"論文を書くこと\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】ディープラーニングの最大の特徴であり、人間による特徴量エンジニアリングを不要にしました。\""
    },
    {
        "ID": "\"DL-126\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"「データ並列」において、バッチサイズを大きくした際に注意すべき点は？\"",
        "Opt1": "\"学習率も適切に調整（一般的には大きく）する必要がある\"",
        "Opt2": "\"学習率を0にする必要がある\"",
        "Opt3": "\"メモリが余ること\"",
        "Opt4": "\"計算が遅くなること\"",
        "Opt5": "\"画像が劣化すること\"",
        "Opt6": "\"ラベルが消えること\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】バッチサイズが増えると1回の更新の重みが変わるため、それに合わせて学習率をスケーリングさせるのが一般的です。\""
    },
    {
        "ID": "\"DL-127\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"中間層の活性化関数として「シグモイド関数」の代わりに「ReLU」が普及した歴史的な理由は？\"",
        "Opt1": "\"ReLUの方が美しいから\"",
        "Opt2": "\"多層化した際の勾配消失問題を実用レベルで解決したから\"",
        "Opt3": "\"ReLUは数学的に複雑だから\"",
        "Opt4": "\"シグモイドは特許があるから\"",
        "Opt5": "\"Googleが決めたから\"",
        "Opt6": "\"GPUで計算できないから\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】ReLUの登場により、3層以上の深いネットワークでも学習が収束するようになり、深層学習が実用化されました。\""
    },
    {
        "ID": "\"DL-128\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"「誤差逆伝播法」を適用できないモデルはどれか？\"",
        "Opt1": "\"微分不可能な処理が含まれるネットワーク\"",
        "Opt2": "\"全結合層のみのネットワーク\"",
        "Opt3": "\"CNN\"",
        "Opt4": "\"RNN\"",
        "Opt5": "\"ResNet\"",
        "Opt6": "\"オートエンコーダ\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】勾配（微分）を使って重みを更新するため、関数がどこかで微分できない（不連続）場合は、特別な工夫がない限り計算が止まります。\""
    },
    {
        "ID": "\"DL-129\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"「回帰」のタスクで、出力層に活性化関数を適用せず、計算結果をそのまま出すことを何というか？\"",
        "Opt1": "\"シグモイド適用\"",
        "Opt2": "\"ソフトマックス適用\"",
        "Opt3": "\"恒等写像（恒等関数）\"",
        "Opt4": "\"ステップ関数\"",
        "Opt5": "\"ゼロクリア\"",
        "Opt6": "\"ドロップアウト\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】数値そのものを予測したいため、変換を行わずに（または直線を掛けるだけで）出力します。\""
    },
    {
        "ID": "\"DL-130\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"ディープラーニングにおいて、入力データの次元数に対して、訓練データのサンプル数が少なすぎるとどうなりやすいか？\"",
        "Opt1": "\"学習が高速になる\"",
        "Opt2": "\"過学習（オーバーフィッティング）が起きやすくなる\"",
        "Opt3": "\"未学習になる\"",
        "Opt4": "\"メモリが余る\"",
        "Opt5": "\"精度が安定する\"",
        "Opt6": "\"何も起きない\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】「次元の呪い」に関連し、データがスカスカな状態で複雑なモデルを使うと、特定のサンプルにのみ特化してしまいます。\""
    },
    {
        "ID": "\"DL-131\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"「クロスバリデーション（交差検証）」を行う目的は？\"",
        "Opt1": "\"データを増やすため\"",
        "Opt2": "\"限られたデータで、より信頼性の高いモデル評価（汎化性能の推定）を行うため\"",
        "Opt3": "\"学習速度を上げるため\"",
        "Opt4": "\"GPUのメモリを節約するため\"",
        "Opt5": "\"欠損値を埋めるため\"",
        "Opt6": "\"重みを正規化するため\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】データを数個のグループに分け、交代でテスト役を務めることで、データの分割の仕方に依存しない評価が可能です。\""
    },
    {
        "ID": "\"DL-132\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"「早期終了（Early Stopping）」で学習を止める基準として最も適切なのは？\"",
        "Opt1": "\"訓練誤差が最小になった時\"",
        "Opt2": "\"検証誤差（Validation Loss）が上がり始めた時\"",
        "Opt3": "\"エポック数が100に達した時\"",
        "Opt4": "\"コンピュータが重くなった時\"",
        "Opt5": "\"正解率が50%になった時\"",
        "Opt6": "\"勾配が1.0になった時\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】検証誤差の反転は過学習の開始を意味するため、その直前で止めるのがベストです。\""
    },
    {
        "ID": "\"DL-133\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"ニューラルネットワークの「重み」を初期化する際、標準偏差を $ \\sqrt{1/n} $ （nは入力ノード数）とする手法の名称は？\"",
        "Opt1": "\"Heの初期化\"",
        "Opt2": "\"Xavierの初期化\"",
        "Opt3": "\"ゼロ初期化\"",
        "Opt4": "\"ガウス初期化\"",
        "Opt5": "\"定数初期化\"",
        "Opt6": "\"直交初期化\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】2010年に提案された。シグモイド系関数の学習を劇的に安定させました。\""
    },
    {
        "ID": "\"DL-134\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"「バッチ正規化」を適用する場所として、論文等で推奨されているのは？\"",
        "Opt1": "\"活性化関数の直前（重みの線形和の直後）\"",
        "Opt2": "\"ネットワークの一番最後のみ\"",
        "Opt3": "\"入力データのみ\"",
        "Opt4": "\"重みの更新後のみ\"",
        "Opt5": "\"損失関数の計算後\"",
        "Opt6": "\"ドロップアウトの直後\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】入力の分布を整えてから活性化関数に通すことで、飽和（勾配消失）を防ぐ効果が高まります。\""
    },
    {
        "ID": "\"DL-135\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"「モーメンタム（Momentum）」における「慣性項」の係数は、通常どのような値に設定されるか？\"",
        "Opt1": "\"0.001前後\"",
        "Opt2": "\"0.9前後\"",
        "Opt3": "\"10.0以上\"",
        "Opt4": "\" -1.0\"",
        "Opt5": "\"0\"",
        "Opt6": "\"無限大\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】過去の勾配の9割を保持し、今の勾配を1割加える、といったバランスで更新をスムーズにします。\""
    },
    {
        "ID": "\"DL-136\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"「Adam」が最適化手法として広く使われる理由は？\"",
        "Opt1": "\"メモリを全く使わないから\"",
        "Opt2": "\"学習率を自動で調整し、モーメンタムの効果も備え、多くの場合で良好な結果が出るから\"",
        "Opt3": "\"1950年代からある枯れた技術だから\"",
        "Opt4": "\"数学的に100%正解に辿り着くことが証明されているから\"",
        "Opt5": "\"活性化関数が不要になるから\"",
        "Opt6": "\"Googleが無料で配布しているから\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】RMSpropとMomentumのハイブリッドのような性質を持ち、実務上の「とりあえずこれ」という手法になっています。\""
    },
    {
        "ID": "\"DL-137\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"「損失関数」を $ E $、重みを $ w $ としたとき、勾配を表す記号はどれか？\"",
        "Opt1": "\"$\\Delta E$\"",
        "Opt2": "\"$\\nabla E$ （ナブラ）\"",
        "Opt3": "\"$E^2$\"",
        "Opt4": "\"$\\sum E$\"",
        "Opt5": "\"$\\log E$\"",
        "Opt6": "\"$\\exp E$\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】ベクトル解析で「各変数での偏微分のベクトル」を意味する記号です。\""
    },
    {
        "ID": "\"DL-138\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"「データ拡張」において、画像認識モデルに対して行うべきではない処理は？\"",
        "Opt1": "\"わずかなノイズ付与\"",
        "Opt2": "\"左右反転\"",
        "Opt3": "\"上下反転（文字認識タスクの場合）\"",
        "Opt4": "\"色のわずかな変更\"",
        "Opt5": "\"一部を隠す（消去）\"",
        "Opt6": "\"微小な回転\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】「6」と「9」や「b」と「p」を区別するタスクで上下反転を行うと、正解ラベルが変わってしまうため不適切です。\""
    },
    {
        "ID": "\"DL-139\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"ニューラルネットワークにおいて、ある層の入力をそのまま出力する関数 $ f(x) = x $ を何というか？\"",
        "Opt1": "\"ステップ関数\"",
        "Opt2": "\"シグモイド関数\"",
        "Opt3": "\"ReLU関数\"",
        "Opt4": "\"恒等関数（Identity function）\"",
        "Opt5": "\"ソフトマックス関数\"",
        "Opt6": "\"tanh関数\"",
        "Answer_Idx": 3,
        "Explanation": "\"【解説】回帰問題の出力層などで、計算結果の数値を加工せずにそのまま出すために使います。\""
    },
    {
        "ID": "\"DL-140\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"ディープラーニングの「学習エポック数」を増やしすぎた場合に懸念されることは？\"",
        "Opt1": "\"未学習\"",
        "Opt2": "\"過学習\"",
        "Opt3": "\"データの消失\"",
        "Opt4": "\"計算機が新品になること\"",
        "Opt5": "\"精度の急激な向上\"",
        "Opt6": "\"バッチサイズの増大\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】学習を長く続けすぎると、モデルが訓練データの些細な特徴（ノイズ）まで「丸暗記」してしまいます。\""
    },
    {
        "ID": "\"DL-141\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"「損失関数」の出力値としてあり得ないものはどれか？（一般的なMSEや交差エントロピーの場合）\"",
        "Opt1": "\"0.0\"",
        "Opt2": "\"100.0\"",
        "Opt3": "\"0.5\"",
        "Opt4": "\"-5.0\"",
        "Opt5": "\"1.0\"",
        "Opt6": "\"無限大\"",
        "Answer_Idx": 3,
        "Explanation": "\"【解説】損失（誤差）は「ズレ」の大きさを示すため、通常は0以上の値をとります。\""
    },
    {
        "ID": "\"DL-142\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"多クラス分類において、あるクラスに対する予測確率が0.9、正解ラベルが1.0の場合、そのデータに対する「誤差」はどうなるか？\"",
        "Opt1": "\"非常に大きい\"",
        "Opt2": "\"非常に小さい（0に近い）\"",
        "Opt3": "\"マイナスになる\"",
        "Opt4": "\"1.0になる\"",
        "Opt5": "\"計算不能\"",
        "Opt6": "\"0.9になる\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】予測が正解に近いため、誤差（損失）は小さくなります。\""
    },
    {
        "ID": "\"DL-143\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"「順伝播（Forward propagation）」の目的は？\"",
        "Opt1": "\"重みを更新すること\"",
        "Opt2": "\"入力データから予測値（出力）を計算すること\"",
        "Opt3": "\"勾配を求めること\"",
        "Opt4": "\"データを拡張すること\"",
        "Opt5": "\"モデルを保存すること\"",
        "Opt6": "\"初期化すること\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】入力を層ごとに順次計算し、最終的な判定結果を出す工程です。\""
    },
    {
        "ID": "\"DL-144\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"「逆伝播（Backward propagation）」の目的は？\"",
        "Opt1": "\"予測値を出すこと\"",
        "Opt2": "\"各パラメータの勾配を計算し、重みを更新すること\"",
        "Opt3": "\"データを入力すること\"",
        "Opt4": "\"画像を反転させること\"",
        "Opt5": "\"GPUを停止させること\"",
        "Opt6": "\"損失を0に固定すること\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】出力層で計算された誤差を後ろから伝え、各層の重みをどれくらい修正すべきか決定します。\""
    },
    {
        "ID": "\"DL-145\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"活性化関数の微分が「定数（1など）」であるメリットは？\"",
        "Opt1": "\"計算が楽で、深い層でも勾配が弱まらずに伝わること\"",
        "Opt2": "\"常に0になること\"",
        "Opt3": "\"無限に大きくなること\"",
        "Opt4": "\"メモリを食わないこと\"",
        "Opt5": "\"バイアスを消せること\"",
        "Opt6": "\"重みが1になること\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】ReLUなどの最大の特徴であり、勾配消失を防いで深層学習を成功させた要因です。\""
    },
    {
        "ID": "\"DL-146\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"「全結合層」の重みの数（パラメータ数）を計算せよ。入力ノード100、出力ノード50の場合（バイアスは無視する）。\"",
        "Opt1": "\"150\"",
        "Opt2": "\"500\"",
        "Opt3": "\"5000\"",
        "Opt4": "\"2\"",
        "Opt5": "\"10050\"",
        "Opt6": "\"50\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】$ 100 \\times 50 = 5000 $ です。すべての入力がすべての出力に繋がるため、掛け算になります。\""
    },
    {
        "ID": "\"DL-147\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"ディープラーニングにおいて、データ1件ごとに重みを更新する手法の欠点は？\"",
        "Opt1": "\"精度が低すぎること\"",
        "Opt2": "\"計算が非常に不安定で、並列化（高速化）もしにくいこと\"",
        "Opt3": "\"過学習が絶対に起きないこと\"",
        "Opt4": "\"画像が扱えないこと\"",
        "Opt5": "\"理論が間違っていること\"",
        "Opt6": "\"メモリが余りすぎること\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】1個ごとに更新するとノイズに振り回されやすく、またGPUの並列計算能力（まとめて計算する力）を活かせません。\""
    },
    {
        "ID": "\"DL-148\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"「シグモイド関数」の出力範囲は？\"",
        "Opt1": "\"[-1",
        "Opt2": "1]\"",
        "Opt3": "\"(0",
        "Opt4": "1)\"",
        "Opt5": "\"[0",
        "Opt6": "無限]\"",
        "Answer_Idx": "\"[-無限",
        "Explanation": "無限]\""
    },
    {
        "ID": "\"DL-149\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"活性化関数「tanh（ハイパボリックタンジェント）」の出力範囲は？\"",
        "Opt1": "\"(0",
        "Opt2": "1)\"",
        "Opt3": "\"[-1",
        "Opt4": "1]\"",
        "Opt5": "\"[0",
        "Opt6": "無限]\"",
        "Answer_Idx": "\"[-無限",
        "Explanation": "0]\""
    },
    {
        "ID": "\"DL-150\"",
        "Category": "\"4.ディープラーニング概要\"",
        "Question": "\"1エポックの学習において、全データ数1000、バッチサイズ100とした場合、重みの更新回数（イテレーション数）は？\"",
        "Opt1": "\"1回\"",
        "Opt2": "\"10回\"",
        "Opt3": "\"100回\"",
        "Opt4": "\"1000回\"",
        "Opt5": "\"10000回\"",
        "Opt6": "\"0.1回\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】$ 1000 \\div 100 = 10 $。10個のミニバッチが処理されるため、更新は10回行われます。\""
    },
    {
        "ID": "\"DL-5-001\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"畳み込みニューラルネットワーク（CNN）において、フィルタ（カーネル）をスライドさせる間隔を何というか？\"",
        "Opt1": "\"パディング\"",
        "Opt2": "\"ストライド\"",
        "Opt3": "\"プーリング\"",
        "Opt4": "\"チャネル\"",
        "Opt5": "\"バイアス\"",
        "Opt6": "\"カーネルサイズ\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】ストライド（Stride）を大きくすると、出力される特徴マップのサイズは小さくなります。\""
    },
    {
        "ID": "\"DL-5-002\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"CNNで入力画像の端の情報を失わないように、周囲に0などの値を埋める操作を何というか？\"",
        "Opt1": "\"プーリング\"",
        "Opt2": "\"ストライド\"",
        "Opt3": "\"パディング\"",
        "Opt4": "\"平滑化\"",
        "Opt5": "\"ドロップアウト\"",
        "Opt6": "\"正規化\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】パディング（Padding）を行うことで、出力サイズを調整したり、端の特徴を保持したりします。\""
    },
    {
        "ID": "\"DL-5-003\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"畳み込み層の直後に置かれ、微小な位置変化に対する不変性を高めたり、計算量を削減したりする層は？\"",
        "Opt1": "\"全結合層\"",
        "Opt2": "\"プーリング層\"",
        "Opt3": "\"回帰層\"",
        "Opt4": "\"再帰層\"",
        "Opt5": "\"正規化層\"",
        "Opt6": "\"活性化層\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】最大値をとる「Max Pooling」や平均をとる「Average Pooling」が一般的です。\""
    },
    {
        "ID": "\"DL-5-004\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"CNNにおいて、最終的な分類を行うために多次元のテンソルを1次元の配列に変換する操作を何というか？\"",
        "Opt1": "\"正則化\"",
        "Opt2": "\"量子化\"",
        "Opt3": "\"平滑化（Flatten）\"",
        "Opt4": "\"蒸留\"",
        "Opt5": "\"剪定\"",
        "Opt6": "\"畳み込み\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】Flattenによって、抽出された特徴を全結合層へ入力できる形式に直します。\""
    },
    {
        "ID": "\"DL-5-005\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"画像認識モデル「VGG16」の構造的な特徴として正しいものはどれか？\"",
        "Opt1": "\"7x7の大きなフィルタを多用する\"",
        "Opt2": "\"3x3の小さなフィルタを重ねて層を深くする\"",
        "Opt3": "\"畳み込み層を全く使わない\"",
        "Opt4": "\"全結合層を持たない\"",
        "Opt5": "\"プーリング層の代わりにドロップアウトを使う\"",
        "Opt6": "\"RNNと組み合わされている\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】小さなフィルタを重ねることで、パラメータ数を抑えつつ受容野（見える範囲）を広げ、非線形性を高めています。\""
    },
    {
        "ID": "\"DL-5-006\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"Googleによって提案され、1x1、3x3、5x5の畳み込みやプーリングを並列に組み合わせる「Inception構造」を採用したモデルは？\"",
        "Opt1": "\"AlexNet\"",
        "Opt2": "\"ResNet\"",
        "Opt3": "\"GoogLeNet\"",
        "Opt4": "\"VGGNet\"",
        "Opt5": "\"LeNet\"",
        "Opt6": "\"MobileNet\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】インセプションモジュールにより、異なるサイズの情報を並列に抽出するのが特徴です。\""
    },
    {
        "ID": "\"DL-5-007\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"ResNetにおいて、層を飛ばして入力を直接後ろに伝える「スキップ接続」により解決された問題は？\"",
        "Opt1": "\"データの欠損\"",
        "Opt2": "\"ハイパーパラメータの爆発\"",
        "Opt3": "\"多層化に伴う勾配消失・劣化問題\"",
        "Opt4": "\"計算時間の増加\"",
        "Opt5": "\"メモリ不足\"",
        "Opt6": "\"画像の解像度低下\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】ショートカットによって勾配が直接伝わるため、100層を超える超多層ネットワークの学習が可能になりました。\""
    },
    {
        "ID": "\"DL-5-008\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"自然言語処理などの時系列データに適した、前の時間の出力を自分の入力として再利用するネットワークは？\"",
        "Opt1": "\"CNN\"",
        "Opt2": "\"RNN（再帰型ニューラルネットワーク）\"",
        "Opt3": "\"GAN\"",
        "Opt4": "\"オートエンコーダ\"",
        "Opt5": "\"ボルツマンマシン\"",
        "Opt6": "\"パーセプトロン\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】Recurrent Neural Network。内部に「状態（メモリ）」を持つことで、文脈などの逐次的な情報を処理します。\""
    },
    {
        "ID": "\"DL-5-009\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"RNNにおいて、過去の情報を長く保持できない「長期依存性の問題」の主な原因は？\"",
        "Opt1": "\"計算速度が遅いこと\"",
        "Opt2": "\"勾配消失または勾配爆発\"",
        "Opt3": "\"メモリが足りないこと\"",
        "Opt4": "\"活性化関数がReLUだから\"",
        "Opt5": "\"正解ラベルが間違っているから\"",
        "Opt6": "\"GPUが使えないから\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】同じ重みを何度も掛け合わせる構造上、層が長くなると勾配が指数関数的に消失・爆発しやすくなります。\""
    },
    {
        "ID": "\"DL-5-010\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"RNNの長期記憶問題を解決するために提案された、入力ゲート、出力ゲート、忘却ゲートを持つユニットは？\"",
        "Opt1": "\"GRU\"",
        "Opt2": "\"LSTM（Long Short-Term Memory）\"",
        "Opt3": "\"ReLU\"",
        "Opt4": "\"Affine\"",
        "Opt5": "\"Batch Norm\"",
        "Opt6": "\"Attention\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】セルの状態（記憶）をゲートで制御することで、必要な情報を長期間保持し、不要な情報を捨てることができます。\""
    },
    {
        "ID": "\"DL-5-011\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"LSTMを簡略化し、ゲートの数を2つ（更新ゲート・リセットゲート）に減らした計算効率の良いモデルは？\"",
        "Opt1": "\"RNN\"",
        "Opt2": "\"CNN\"",
        "Opt3": "\"GRU（Gated Recurrent Unit）\"",
        "Opt4": "\"ResNet\"",
        "Opt5": "\"Transformer\"",
        "Opt6": "\"BERT\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】パラメータ数が少ないため学習が速く、小規模なデータセットでLSTMと同等の性能を出すことがあります。\""
    },
    {
        "ID": "\"DL-5-012\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"文章の翻訳などにおいて、入力系列から固定長のベクトルを生成し、それを元に出力系列を生成するモデルを何というか？\"",
        "Opt1": "\"GAN\"",
        "Opt2": "\"Seq2Seq（Encoder-Decoderモデル）\"",
        "Opt3": "\"自己符号化器\"",
        "Opt4": "\"分類器\"",
        "Opt5": "\"回帰器\"",
        "Opt6": "\"畳み込み\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】Sequence-to-Sequence。翻訳、要約、対話システムなどで基礎となる構造です。\""
    },
    {
        "ID": "\"DL-5-013\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"Seq2Seqにおいて、入力系列のどの部分を重視すべきかを動的に重み付けする機構を何というか？\"",
        "Opt1": "\"ドロップアウト\"",
        "Opt2": "\"バッチ正規化\"",
        "Opt3": "\"Attention（注意機構）\"",
        "Opt4": "\"プーリング\"",
        "Opt5": "\"パディング\"",
        "Opt6": "\"平滑化\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】長い文章でも「今、訳すべき単語」に焦点を当てることができるようになり、精度が劇的に向上しました。\""
    },
    {
        "ID": "\"DL-5-014\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"RNNを一切使わず、Attention機構のみで構成された、現在の自然言語処理の主流となっているモデルは？\"",
        "Opt1": "\"LSTM\"",
        "Opt2": "\"GRU\"",
        "Opt3": "\"Transformer\"",
        "Opt4": "\"VGG\"",
        "Opt5": "\"LeNet\"",
        "Opt6": "\"AlexNet\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】並列計算が可能で非常に高速かつ高性能。BERTやGPTの基盤技術です。\""
    },
    {
        "ID": "\"DL-5-015\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"Transformerの内部で使われる、自分自身の文の中での単語同士の関連性を計算する仕組みを何というか？\"",
        "Opt1": "\"Cross Attention\"",
        "Opt2": "\"Self-Attention\"",
        "Opt3": "\"Global Attention\"",
        "Opt4": "\"Local Attention\"",
        "Opt5": "\"Simple Attention\"",
        "Opt6": "\"Multi-layer Attention\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】「it」が何を指しているかなど、文内の文脈を把握するのに非常に強力です。\""
    },
    {
        "ID": "\"DL-5-016\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"BERTが学習するタスクの一つで、文中の一部の単語を隠してそれを予測させるタスクを何というか？\"",
        "Opt1": "\"次文予測\"",
        "Opt2": "\"マスク付き言語モデル（MLM）\"",
        "Opt3": "\"感情分析\"",
        "Opt4": "\"固有表現抽出\"",
        "Opt5": "\"文法チェック\"",
        "Opt6": "\"要約\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】Masked Language Model。文の前後両方向の文脈を使って学習を行うことがBERTの強みです。\""
    },
    {
        "ID": "\"DL-5-017\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"画像から「何が」「どこに（座標）」あるかを同時に特定するタスクを何というか？\"",
        "Opt1": "\"画像分類\"",
        "Opt2": "\"物体検出（Object Detection）\"",
        "Opt3": "\"セマンティックセグメンテーション\"",
        "Opt4": "\"インスタンスセグメンテーション\"",
        "Opt5": "\"超解像\"",
        "Opt6": "\"画像生成\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】「何が」を特定する分類と、「どこに」を特定する回帰の両方を行います。\""
    },
    {
        "ID": "\"DL-5-018\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"物体検出において、あらかじめ設定された様々なサイズの矩形領域を何というか？\"",
        "Opt1": "\"境界ボックス\"",
        "Opt2": "\"アンカーボックス（またはデフォルトボックス）\"",
        "Opt3": "\"パディングエリア\"",
        "Opt4": "\"プーリングサイズ\"",
        "Opt5": "\"ストライドマップ\"",
        "Opt6": "\"特徴マップ\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】このボックスを基準に、実際の物体の位置とのズレを予測します。\""
    },
    {
        "ID": "\"DL-5-019\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"物体検出モデルの中で、物体の候補領域を探す部分と分類する部分を分ける手法を何というか？\"",
        "Opt1": "\"1段階（One-stage）手法\"",
        "Opt2": "\"2段階（Two-stage）手法\"",
        "Opt3": "\"エンドツーエンド手法\"",
        "Opt4": "\"全結合手法\"",
        "Opt5": "\"リカレント手法\"",
        "Opt6": "\"生成手法\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】R-CNN系が代表的です。精度は高いですが、計算速度は比較的遅めです。\""
    },
    {
        "ID": "\"DL-5-020\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"YOLOやSSDのように、領域候補の選別と分類を単一のネットワークで同時に行う高速な物体検出手法は？\"",
        "Opt1": "\"R-CNN\"",
        "Opt2": "\"Faster R-CNN\"",
        "Opt3": "\"1段階（One-stage）手法\"",
        "Opt4": "\"Seq2Seq\"",
        "Opt5": "\"Transformer\"",
        "Opt6": "\"DeepLab\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】You Only Look Once（YOLO）。リアルタイムでの検出が可能で、自動運転や監視カメラなどで重宝されます。\""
    },
    {
        "ID": "\"DL-5-021\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"物体検出の評価指標として使われる、予測した枠と正解の枠の「重なり具合」を示す指標は？\"",
        "Opt1": "\"Precision\"",
        "Opt2": "\"Recall\"",
        "Opt3": "\"IoU（Intersection over Union）\"",
        "Opt4": "\"mAP\"",
        "Opt5": "\"F1-score\"",
        "Opt6": "\"Accuracy\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】（重なった面積）/（全体の面積）で計算され、通常0.5以上だと正解とみなされます。\""
    },
    {
        "ID": "\"DL-5-022\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"画像内のすべてのピクセルに対してクラスを割り当てるが、同じクラスの個々の物体（車Aと車Bなど）は区別しない手法は？\"",
        "Opt1": "\"物体検出\"",
        "Opt2": "\"セマンティックセグメンテーション\"",
        "Opt3": "\"インスタンスセグメンテーション\"",
        "Opt4": "\"パノプティックセグメンテーション\"",
        "Opt5": "\"画像分類\"",
        "Opt6": "\"超解像\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】Semantic Segmentation。空、道路、建物などの「領域」を塗り分けるイメージです。\""
    },
    {
        "ID": "\"DL-5-023\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"セマンティックセグメンテーションにおいて、縮小された特徴マップを元の解像度に戻す操作を何というか？\"",
        "Opt1": "\"畳み込み\"",
        "Opt2": "\"プーリング\"",
        "Opt3": "\"アップサンプリング（または逆畳み込み）\"",
        "Opt4": "\"平滑化\"",
        "Opt5": "\"ドロップアウト\"",
        "Opt6": "\"早期終了\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】エンコーダで抽出した特徴を、デコーダで元のサイズに引き延ばしてピクセルごとの分類を行います。\""
    },
    {
        "ID": "\"DL-5-024\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"医療画像のセグメンテーションでよく使われる、エンコーダ層の特徴をデコーダ層へ直接渡す「スキップ接続」を持つU字型のモデルは？\"",
        "Opt1": "\"VGG\"",
        "Opt2": "\"ResNet\"",
        "Opt3": "\"U-Net\"",
        "Opt4": "\"Inception\"",
        "Opt5": "\"YOLO\"",
        "Opt6": "\"Transformer\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】形がU字に見えることから命名。細かい位置情報を補うことができるため、高精度なセグメンテーションが可能です。\""
    },
    {
        "ID": "\"DL-5-025\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"生成モデルの「GAN（敵対的生成ネットワーク）」を構成する2つのネットワークの組み合わせは？\"",
        "Opt1": "\"EncoderとDecoder\"",
        "Opt2": "\"Generator（生成器）とDiscriminator（識別器）\"",
        "Opt3": "\"InputとOutput\"",
        "Opt4": "\"ActorとCritic\"",
        "Opt5": "\"AgentとEnvironment\"",
        "Opt6": "\"CNNとRNN\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】Generatorが偽物を作り、Discriminatorが本物か偽物かを見破ることで、互いに精度を高め合います。\""
    },
    {
        "ID": "\"DL-5-026\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"GANの学習において、Generator（生成器）が目指すゴールとして正しいものはどれか？\"",
        "Opt1": "\"損失を最大化すること\"",
        "Opt2": "\"Discriminatorを完璧に騙すこと（本物と間違えさせること）\"",
        "Opt3": "\"本物データを消去すること\"",
        "Opt4": "\"画像を分類すること\"",
        "Opt5": "\"計算速度を上げること\"",
        "Opt6": "\"メモリを節約すること\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】Discriminatorに「本物である確率0.5」と答えさせることができれば、完璧な偽物が作れたことになります。\""
    },
    {
        "ID": "\"DL-5-027\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"オートエンコーダ（自己符号化器）において、入力データを圧縮した後の低次元な表現を何というか？\"",
        "Opt1": "\"ノイズ\"",
        "Opt2": "\"潜在変数（潜在空間）\"",
        "Opt3": "\"正解ラベル\"",
        "Opt4": "\"重み行列\"",
        "Opt5": "\"バイアス\"",
        "Opt6": "\"活性化値\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】Latent Variable。データの「本質的な特徴」が凝縮された情報です。\""
    },
    {
        "ID": "\"DL-5-028\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"潜在変数に確率分布（通常はガウス分布）を導入し、そこからサンプリングすることで新しいデータを生成できるオートエンコーダは？\"",
        "Opt1": "\"Denoising Autoencoder\"",
        "Opt2": "\"VAE（変分自己符号化器）\"",
        "Opt3": "\"GAN\"",
        "Opt4": "\"Sparse Autoencoder\"",
        "Opt5": "\"Stacked Autoencoder\"",
        "Opt6": "\"CNN\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】Variational Autoencoder。生成モデルの一つであり、潜在空間を連続的に扱えるのが特徴です。\""
    },
    {
        "ID": "\"DL-5-029\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"GANの学習において、生成器が特定のパターンの画像ばかりを生成してしまい、多様性が失われる現象を何というか？\"",
        "Opt1": "\"勾配消失\"",
        "Opt2": "\"過学習\"",
        "Opt3": "\"モード崩壊（Mode Collapse）\"",
        "Opt4": "\"次元の呪い\"",
        "Opt5": "\"剪定\"",
        "Opt6": "\"蒸留\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】学習が不安定なGANでよく起きる問題。これを防ぐために様々な改良GAN（WGANなど）が提案されています。\""
    },
    {
        "ID": "\"DL-5-030\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"最近の画像生成AI（Stable Diffusionなど）の基盤技術で、データにノイズを加えていき、それを逆順に除去していくことで画像を生成する手法は？\"",
        "Opt1": "\"GAN\"",
        "Opt2": "\"VAE\"",
        "Opt3": "\"拡散モデル（Diffusion Model）\"",
        "Opt4": "\"RNN\"",
        "Opt5": "\"CNN\"",
        "Opt6": "\"Transformer\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】ノイズから徐々に鮮明な画像を作り出すプロセスを学習します。\""
    },
    {
        "ID": "\"DL-5-031\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"強化学習において、エージェントが行動を決定するための「ルール」や「戦略」を何というか？\"",
        "Opt1": "\"報酬\"",
        "Opt2": "\"状態\"",
        "Opt3": "\"方策（ポリシー）\"",
        "Opt4": "\"環境\"",
        "Opt5": "\"価値\"",
        "Opt6": "\"割引率\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】Policy。どの状態でどの行動をとるかの確率分布を指します。\""
    },
    {
        "ID": "\"DL-5-032\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"強化学習において、将来得られる報酬を現在の価値に換算する際に用いる係数を何というか？\"",
        "Opt1": "\"学習率\"",
        "Opt2": "\"割引率\"",
        "Opt3": "\"探索率\"",
        "Opt4": "\"正則化係数\"",
        "Opt5": "\"モーメンタム\"",
        "Opt6": "\"慣性\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】Discount rate。通常0から1の値をとり、1に近いほど遠い将来の報酬を重視します。\""
    },
    {
        "ID": "\"DL-5-033\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"エージェントが、今の知識を使って最大の報酬を得る「活用」と、新しい行動を試す「探索」のバランスを何というか？\"",
        "Opt1": "\"バイアス・バリアンス・トレードオフ\"",
        "Opt2": "\"探索と活用のトレードオフ\"",
        "Opt3": "\"過学習と未学習\"",
        "Opt4": "\"次元の呪い\"",
        "Opt5": "\"勾配消失問題\"",
        "Opt6": "\"精度と速度のトレードオフ\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】強化学習の根幹的な課題です。一方に偏ると、最適な行動に辿り着けません。\""
    },
    {
        "ID": "\"DL-5-034\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"Q学習に深層学習（CNNなど）を組み合わせ、ゲームの画面入力から直接行動を学習できるようにしたモデルは？\"",
        "Opt1": "\"AlphaGo\"",
        "Opt2": "\"DQN（Deep Q-Network）\"",
        "Opt3": "\"Seq2Seq\"",
        "Opt4": "\"GAN\"",
        "Opt5": "\"BERT\"",
        "Opt6": "\"ResNet\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】DeepMindが発表。経験再生（Experience Replay）などの工夫により学習を安定させました。\""
    },
    {
        "ID": "\"DL-5-035\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"DQNにおいて、過去の「状態・行動・報酬・次の状態」をメモリに保存しておき、ランダムにサンプリングして学習に使う手法は？\"",
        "Opt1": "\"ターゲットネットワーク\"",
        "Opt2": "\"経験再生（Experience Replay）\"",
        "Opt3": "\"バッチ正規化\"",
        "Opt4": "\"ドロップアウト\"",
        "Opt5": "\"割引報酬\"",
        "Opt6": "\"方策勾配\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】データの相関を断ち切り、学習を安定させる非常に重要なテクニックです。\""
    },
    {
        "ID": "\"DL-5-036\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"方策（ポリシー）を直接パラメータ化し、報酬の期待値が最大になるように勾配法で更新する手法を何というか？\"",
        "Opt1": "\"Q学習\"",
        "Opt2": "\"方策勾配法\"",
        "Opt3": "\"モンテカルロ法\"",
        "Opt4": "\"動的計画法\"",
        "Opt5": "\"教師あり学習\"",
        "Opt6": "\"半教師あり学習\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】価値関数を経由せず、直接「どの行動をとるべきか」を学習します。\""
    },
    {
        "ID": "\"DL-5-037\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"囲碁AI「AlphaGo」において、次に打つべき手の探索を効率化するために用いられた手法は？\"",
        "Opt1": "\"グリッドサーチ\"",
        "Opt2": "\"モンテカルロ木探索\"",
        "Opt3": "\"幅優先探索\"",
        "Opt4": "\"深さ優先探索\"",
        "Opt5": "\"ランダムウォーク\"",
        "Opt6": "\"全探索\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】ニューラルネットワークによる価値判断と、ランダムなシミュレーション（モンテカルロ法）を組み合わせています。\""
    },
    {
        "ID": "\"DL-5-038\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"音声認識において、音声の波形から時間的な特徴を抽出する際によく使われる変換手法は？\"",
        "Opt1": "\"フーリエ変換（またはメル周波数ケプストラム係数：MFCC）\"",
        "Opt2": "\"シグモイド変換\"",
        "Opt3": "\"ハフ変換\"",
        "Opt4": "\"カーネル変換\"",
        "Opt5": "\"アフィン変換\"",
        "Opt6": "\"ウェーブレット変換\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】音声を周波数成分に分解し、人間が聞き取りやすい特徴量に変換してCNN等に入力します。\""
    },
    {
        "ID": "\"DL-5-039\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"動画認識タスクにおいて、画像の2次元情報に「時間」の次元を加えて処理するCNNを何というか？\"",
        "Opt1": "\"1D-CNN\"",
        "Opt2": "\"2D-CNN\"",
        "Opt3": "\"3D-CNN\"",
        "Opt4": "\"RNN\"",
        "Opt5": "\"LSTM\"",
        "Opt6": "\"GRU\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】縦・横・時間にわたるフィルタを用いることで、動きの特徴を捉えます。\""
    },
    {
        "ID": "\"DL-040\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"大規模言語モデル（LLM）において、特定のタスクに適応させるために、あらかじめ学習したモデルを少量のデータで追加学習させることを何というか？\"",
        "Opt1": "\"事前学習\"",
        "Opt2": "\"蒸留\"",
        "Opt3": "\"ファインチューニング（微調整）\"",
        "Opt4": "\"剪定\"",
        "Opt5": "\"正規化\"",
        "Opt6": "\"初期化\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】Fine-tuning。汎用的な知識を持つモデルを、専門的なタスク（医療、法律など）に特化させます。\""
    },
    {
        "ID": "\"DL-041\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"BERTが「双方向」であると言われる理由は？\"",
        "Opt1": "\"入力と出力を同時に行うから\"",
        "Opt2": "\"文の左から右、右から左の両方の文脈を同時に考慮して学習するから\"",
        "Opt3": "\"RNNを2つ重ねているから\"",
        "Opt4": "\"画像を反転させて学習するから\"",
        "Opt5": "\"GPUとCPUを交互に使うから\"",
        "Opt6": "\"教師ありと教師なしを混ぜるから\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】Transformerのエンコーダを用いることで、従来（左から右のみ）よりも深い文脈理解が可能になりました。\""
    },
    {
        "ID": "\"DL-042\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"画像生成GANにおいて、より高解像度で安定した画像を生成するために、低解像度から段階的に層を増やしていく手法は？\"",
        "Opt1": "\"DCGAN\"",
        "Opt2": "\"CycleGAN\"",
        "Opt3": "\"PGGAN（Progressive Growing of GANs）\"",
        "Opt4": "\"StyleGAN\"",
        "Opt5": "\"VAE\"",
        "Opt6": "\"U-Net\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】4x4から始め、8x8、16x16...と解像度を上げて学習することで、高精細な画像生成に成功しました。\""
    },
    {
        "ID": "\"DL-043\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"「馬の画像」を「ゼブラ（シマウマ）の画像」に変換するように、ペアでないデータセット間で画風変換を行うGANは？\"",
        "Opt1": "\"DCGAN\"",
        "Opt2": "\"CycleGAN\"",
        "Opt3": "\"Pix2Pix\"",
        "Opt4": "\"VAE\"",
        "Opt5": "\"SRGAN\"",
        "Opt6": "\"BigGAN\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】一貫性（Cycle Consistency）を利用し、変換して元に戻しても同じ画像になるように学習します。\""
    },
    {
        "ID": "\"DL-044\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"低解像度の画像を高解像度に変換する「超解像（Super-Resolution）」にGANを応用したモデルは？\"",
        "Opt1": "\"DCGAN\"",
        "Opt2": "\"SRGAN\"",
        "Opt3": "\"StyleGAN\"",
        "Opt4": "\"VAE\"",
        "Opt5": "\"U-Net\"",
        "Opt6": "\"SSD\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】従来のMSE（平均二乗誤差）最小化よりも、人間にとって「リアル」に見えるテクスチャを生成できます。\""
    },
    {
        "ID": "\"DL-045\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"音声合成（TTS）において、テキストから自然な人間の声を生成する深層学習モデルの代表例は？\"",
        "Opt1": "\"WaveNet\"",
        "Opt2": "\"VGG\"",
        "Opt3": "\"BERT\"",
        "Opt4": "\"YOLO\"",
        "Opt5": "\"AlphaGo\"",
        "Opt6": "\"ResNet\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】Google（DeepMind）が開発。1つ前の音の波形から次の波形を予測する自己回帰的な構造です。\""
    },
    {
        "ID": "\"DL-046\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"複数の異なる種類のデータ（画像とテキスト、音声と映像など）を組み合わせて学習・処理する手法を何というか？\"",
        "Opt1": "\"アンサンブル学習\"",
        "Opt2": "\"マルチモーダル学習\"",
        "Opt3": "\"マルチタスク学習\"",
        "Opt4": "\"転移学習\"",
        "Opt5": "\"強化学習\"",
        "Opt6": "\"能動学習\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】Multimodal learning。画像から説明文を作る（画像キャプショニング）などが含まれます。\""
    },
    {
        "ID": "\"DL-047\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"モデルの予測結果に対して、どの入力（画像のどの部分やどの単語）が寄与したかを可視化する手法の代表例は？\"",
        "Opt1": "\"L2正則化\"",
        "Opt2": "\"Grad-CAM\"",
        "Opt3": "\"ドロップアウト\"",
        "Opt4": "\"バッチ正規化\"",
        "Opt5": "\"早期終了\"",
        "Opt6": "\"主成分分析\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】勾配を利用して、画像の「注目エリア」をヒートマップで示し、AIの判断根拠（説明性）を助けます。\""
    },
    {
        "ID": "\"DL-048\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"入力データに対して、人間には判別できない程度の微小なノイズを加え、AIを意図的に誤認させる攻撃を何というか？\"",
        "Opt1": "\"ブルートフォース攻撃\"",
        "Opt2": "\"SQLインジェクション\"",
        "Opt3": "\"敵対的攻撃（Adversarial Attack）\"",
        "Opt4": "\"DoS攻撃\"",
        "Opt5": "\"フィッシング\"",
        "Opt6": "\"なりすまし\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】AIの脆さを突く攻撃で、対策として「敵対的学習（Adversarial Training）」などが行われます。\""
    },
    {
        "ID": "\"DL-049\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"特定のタスクに対するニューラルネットワークの構造（層の数や種類など）を、AI自体に自動で探索・設計させる技術は？\"",
        "Opt1": "\"NAS（Neural Architecture Search）\"",
        "Opt2": "\"SGD\"",
        "Opt3": "\"CNN\"",
        "Opt4": "\"RNN\"",
        "Opt5": "\"GAN\"",
        "Opt6": "\"VAE\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】人間が試行錯誤してネットワークを組む代わりに、強化学習などを用いて最適な構造を見つけます。\""
    },
    {
        "ID": "\"DL-050\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"エッジデバイス（スマートフォンやIoT機器）でAIを効率よく動かすために行われる、重みの値を間引く技術は？\"",
        "Opt1": "\"量子化\"",
        "Opt2": "\"剪定（プルーニング）\"",
        "Opt3": "\"蒸留\"",
        "Opt4": "\"データ拡張\"",
        "Opt5": "\"正規化\"",
        "Opt6": "\"平滑化\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】重要度の低い（値が小さい）重みを0にして削除することで、計算量とメモリ消費を劇的に抑えます。\""
    },
    {
        "ID": "\"DL-5-051\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"モバイルデバイス向けCNNであるMobileNetで採用されている、計算量を劇的に削減する畳み込み手法は？\"",
        "Opt1": "\"拡張畳み込み（Dilated Convolution）\"",
        "Opt2": "\"深層分離畳み込み（Depthwise Separable Convolution）\"",
        "Opt3": "\"転置畳み込み（Transposed Convolution）\"",
        "Opt4": "\"点単位畳み込み（Pointwise Convolution）のみ\"",
        "Opt5": "\"1x1畳み込みの排除\"",
        "Opt6": "\"全結合層の完全削除\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】Depthwise畳み込み（各チャネルごと）とPointwise畳み込み（1x1でチャネル統合）に分離することで、精度を保ちつつ計算量を1/8～1/9程度に削減します。\""
    },
    {
        "ID": "\"DL-5-052\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"画像認識において、入力画像の解像度は変えずに受容野（見える範囲）を広げるために、フィルタの要素間に隙間を設ける手法は？\"",
        "Opt1": "\"パディング\"",
        "Opt2": "\"ストライド\"",
        "Opt3": "\"拡張畳み込み（Dilated / Atrous Convolution）\"",
        "Opt4": "\"最大プーリング\"",
        "Opt5": "\"平均プーリング\"",
        "Opt6": "\"スキップ接続\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】フィルタ間に空間（Dilation）を設けることで、解像度を維持したまま広い範囲の特徴を抽出できます。セグメンテーション（DeepLab等）で多用されます。\""
    },
    {
        "ID": "\"DL-5-053\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"Transformerの学習において、Decoder側が未来の情報を参照しないようにするために適用される処理は？\"",
        "Opt1": "\"バッチ正規化\"",
        "Opt2": "\"ドロップアウト\"",
        "Opt3": "\"マスキング（Masked Self-Attention）\"",
        "Opt4": "\"パディング\"",
        "Opt5": "\"平滑化\"",
        "Opt6": "\"量子化\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】推論時に未来の単語は未知であるため、学習時も未来の単語へのAttentionの重みを0に強制（マスク）して計算します。\""
    },
    {
        "ID": "\"DL-5-054\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"TransformerのAttention機構において、各単語の「意味・内容」を表すベクトルとして、Query、Keyと共に用いられるものは？\"",
        "Opt1": "\"Bias\"",
        "Opt2": "\"Weight\"",
        "Opt3": "\"Value\"",
        "Opt4": "\"Scale\"",
        "Opt5": "\"Mask\"",
        "Opt6": "\"Token\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】Query（検索条件）とKey（検索対象）の一致度を計算し、その結果に基づいてValue（値）を重み付け加算します。\""
    },
    {
        "ID": "\"DL-5-055\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"Transformerにおいて、単語の「順序」情報をモデルに伝えるために、入力ベクトルに加算される固定値のベクトルを何というか？\"",
        "Opt1": "\"位置エンコーディング（Positional Encoding）\"",
        "Opt2": "\"単語埋め込み（Word Embedding）\"",
        "Opt3": "\"バイアス項\"",
        "Opt4": "\"注意重み\"",
        "Opt5": "\"ストライド\"",
        "Opt6": "\"コンテキストベクトル\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】TransformerはRNNと異なり系列を並列処理するため、単語の位置関係が失われます。これを補うためにsin/cos関数などを用いた位置情報を加算します。\""
    },
    {
        "ID": "\"DL-5-056\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"物体検出の代表的なアルゴリズム「SSD（Single Shot MultiBox Detector）」の特徴として正しいものは？\"",
        "Opt1": "\"候補領域を提案してから分類する2段階構成である\"",
        "Opt2": "\"異なる解像度の複数の特徴マップから物体を検出する\"",
        "Opt3": "\"RNNを内部に持っている\"",
        "Opt4": "\"全結合層を10層重ねている\"",
        "Opt5": "\"必ず白黒画像で学習する\"",
        "Opt6": "\"動画にしか適用できない\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】浅い層（高解像度）で小さい物体を、深い層（低解像度）で大きい物体を検出することで、様々なサイズの物体に一度の処理で対応します。\""
    },
    {
        "ID": "\"DL-5-057\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"物体検出で、同一物体に対して複数の予測ボックスが出てしまった際に、スコアが最大のものを残して他を消去する処理は？\"",
        "Opt1": "\"バッチ正規化\"",
        "Opt2": "\"NMS（Non-Maximum Suppression：非最大値抑制）\"",
        "Opt3": "\"IoU計算\"",
        "Opt4": "\"データ拡張\"",
        "Opt5": "\"平滑化\"",
        "Opt6": "\"早期終了\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】重なり（IoU）が一定以上のボックスのうち、信頼度スコアが低いものを排除して検出結果を整理します。\""
    },
    {
        "ID": "\"DL-5-058\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"セグメンテーションモデル「Mask R-CNN」が「Faster R-CNN」に加えて出力するものは？\"",
        "Opt1": "\"物体の座標のみ\"",
        "Opt2": "\"物体のクラスのみ\"",
        "Opt3": "\"ピクセル単位のマスク（物体の形）\"",
        "Opt4": "\"物体の将来の移動方向\"",
        "Opt5": "\"画像のキャプション\"",
        "Opt6": "\"深度マップ\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】インスタンスセグメンテーションの手法であり、物体検出の枠に加え、個々の物体の形を塗り分けるマスクを出力します。\""
    },
    {
        "ID": "\"DL-5-059\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"オートエンコーダの派生で、入力に意図的にノイズを加え、元のノイズのない画像を復元するように学習させる手法は？\"",
        "Opt1": "\"変分自己符号化器（VAE）\"",
        "Opt2": "\"積層自己符号化器\"",
        "Opt3": "\"ノイズ除去自己符号化器（Denoising Autoencoder）\"",
        "Opt4": "\"スパース自己符号化器\"",
        "Opt5": "\"GAN\"",
        "Opt6": "\"CNN\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】ノイズ耐性を高めることで、データのより本質的な特徴を学習させることが可能です。\""
    },
    {
        "ID": "\"DL-5-060\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"GANの派生で、Generatorに入力するノイズに「ラベル（クラス情報）」などを付加し、生成する画像を制御する手法は？\"",
        "Opt1": "\"DCGAN\"",
        "Opt2": "\"CycleGAN\"",
        "Opt3": "\"CGAN（Conditional GAN）\"",
        "Opt4": "\"Pix2Pix\"",
        "Opt5": "\"StyleGAN\"",
        "Opt6": "\"SRGAN\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】「猫」という条件（Condition）を与えれば猫の画像を生成できるようになります。\""
    },
    {
        "ID": "\"DL-5-061\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"強化学習のQ学習（Q-Learning）において、更新式に含まれる「max Q(s'",
        "Opt1": "a')」が意味することは？\"",
        "Opt2": "\"現在の報酬を最大化すること\"",
        "Opt3": "\"次の状態で得られる最大の価値（見積もり）を現在の価値に反映すること\"",
        "Opt4": "\"過去の失敗を忘れること\"",
        "Opt5": "\"行動をランダムに選ぶこと\"",
        "Opt6": "\"環境をリセットすること\"",
        "Answer_Idx": "\"計算を停止すること\"",
        "Explanation": 1
    },
    {
        "ID": "\"DL-5-062\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"強化学習において、エージェントが現在の方策に従って行動し、その結果得られた報酬から価値を推定する手法を何というか？\"",
        "Opt1": "\"オンポリシー（方策内）型手法\"",
        "Opt2": "\"オフポリシー（方策外）型手法\"",
        "Opt3": "\"教師あり学習\"",
        "Opt4": "\"能動学習\"",
        "Opt5": "\"アンサンブル学習\"",
        "Opt6": "\"転移学習\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】SARSAなどが代表例です。一方で、自分の今の方策とは無関係な過去の経験などから学ぶ手法をオフポリシー（Q学習など）と呼びます。\""
    },
    {
        "ID": "\"DL-5-063\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"「AlphaGo Zero」が、従来のAlphaGo（LeeやMaster）と大きく異なる点は何か？\"",
        "Opt1": "\"プロ棋士の棋譜データを使わず、自己対局のみで強くなった\"",
        "Opt2": "\"GPUを使わなくなった\"",
        "Opt3": "\"RNNを導入した\"",
        "Opt4": "\"囲碁以外のゲームができなくなった\"",
        "Opt5": "\"人間との対局をやめた\"",
        "Opt6": "\"探索を一切行わない\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】人間の知識（棋譜）を一切借りず、強化学習のみで人間を凌駕したことで大きな衝撃を与えました。\""
    },
    {
        "ID": "\"DL-5-064\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"音声をテキストに変換する「音声認識」において、音響モデルとしてRNNの代わりにCNNやTransformerを使う理由として適切なのは？\"",
        "Opt1": "\"並列計算が可能で学習が速く、長期間の依存関係も捉えやすいから\"",
        "Opt2": "\"音声は画像と同じだから\"",
        "Opt3": "\"計算が簡単だから\"",
        "Opt4": "\"RNNは音声に使えないから\"",
        "Opt5": "\"メモリを消費しないから\"",
        "Opt6": "\"マイクが不要になるから\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】音声も時系列データですが、CNNの1D畳み込みやTransformerのAttentionが有効であることが近年のトレンドです。\""
    },
    {
        "ID": "\"DL-5-065\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"画像から「この画像は公園で子供が遊んでいる」といった説明文を生成するタスクを何というか？\"",
        "Opt1": "\"画像分類\"",
        "Opt2": "\"物体検出\"",
        "Opt3": "\"画像キャプショニング\"",
        "Opt4": "\"セグメンテーション\"",
        "Opt5": "\"超解像\"",
        "Opt6": "\"スタイル変換\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】CNN（画像特徴抽出）とRNN/Transformer（文章生成）を組み合わせたマルチモーダルなタスクです。\""
    },
    {
        "ID": "\"DL-5-066\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"大規模言語モデルのGPTシリーズにおいて、パラメータ数を増やし続けることで生じる「スケーリング則」が示す内容は？\"",
        "Opt1": "\"精度は途中で必ず下がる\"",
        "Opt2": "\"計算量、データ量、パラメータ数を増やすほど、性能はべき乗則に従って向上し続ける\"",
        "Opt3": "\"モデルは小さい方が良い\"",
        "Opt4": "\"日本語は学習できない\"",
        "Opt5": "\"インターネットが壊れる\"",
        "Opt6": "\"GPUが不要になる\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】OpenAIによって提唱され、LLMの巨大化競争の理論的根拠となりました。\""
    },
    {
        "ID": "\"DL-5-067\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"BERTなどのモデルにおいて、特定の単語（Appleなど）が文脈によって異なる意味（リンゴか企業か）を持つことを考慮したベクトル表現を何というか？\"",
        "Opt1": "\"静的単語分散表現（Word2Vec）\"",
        "Opt2": "\"文脈依存型単語表現（Contextualized Embeddings）\"",
        "Opt3": "\"One-hotベクトル\"",
        "Opt4": "\"スカラー値\"",
        "Opt5": "\"バイアス表現\"",
        "Opt6": "\"固有表現\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】Word2Vecは1単語1ベクトルでしたが、BERTなどは周辺の単語に応じてベクトルの値を変化させます。\""
    },
    {
        "ID": "\"DL-5-068\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"物体検出の精度評価指標「mAP（mean Average Precision）」は、何について平均をとったものか？\"",
        "Opt1": "\"全画像数\"",
        "Opt2": "\"全クラス（カテゴリ）\"",
        "Opt3": "\"全エポック\"",
        "Opt4": "\"全GPU\"",
        "Opt5": "\"全ボックス\"",
        "Opt6": "\"全信頼度\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】各クラスごとのAP（Average Precision）を計算し、全クラス分で平均したものです。\""
    },
    {
        "ID": "\"DL-5-069\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"画像分類モデルの評価で、正解を正解と、不正解を不正解と予測した割合をマトリックスにしたものを何というか？\"",
        "Opt1": "\"散布図\"",
        "Opt2": "\"混同行列（Confusion Matrix）\"",
        "Opt3": "\"ヒストグラム\"",
        "Opt4": "\"決定木\"",
        "Opt5": "\"ROC曲線\"",
        "Opt6": "\"PR曲線\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】縦軸に正解クラス、横軸に予測クラスを並べることで、どのクラスを間違えやすいか可視化します。\""
    },
    {
        "ID": "\"DL-5-070\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"モデルの評価指標において、「実際に正解であるもののうち、どれだけを正解と予測できたか」を示す割合は？\"",
        "Opt1": "\"適合率（Precision）\"",
        "Opt2": "\"再現率（Recall / 感度）\"",
        "Opt3": "\"F値\"",
        "Opt4": "\"正解率（Accuracy）\"",
        "Opt5": "\"特異度\"",
        "Opt6": "\"誤差率\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】再現率は「見逃しの少なさ」を重視する指標（病気の診断など）で重要です。\""
    },
    {
        "ID": "\"DL-5-071\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"適合率（Precision）と再現率（Recall）がトレードオフの関係にあるとき、そのバランスをとった指標は？\"",
        "Opt1": "\"正解率\"",
        "Opt2": "\"F値（F-measure / F1-score）\"",
        "Opt3": "\"決定係数 $R^2$\"",
        "Opt4": "\"相関係数\"",
        "Opt5": "\"p値\"",
        "Opt6": "\"カイ二乗値\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】両者の調和平均をとったものです。どちらか一方が極端に低いと、F値も低くなります。\""
    },
    {
        "ID": "\"DL-5-072\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"「不均衡データ（例：100万件中1件だけが不正取引）」の分類において、正解率（Accuracy）を指標にすることの問題点は？\"",
        "Opt1": "\"計算が遅くなる\"",
        "Opt2": "\"全てのデータを「正常」と予測するだけで正解率が99.99%になり、本来見つけたい「不正」を見逃しても高く評価されてしまう\"",
        "Opt3": "\"正解率が0%になる\"",
        "Opt4": "\"メモリを消費する\"",
        "Opt5": "\"数理的に定義できない\"",
        "Opt6": "\"GPUが壊れる\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】不均衡データでは、AccuracyよりもRecallやPrecision、AUCなどを重視すべきです。\""
    },
    {
        "ID": "\"DL-5-073\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"TransformerのAttention計算において、内積の値を「キー（Key）の次元数の平方根」で割るスケーリングを行う主な目的は？\"",
        "Opt1": "\"計算速度を上げるため\"",
        "Opt2": "\"Softmaxを適用する際に、値が大きすぎて勾配が消失するのを防ぐため\"",
        "Opt3": "\"メモリを節約するため\"",
        "Opt4": "\"次元を増やすため\"",
        "Opt5": "\"バイアスを加えるため\"",
        "Opt6": "\"負の値を消すため\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】スケーリングしないと内積値が非常に大きくなり、Softmaxの勾配が極端に小さくなって学習が停滞します。\""
    },
    {
        "ID": "\"DL-5-074\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"CNNにおける「1x1畳み込み（1x1 Convolution）」の主な役割は？\"",
        "Opt1": "\"画像の縦横サイズを半分にすること\"",
        "Opt2": "\"チャネル（次元）数の削減または増加（次元圧縮）\"",
        "Opt3": "\"エッジの抽出\"",
        "Opt4": "\"位置情報の削除\"",
        "Opt5": "\"活性化関数の代用\"",
        "Opt6": "\"パディングの代わり\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】NiN（Network in Network）やGoogLeNet、ResNetなどで計算コストを抑えつつ表現力を高めるために多用されます。\""
    },
    {
        "ID": "\"DL-5-075\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"リカレントニューラルネットワーク（RNN）を双方向から適用し、過去と未来の両方の情報を利用できるようにしたモデルは？\"",
        "Opt1": "\"LSTM\"",
        "Opt2": "\"GRU\"",
        "Opt3": "\"双方向RNN（Bi-directional RNN）\"",
        "Opt4": "\"CNN\"",
        "Opt5": "\"Transformer\"",
        "Opt6": "\"GAN\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】文章全体が既知のタスク（翻訳など）において、後ろの文脈も考慮できるため非常に強力です。\""
    },
    {
        "ID": "\"DL-5-076\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"転移学習とファインチューニングの違いとして、一般的に適切な説明は？\"",
        "Opt1": "\"転移学習は一から学習し、ファインチューニングは既存のモデルを使う\"",
        "Opt2": "\"ファインチューニングは学習済みモデルの重みを「初期値」として使い、全ての層（または一部）の重みを更新すること\"",
        "Opt3": "\"転移学習は画像にのみ使い、ファインチューニングはテキストにのみ使う\"",
        "Opt4": "\"両者は全く同じ意味である\"",
        "Opt5": "\"転移学習はGPUを使い、ファインチューニングはCPUを使う\"",
        "Opt6": "\"ファインチューニングはデータを増やすことである\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】狭義には、出力層だけを入れ替えてそこだけ学習させるのが転移学習、全体を微調整するのがファインチューニングと呼び分けられます。\""
    },
    {
        "ID": "\"DL-5-077\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"物体検出モデル「YOLO（You Only Look Once）」が非常に高速である最大の理由は？\"",
        "Opt1": "\"画像を格子状に分割し、一度のネットワーク実行で全ての領域の候補とクラスを同時に予測するから\"",
        "Opt2": "\"画像を1ピクセルずつ処理するから\"",
        "Opt3": "\"RNNを100層重ねているから\"",
        "Opt4": "\"全結合層を持たないから\"",
        "Opt5": "\"CPUだけで動くから\"",
        "Opt6": "\"画像を事前に白黒にするから\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】「一度見るだけ」の名の通り、物体候補の抽出と分類を同時に行う1段階（One-stage）方式のため高速です。\""
    },
    {
        "ID": "\"DL-5-078\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"画像生成モデルの評価指標として、生成された画像が「本物に近いか」と「多様性があるか」を数値化するものは？\"",
        "Opt1": "\"mAP\"",
        "Opt2": "\"IoU\"",
        "Opt3": "\"FID（Fréchet Inception Distance）\"",
        "Opt4": "\"正解率\"",
        "Opt5": "\"F値\"",
        "Opt6": "\"BLEUスコア\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】事前学習済みのInceptionモデルの特徴量空間において、本物と生成物の分布の距離を測ります。値が小さいほど高品質です。\""
    },
    {
        "ID": "\"DL-5-079\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"自然言語処理の評価において、機械翻訳の文と人間が作成した参照文の「単語の一致度（n-gram）」を測定する指標は？\"",
        "Opt1": "\"mAP\"",
        "Opt2": "\"FID\"",
        "Opt3": "\"BLEU（Bilingual Evaluation Understudy）\"",
        "Opt4": "\"ROUGE\"",
        "Opt5": "\"IoU\"",
        "Opt6": "\"Accuracy\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】翻訳の質を客観的に評価する標準的な指標です。要約タスクではROUGEがよく使われます。\""
    },
    {
        "ID": "\"DL-5-080\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"強化学習において、エージェントが「環境」から受け取る情報のことを何というか？\"",
        "Opt1": "\"報酬\"",
        "Opt2": "\"方策\"",
        "Opt3": "\"状態（State）\"",
        "Opt4": "\"行動\"",
        "Opt5": "\"価値\"",
        "Opt6": "\"割引率\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】今のゲーム画面やロボットのセンサー値など、エージェントが判断材料にする情報のことです。\""
    },
    {
        "ID": "\"DL-5-081\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"「Actor-Critic」という強化学習の枠組みにおいて、「Critic（批評家）」の役割は？\"",
        "Opt1": "\"行動を決定すること\"",
        "Opt2": "\"行動の「価値」を推定し、Actorの学習をサポートすること\"",
        "Opt3": "\"環境をリセットすること\"",
        "Opt4": "\"データを増やすこと\"",
        "Opt5": "\"報酬を減らすこと\"",
        "Opt6": "\"エピソードを終わらせること\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】Actorが行動を選び、Criticがその行動の良し悪しを評価することで、効率的に学習を進めます。\""
    },
    {
        "ID": "\"DL-5-082\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"大量のラベルなしデータから、データ自身の構造（次の単語の予測など）を使って学習し、汎用的な特徴を得る手法は？\"",
        "Opt1": "\"教師あり学習\"",
        "Opt2": "\"強化学習\"",
        "Opt3": "\"自己教師あり学習（Self-supervised learning）\"",
        "Opt4": "\"アンサンブル学習\"",
        "Opt5": "\"転移学習\"",
        "Opt6": "\"能動学習\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】BERTやGPTの事前学習などは、人間によるラベル付けを必要としない「自己教師あり学習」の典型例です。\""
    },
    {
        "ID": "\"DL-5-083\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"画像分類において、複数のCNNモデルの予測を平均したり、多数決をとったりして精度を向上させる手法を何というか？\"",
        "Opt1": "\"ドロップアウト\"",
        "Opt2": "\"バッチ正規化\"",
        "Opt3": "\"アンサンブル学習\"",
        "Opt4": "\"知識蒸留\"",
        "Opt5": "\"データ拡張\"",
        "Opt6": "\"量子化\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】個々のモデルが持つ弱点を補い合い、より安定した高い精度を出すことができます。\""
    },
    {
        "ID": "\"DL-5-084\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"CNNにおける「受容野（Receptive Field）」とは何を指すか？\"",
        "Opt1": "\"画像全体のサイズ\"",
        "Opt2": "\"出力層の特定のユニットが影響を受ける、入力画像の領域の範囲\"",
        "Opt3": "\"フィルタの枚数\"",
        "Opt4": "\"学習データの数\"",
        "Opt5": "\"GPUのメモリ容量\"",
        "Opt6": "\"パディングの幅\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】層が深くなるほど、1つのユニットが考慮できる入力画像の範囲（受容野）は広がっていきます。\""
    },
    {
        "ID": "\"DL-5-085\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"生成モデルの「VAE（変分自己符号化器）」の損失関数に含まれる、潜在変数の分布をガウス分布に近づけるための項は？\"",
        "Opt1": "\"平均二乗誤差\"",
        "Opt2": "\"交差エントロピー\"",
        "Opt3": "\"KLダイバージェンス（KL情報量）\"",
        "Opt4": "\"ヒンジ損失\"",
        "Opt5": "\"ジニ係数\"",
        "Opt6": "\"標準偏差\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】Kullback-Leibler Divergence。2つの確率分布の「近さ」を測る指標で、VAEの正則化項として機能します。\""
    },
    {
        "ID": "\"DL-5-086\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"強化学習において、報酬の合計（利得）を最大化するように行動する際、将来の報酬をどれくらい割り引くかを示す $ \\gamma $ の名前は？\"",
        "Opt1": "\"学習率\"",
        "Opt2": "\"モーメンタム\"",
        "Opt3": "\"割引率（Discount Factor）\"",
        "Opt4": "\"温度パラメータ\"",
        "Opt5": "\"減衰率\"",
        "Opt6": "\"しきい値\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】0に近いほど「目先の報酬」を、1に近いほど「将来の報酬」を重視するようになります。\""
    },
    {
        "ID": "\"DL-5-087\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"CNNの畳み込み層で、同じフィルタ（重み）を画像全体にスライドさせて適用することを何というか？\"",
        "Opt1": "\"重み共有（Weight Sharing）\"",
        "Opt2": "\"重み減衰\"",
        "Opt3": "\"勾配消失\"",
        "Opt4": "\"正則化\"",
        "Opt5": "\"初期化\"",
        "Opt6": "\"量子化\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】これにより、画像内のどこに特徴が現れても検出でき（並進不変性）、パラメータ数も大幅に削減できます。\""
    },
    {
        "ID": "\"DL-5-088\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"自然言語処理のTransformerにおいて、RNNを使わずに「文の構造」を捉えるための最も重要な層は？\"",
        "Opt1": "\"全結合層\"",
        "Opt2": "\"マルチヘッド・アテンション（Multi-Head Attention）\"",
        "Opt3": "\"畳み込み層\"",
        "Opt4": "\"プーリング層\"",
        "Opt5": "\"回帰層\"",
        "Opt6": "\"再帰層\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】複数のAttentionを並列に動かすことで、単語間の様々な種類の関係性を同時に捉えることができます。\""
    },
    {
        "ID": "\"DL-5-089\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"RNNの学習において、系列データが非常に長い場合に、適当な長さで区切って逆伝播を行う手法を何というか？\"",
        "Opt1": "\"早期終了\"",
        "Opt2": "\"ドロップアウト\"",
        "Opt3": "\"BPTT（Backpropagation Through Time）の切り捨て（Truncated BPTT）\"",
        "Opt4": "\"バッチ正規化\"",
        "Opt5": "\"データ拡張\"",
        "Opt6": "\"平滑化\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】無限に遡ると計算量とメモリが爆発するため、一定のステップ数で逆伝播を打ち切ります。\""
    },
    {
        "ID": "\"DL-5-090\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"「DeepLab」などのセグメンテーションモデルで、物体の境界を精緻に捉えるためにCNNの最後の方で導入される手法は？\"",
        "Opt1": "\"全結合層への変換\"",
        "Opt2": "\"CRF（条件付き確率場）やDilation\"",
        "Opt3": "\"最大プーリングの繰り返し\"",
        "Opt4": "\"画像のモノクロ化\"",
        "Opt5": "\"ドロップアウトの除去\"",
        "Opt6": "\"学習率の固定\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】CNNだけでは境界がぼやけやすいため、CRFなどの後処理やDilated Convolutionで解像度を維持します。\""
    },
    {
        "ID": "\"DL-5-091\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"「StyleGAN」において、画像の特定の要素（髪の色、顔の向きなど）を独立して制御できる性質を何というか？\"",
        "Opt1": "\"汎化性能\"",
        "Opt2": "\"解きほぐし（Disentanglement）\"",
        "Opt3": "\"量子化性能\"",
        "Opt4": "\"蒸留\"",
        "Opt5": "\"頑健性\"",
        "Opt6": "\"収束性\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】各特徴が混ざり合わず、独立した「スタイル」として調整可能なネットワーク構造になっています。\""
    },
    {
        "ID": "\"DL-5-092\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"GANの派生「Pix2Pix」が、学習のために必要とするデータの形式は？\"",
        "Opt1": "\"ラベルなしの大量の画像\"",
        "Opt2": "\"入力画像と、それに対応する正解出力画像（ペアデータ）\"",
        "Opt3": "\"テキストデータのみ\"",
        "Opt4": "\"音声データのみ\"",
        "Opt5": "\"動画のフレームのみ\"",
        "Opt6": "\"ランダムなノイズのみ\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】「線画」と「実写写真」のように、ペアになったデータを使って1対1の変換を学習します。\""
    },
    {
        "ID": "\"DL-5-093\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"強化学習のDQNにおいて、学習が不安定になるのを防ぐため、Q値の更新先を計算するネットワークを別に用意する手法は？\"",
        "Opt1": "\"経験再生\"",
        "Opt2": "\"ターゲットネットワーク\"",
        "Opt3": "\"エラスティックネット\"",
        "Opt4": "\"ドロップアウト\"",
        "Opt5": "\"ソフトマックス方策\"",
        "Opt6": "\"モンテカルロ法\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】更新されるネットワークを自分自身の目標にすると値が振動しやすいため、古い重みを固定した目標用のネットワークを別に持ちます。\""
    },
    {
        "ID": "\"DL-5-094\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"画像分類において、学習データにない「未知のクラス」の画像を、数枚の見本（サポートデータ）だけで識別するタスクを何というか？\"",
        "Opt1": "\"ゼロショット学習\"",
        "Opt2": "\"フューショット学習（Few-shot learning）\"",
        "Opt3": "\"能動学習\"",
        "Opt4": "\"アンサンブル学習\"",
        "Opt5": "\"事前学習\"",
        "Opt6": "\"蒸留\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】「1つだけ（One-shot）」や「数個（Few-shot）」の例示だけで、人間のように新しい概念を学習する手法です。\""
    },
    {
        "ID": "\"DL-5-095\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"BERTなどのモデルを、計算コストを抑えるために知識蒸留を用いて軽量化したモデルはどれか？\"",
        "Opt1": "\"GPT-3\"",
        "Opt2": "\"DistilBERT\"",
        "Opt3": "\"ResNet-18\"",
        "Opt4": "\"VGG-11\"",
        "Opt5": "\"YOLOv5\"",
        "Opt6": "\"DQN\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】巨大なBERT（教師）の振る舞いを小さなモデル（生徒）に学ばせることで、スマホ等でも動くサイズに圧縮したモデルです。\""
    },
    {
        "ID": "\"DL-5-096\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"マルチモーダル学習の「CLIP」が学習に使用するペアデータは？\"",
        "Opt1": "\"画像とその「説明文（テキスト）」\"",
        "Opt2": "\"音声とその「波形」\"",
        "Opt3": "\"英語の文とその「日本語訳」\"",
        "Opt4": "\"動画とその「字幕」\"",
        "Opt5": "\"画像とその「座標」\"",
        "Opt6": "\"テキストとその「感情」\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】画像とテキストを共通のベクトル空間に埋め込むことで、言葉で画像を検索したり、未知の画像を分類したりできます。\""
    },
    {
        "ID": "\"DL-5-097\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"「拡散モデル」の逆プロセスにおいて、ノイズを取り除く（デノイジング）際に条件として与えられることが多い情報は？\"",
        "Opt1": "\"画像サイズ\"",
        "Opt2": "\"テキスト（プロンプト）やラベル\"",
        "Opt3": "\"学習率\"",
        "Opt4": "\"バッチサイズ\"",
        "Opt5": "\"GPUの型番\"",
        "Opt6": "\"エポック数\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】プロンプトを与えることで、ノイズから「赤い車」などの指定された画像を生成するように誘導します。\""
    },
    {
        "ID": "\"DL-5-098\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"画像分類タスクで、モデルが「どれだけ自信を持って予測しているか」と「実際の正解率」のズレを調整することを何というか？\"",
        "Opt1": "\"正規化\"",
        "Opt2": "\"校正（キャリブレーション）\"",
        "Opt3": "\"蒸留\"",
        "Opt4": "\"量子化\"",
        "Opt5": "\"パディング\"",
        "Opt6": "\"ストライド\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】Calibration。ニューラルネットワークは自信過剰（確率0.99と言いつつ正解率0.8など）になりやすいため、これを修正します。\""
    },
    {
        "ID": "\"DL-5-099\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"深層学習における「ドメイン適応（Domain Adaptation）」の目的は？\"",
        "Opt1": "\"新しいプログラミング言語を学ぶこと\"",
        "Opt2": "\"学習データ（ソース）と実際の運用データ（ターゲット）の分布の違いを克服すること\"",
        "Opt3": "\"モデルの層を増やすこと\"",
        "Opt4": "\"学習を早く終わらせること\"",
        "Opt5": "\"画像を圧縮すること\"",
        "Opt6": "\"音声を文字にすること\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】シミュレーションデータで学習したAIを、実世界のデータでも動くように調整する手法などが含まれます。\""
    },
    {
        "ID": "\"DL-100\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"強化学習において、エージェントが報酬を得るまでの手順が非常に長く、どの行動が報酬に寄与したか判断が難しい問題を何というか？\"",
        "Opt1": "\"次元の呪い\"",
        "Opt2": "\"報酬遅延問題（信用割当問題）\"",
        "Opt3": "\"勾配消失問題\"",
        "Opt4": "\"モード崩壊\"",
        "Opt5": "\"内部共変量シフト\"",
        "Opt6": "\"過学習\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】Credit Assignment Problem。最後にゴールした瞬間にしか報酬が出ない場合、その前のどの移動が正解だったか特定するのが難しくなります。\""
    },
    {
        "ID": "\"DL-5-101\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"物体検出において、同一の物体に対して複数のバウンディングボックスが予測された際、最も信頼度の高いものだけを残す処理を何というか？\"",
        "Opt1": "\"NMS（Non-Maximum Suppression）\"",
        "Opt2": "\"バッチ正規化\"",
        "Opt3": "\"アンカーボックスの初期化\"",
        "Opt4": "\"ハードネガティブマイニング\"",
        "Opt5": "\"ソフトマックス回帰\"",
        "Opt6": "\"次元圧縮\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】非最大値抑制（NMS）。重なり（IoU）が一定以上のボックス群から、信頼度スコアが最大のもの以外を除外して冗長な検出を防ぎます。\""
    },
    {
        "ID": "\"DL-5-102\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"SSD（Single Shot MultiBox Detector）が、YOLO（初期）と比較して、サイズの異なる物体の検出に強い理由は？\"",
        "Opt1": "\"層を深くしたから\"",
        "Opt2": "\"マルチスケールな特徴マップを用いて、各層で異なるサイズの物体を検出するから\"",
        "Opt3": "\"RNNを導入したから\"",
        "Opt4": "\"全結合層を増やしたから\"",
        "Opt5": "\"高解像度画像しか入力しないから\"",
        "Opt6": "\"アンカーボックスを使わないから\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】SSDは解像度の異なる複数の層から直接予測を行うため、小さい物体から大きい物体までロバストに検出できます。\""
    },
    {
        "ID": "\"DL-5-103\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"物体検出の精度指標であるAP（Average Precision）を計算する際に用いられる曲線は？\"",
        "Opt1": "\"ROC曲線\"",
        "Opt2": "\"PR曲線（適合率-再現率曲線）\"",
        "Opt3": "\"学習曲線\"",
        "Opt4": "\"シグモイド曲線\"",
        "Opt5": "\"損失曲線\"",
        "Opt6": "\"決定境界線\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】Precision（適合率）とRecall（再現率）の関係を示すPR曲線の下側面積がAPとなります。全クラスの平均がmAPです。\""
    },
    {
        "ID": "\"DL-5-104\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"セマンティックセグメンテーションにおいて、低解像度の特徴マップを元の画像サイズに拡大する「逆畳み込み」の別名は？\"",
        "Opt1": "\"転置畳み込み（Transposed Convolution / Deconvolution）\"",
        "Opt2": "\"拡張畳み込み\"",
        "Opt3": "\"ポイントワイズ畳み込み\"",
        "Opt4": "\"深層分離畳み込み\"",
        "Opt5": "\"プーリング\"",
        "Opt6": "\"ゼロパディング\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】Transposed Convolution。学習可能なパラメータを持ち、特徴マップのサイズを拡大（アップサンプリング）するために使われます。\""
    },
    {
        "ID": "\"DL-5-105\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"セグメンテーションにおいて、同じクラス（例：犬）であっても個体ごとにラベルを分ける手法は？\"",
        "Opt1": "\"セマンティックセグメンテーション\"",
        "Opt2": "\"インスタンスセグメンテーション\"",
        "Opt3": "\"パノプティックセグメンテーション\"",
        "Opt4": "\"物体検出\"",
        "Opt5": "\"画像分類\"",
        "Opt6": "\"超解像\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】個々の「インスタンス（個体）」を区別します。一方、セマンティックはクラス単位の塗り分けのみを行います。\""
    },
    {
        "ID": "\"DL-5-106\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"「U-Net」において、エンコーダの情報をデコーダに渡す「スキップ接続（Skip Connection）」の効果は？\"",
        "Opt1": "\"計算速度を上げること\"",
        "Opt2": "\"物体の境界などの細かい位置情報を復元すること\"",
        "Opt3": "\"過学習を防ぐこと\"",
        "Opt4": "\"画像を白黒にすること\"",
        "Opt5": "\"RNNの長期記憶を助けること\"",
        "Opt6": "\"損失を0にすること\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】縮小過程（エンコーダ）で失われがちな空間情報を、直接拡大過程（デコーダ）に結合することで、精緻な境界の復元を可能にします。\""
    },
    {
        "ID": "\"DL-5-107\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"GANにおいて、生成器（Generator）が識別器（Discriminator）を騙す際、本物と見紛う画像を生成するための損失関数として一般的に使われるのは？\"",
        "Opt1": "\"平均二乗誤差（MSE）\"",
        "Opt2": "\"交差エントロピー誤差\"",
        "Opt3": "\"ヒンジ損失\"",
        "Opt4": "\"KLダイバージェンス\"",
        "Opt5": "\"ジニ係数\"",
        "Opt6": "\"コサイン類似度\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】GANの目的関数は、基本的には二値分類（本物か偽物か）に基づく交差エントロピーに基づきます（初期のGANの場合）。\""
    },
    {
        "ID": "\"DL-5-108\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"GANの派生で、畳み込み層を導入し、バッチ正規化などを用いて学習を安定化させたモデルの名称は？\"",
        "Opt1": "\"VGG16\"",
        "Opt2": "\"ResNet\"",
        "Opt3": "\"DCGAN（Deep Convolutional GAN）\"",
        "Opt4": "\"RNN\"",
        "Opt5": "\"LSTM\"",
        "Opt6": "\"YOLO\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】Generatorに転置畳み込み、Discriminatorに畳み込みを用い、全結合層を排除するなど、GANの実用性を飛躍的に高めた記念碑的モデルです。\""
    },
    {
        "ID": "\"DL-5-109\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"VAE（変分自己符号化器）において、潜在変数からデータを再構成する際の誤差を何というか？\"",
        "Opt1": "\"正則化誤差\"",
        "Opt2": "\"再構成誤差（Reconstruction Loss）\"",
        "Opt3": "\"識別誤差\"",
        "Opt4": "\"分類誤差\"",
        "Opt5": "\"量子化誤差\"",
        "Opt6": "\"予測誤差\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】入力したデータと、潜在変数から復元されたデータがどれだけ一致しているかを測る指標です。\""
    },
    {
        "ID": "\"DL-5-110\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"強化学習のQ学習において、行動価値関数 $Q(s",
        "Opt1": "a)$ を近似するためにニューラルネットワークを使用する手法は？\"",
        "Opt2": "\"RNN\"",
        "Opt3": "\"CNN\"",
        "Opt4": "\"DQN（Deep Q-Network）\"",
        "Opt5": "\"GAN\"",
        "Opt6": "\"BERT\"",
        "Answer_Idx": "\"Transformer\"",
        "Explanation": 2
    },
    {
        "ID": "\"DL-5-111\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"強化学習において、エージェントが過去の行動データ（経験）をメモリに蓄積し、そこからランダムに学習データを取り出す手法は？\"",
        "Opt1": "\"ターゲットネットワーク\"",
        "Opt2": "\"経験再生（Experience Replay）\"",
        "Opt3": "\"報酬シェイピング\"",
        "Opt4": "\"割引報酬\"",
        "Opt5": "\"方策勾配\"",
        "Opt6": "\"モンテカルロ木探索\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】時系列データの相関を断ち切り、学習を安定させるDQNの重要な構成要素です。\""
    },
    {
        "ID": "\"DL-5-112\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"「方策勾配法」をベースに、価値関数による評価（Critic）と方策による行動（Actor）を組み合わせた手法の総称は？\"",
        "Opt1": "\"Q学習\"",
        "Opt2": "\"Actor-Critic\"",
        "Opt3": "\"Sarsa\"",
        "Opt4": "\"モンテカルロ法\"",
        "Opt5": "\"教師あり学習\"",
        "Opt6": "\"自己教師あり学習\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】行動を選ぶ「俳優」と、その行動を採点する「評論家」が協力して学習する枠組みです。\""
    },
    {
        "ID": "\"DL-5-113\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"Googleが開発した、囲碁の「AlphaGo」に続く、将棋やチェスでも世界一になった汎用型強化学習AIは？\"",
        "Opt1": "\"AlphaZero\"",
        "Opt2": "\"DQN\"",
        "Opt3": "\"A3C\"",
        "Opt4": "\"PPO\"",
        "Opt5": "\"GPT-4\"",
        "Opt6": "\"Stable Diffusion\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】人間の棋譜を使わない「AlphaGo Zero」のアルゴリズムを、囲碁以外のボードゲームにも適用できるように一般化したものです。\""
    },
    {
        "ID": "\"DL-5-114\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"大規模言語モデルにおいて、入力された複数のトークンの中から、特定の情報の重要度を動的に計算するTransformerの核となる機構は？\"",
        "Opt1": "\"プーリング\"",
        "Opt2": "\"畳み込み\"",
        "Opt3": "\"アテンション（Attention）\"",
        "Opt4": "\"ドロップアウト\"",
        "Opt5": "\"活性化関数\"",
        "Opt6": "\"バイアス\"",
        "Answer_Idx": 2,
        "Explanation": "\"【解説】特にSelf-Attention（自己注意機構）は、文中の各単語が他のどの単語と関連が深いかを直接計算します。\""
    },
    {
        "ID": "\"DL-5-115\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"Transformerの「Encoder-Decoder」構造において、DecoderがEncoderの出力を参照するために使われるAttentionは？\"",
        "Opt1": "\"Self-Attention\"",
        "Opt2": "\"Source-Target Attention（Cross Attention）\"",
        "Opt3": "\"Multi-head Attentionのみ\"",
        "Opt4": "\"Masked Self-Attention\"",
        "Opt5": "\"Local Attention\"",
        "Opt6": "\"Global Attention\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】翻訳タスクなどで、デコーダがターゲット言語を生成する際、ソース言語（エンコーダの出力）のどこに注目すべきかを決定します。\""
    },
    {
        "ID": "\"DL-5-116\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"BERTが事前学習タスクとして採用している、2つの文章が連続しているかどうかを判定するタスクは？\"",
        "Opt1": "\"マスク付き言語モデル（MLM）\"",
        "Opt2": "\"次文予測（NSP: Next Sentence Prediction）\"",
        "Opt3": "\"感情分析\"",
        "Opt4": "\"質問応答\"",
        "Opt5": "\"要約\"",
        "Opt6": "\"翻訳\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】文章間の関係性を学習することで、読解問題や対話応答などのタスクに強くなります。\""
    },
    {
        "ID": "\"DL-5-117\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"画像分類モデルの評価で、Positive（陽性）をNegative（陰性）と誤判定して見逃してしまうことを何というか？\"",
        "Opt1": "\"第一種の過誤（偽陽性）\"",
        "Opt2": "\"第二種の過誤（偽陰性）\"",
        "Opt3": "\"適合率の低下\"",
        "Opt4": "\"正解率の向上\"",
        "Opt5": "\"オーバーフィッティング\"",
        "Opt6": "\"アンダーフィッティング\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】False Negative。医療診断などで最も警戒すべき「見逃し」に相当します。\""
    },
    {
        "ID": "\"DL-5-118\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"動画認識において、時間方向の情報の流れを考慮するためにRNN（LSTM）とCNNを組み合わせた構造は？\"",
        "Opt1": "\"Convolutional LSTM\"",
        "Opt2": "\"3D-CNNのみ\"",
        "Opt3": "\"Transformerのみ\"",
        "Opt4": "\"VGG\"",
        "Opt5": "\"ResNet\"",
        "Opt6": "\"AlexNet\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】畳み込み層の内部で時間発展（再帰）を扱うことで、映像の空間的な特徴と時間的な変化を同時に学習します。\""
    },
    {
        "ID": "\"DL-5-119\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"音声生成モデルのWaveNetにおいて、受容野を効率よく広げるために「隙間」を開けて畳み込みを行う手法は？\"",
        "Opt1": "\"拡張畳み込み（Dilated Causal Convolution）\"",
        "Opt2": "\"最大プーリング\"",
        "Opt3": "\"ゼロパディング\"",
        "Opt4": "\"全結合\"",
        "Opt5": "\"ドロップアウト\"",
        "Opt6": "\"早期終了\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】音声波形は非常に長いため、通常の畳み込みではなくDilated（拡張）を用いることで、少ない層で長時間の依存関係を捉えます。\""
    },
    {
        "ID": "\"DL-5-120\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"「拡散モデル」において、画像からノイズを除去するステップ（逆拡散プロセス）で学習されるのは？\"",
        "Opt1": "\"元の画像そのもの\"",
        "Opt2": "\"加えられたノイズの推定値（スコア）\"",
        "Opt3": "\"画像のクラスラベル\"",
        "Opt4": "\"計算速度\"",
        "Opt5": "\"メモリ使用量\"",
        "Opt6": "\"パディングの幅\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】「ある状態にどの程度のノイズが乗っているか」を予測するように学習し、それを引くことで画像を浄化していきます。\""
    },
    {
        "ID": "\"DL-5-121\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"異なるドメインのデータ（例：写真と文章）を同じベクトル空間上に写像し、対応関係を学習させる手法を何というか？\"",
        "Opt1": "\"マルチモーダル学習\"",
        "Opt2": "\"アンサンブル学習\"",
        "Opt3": "\"転移学習\"",
        "Opt4": "\"能動学習\"",
        "Opt5": "\"強化学習\"",
        "Opt6": "\"教師なし学習\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】Multimodal learning。CLIP（画像とテキスト）などがその代表例です。\""
    },
    {
        "ID": "\"DL-5-122\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"強化学習のQ学習において、行動を選択する際に一定の確率 $ \\epsilon $ でランダムに行動し、残りの確率で現在の最適行動をとる手法は？\"",
        "Opt1": "\"$\\epsilon$-greedy法\"",
        "Opt2": "\"ソフトマックス方策\"",
        "Opt3": "\"ボルツマン探索\"",
        "Opt4": "\"UCBアルゴリズム\"",
        "Opt5": "\"経験再生\"",
        "Opt6": "\"ターゲットネットワーク\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】「探索（未知の開拓）」と「活用（既知の最善）」のバランスをとるための最も基本的な手法です。\""
    },
    {
        "ID": "\"DL-5-123\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"RNNの学習において、勾配が爆発するのを防ぐために、勾配の大きさが閾値を超えたらノルムを制限する処理は？\"",
        "Opt1": "\"勾配クリッピング\"",
        "Opt2": "\"バッチ正規化\"",
        "Opt3": "\"ドロップアウト\"",
        "Opt4": "\"L2正則化\"",
        "Opt5": "\"ReLUの使用\"",
        "Opt6": "\"重みの初期化\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】Gradient Clipping。特に時系列データが長い場合に勾配が急増する現象を力技で抑え込みます。\""
    },
    {
        "ID": "\"DL-5-124\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"物体検出の「Faster R-CNN」において、画像の中から物体の候補領域（Region Proposal）を抽出するネットワークは？\"",
        "Opt1": "\"RPN（Region Proposal Network）\"",
        "Opt2": "\"SSD\"",
        "Opt3": "\"YOLO\"",
        "Opt4": "\"Transformer\"",
        "Opt5": "\"RNN\"",
        "Opt6": "\"Decoder\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】従来は手法が分かれていましたが、RPNを導入することで候補抽出も含めて全てディープラーニングで完結（End-to-End）できるようになりました。\""
    },
    {
        "ID": "\"DL-5-125\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"GANの学習を安定させるための手法で、重みのスペクトルノルムを1に制限し、関数の滑らかさ（リプシッツ連続性）を保証する手法は？\"",
        "Opt1": "\"スペクトル正規化（Spectral Normalization）\"",
        "Opt2": "\"バッチ正規化\"",
        "Opt3": "\"レイヤー正規化\"",
        "Opt4": "\"L1正則化\"",
        "Opt5": "\"データ拡張\"",
        "Opt6": "\"ドロップアウト\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】Discriminatorの急激な変化を抑えることで、Generatorの学習を崩壊させにくくする高度なテクニックです。\""
    },
    {
        "ID": "\"DL-5-126\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"オートエンコーダの潜在空間を疎（スカース）にすることで、少数のユニットだけが反応するように制限をかける手法は？\"",
        "Opt1": "\"スパース自己符号化器（Sparse Autoencoder）\"",
        "Opt2": "\"Denoising Autoencoder\"",
        "Opt3": "\"VAE\"",
        "Opt4": "\"CNN\"",
        "Opt5": "\"RNN\"",
        "Opt6": "\"GAN\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】データの重要な特徴だけを選別して抽出することを強制する手法です。\""
    },
    {
        "ID": "\"DL-5-127\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"「Transformer」のデコーダにおいて、各層で「Encoderの出力」と「DecoderのSelf-Attentionの出力」を統合する箇所は？\"",
        "Opt1": "\"Cross-Attention（Encoder-Decoder Attention）\"",
        "Opt2": "\"FFN（Feed Forward Network）\"",
        "Opt3": "\"Positional Encoding\"",
        "Opt4": "\"Embedding\"",
        "Opt5": "\"Input\"",
        "Opt6": "\"Output\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】ここが翻訳の肝であり、入力文の情報と現在生成中の文の情報を突き合わせる場所です。\""
    },
    {
        "ID": "\"DL-5-128\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"物体検出の評価指標「IoU（Intersection over Union）」の計算式として正しいものは？\"",
        "Opt1": "\"積集合（重なり） / 和集合（合計面積）\"",
        "Opt2": "\"積集合 / 正解面積\"",
        "Opt3": "\"和集合 / 積集合\"",
        "Opt4": "\"予測面積 / 正解面積\"",
        "Opt5": "\"1 - 誤差\"",
        "Opt6": "\"正解数 / 全数\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】いわゆる「ジャッカード係数」です。2つの矩形がどれだけ重なっているかを0から1で表します。\""
    },
    {
        "ID": "\"DL-5-129\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"セマンティックセグメンテーションのモデル「DeepLab v3+」で、解像度を維持したまま受容野を広げる手法は？\"",
        "Opt1": "\"Atrous Convolution（Dilated Convolution）\"",
        "Opt2": "\"Max Poolingの多用\"",
        "Opt3": "\"全結合層の追加\"",
        "Opt4": "\"画像の縮小\"",
        "Opt5": "\"バッチサイズの増大\"",
        "Opt6": "\"学習率の低下\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】空洞を設けた畳み込みを行うことで、ピクセル単位の精度を保ちつつ、広いコンテキストを把握します。\""
    },
    {
        "ID": "\"DL-5-130\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"強化学習において、状態 $s$ で行動 $a$ をとった直後の「即時報酬」だけでなく、将来にわたる報酬の合計（利得）を何というか？\"",
        "Opt1": "\"価値（Value）\"",
        "Opt2": "\"方策（Policy）\"",
        "Opt3": "\"環境（Environment）\"",
        "Opt4": "\"エージェント（Agent）\"",
        "Opt5": "\"状態（State）\"",
        "Opt6": "\"損失（Loss）\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】価値関数は、この「将来もらえる報酬の期待値」を見積もるために存在します。\""
    },
    {
        "ID": "\"DL-5-131\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"強化学習のアルゴリズム「PPO（Proximal Policy Optimization）」の主な特徴は？\"",
        "Opt1": "\"学習の安定性を高めるために、方策の更新幅に制限（クリップ）をかける\"",
        "Opt2": "\"計算が非常に遅い\"",
        "Opt3": "\"ニューラルネットワークを使わない\"",
        "Opt4": "\"囲碁専用である\"",
        "Opt5": "\"画像を扱えない\"",
        "Opt6": "\"常にランダムに行動する\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】方策勾配法の一種で、急激な更新による性能崩壊を防ぐため、非常に安定した学習が可能です。\""
    },
    {
        "ID": "\"DL-5-132\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"画像生成の「StyleGAN」において、潜在変数 $z$ を中間的な空間 $w$ に写像する「Mapping Network」の役割は？\"",
        "Opt1": "\"各特徴をより独立させ、スタイルの絡まりを解く（解きほぐし）ため\"",
        "Opt2": "\"計算を速くするため\"",
        "Opt3": "\"メモリを節約するため\"",
        "Opt4": "\"解像度を下げるため\"",
        "Opt5": "\"色をモノクロにするため\"",
        "Opt6": "\"学習を即座に止めるため\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】この処理により、顔の向きや表情などの各要素が潜在空間で絡み合わず、個別に操作しやすくなります。\""
    },
    {
        "ID": "\"DL-5-133\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"音声認識の評価で、正解のテキストと予測のテキストの「編集距離（挿入、削除、置換）」に基づく指標は？\"",
        "Opt1": "\"WER（Word Error Rate：単語誤り率）\"",
        "Opt2": "\"mAP\"",
        "Opt3": "\"IoU\"",
        "Opt4": "\"BLEU\"",
        "Opt5": "\"FID\"",
        "Opt6": "\"Accuracy\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】文字単位の場合はCER（Character Error Rate）と呼ばれます。値が低いほど正確です。\""
    },
    {
        "ID": "\"DL-5-134\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"大規模言語モデルのGPT（Generative Pre-trained Transformer）は、Transformerのどの部分をベースにしているか？\"",
        "Opt1": "\"Decoderのみ\"",
        "Opt2": "\"Encoderのみ\"",
        "Opt3": "\"EncoderとDecoderの両方\"",
        "Opt4": "\"CNN\"",
        "Opt5": "\"RNN\"",
        "Opt6": "\"全結合層のみ\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】GPTは「次の単語を予測する（自己回帰）」タスクに特化しているため、Decoder側のみを多層に重ねた構造です。\""
    },
    {
        "ID": "\"DL-5-135\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"BERTが事前学習で行う「MLM（Masked Language Model）」において、ランダムに隠される単語の割合は通常どの程度か？\"",
        "Opt1": "\"5%\"",
        "Opt2": "\"15%\"",
        "Opt3": "\"50%\"",
        "Opt4": "\"80%\"",
        "Opt5": "\"100%\"",
        "Opt6": "\"0%\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】約15%のトークンが隠され（マスクされ）、それを周辺の単語から当てることで学習が進みます。\""
    },
    {
        "ID": "\"DL-5-136\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"物体検出の「SSD」と「YOLO」に共通する、領域候補を個別に切り出さない高速な手法の名称は？\"",
        "Opt1": "\"1段階（One-stage）検出器\"",
        "Opt2": "\"2段階（Two-stage）検出器\"",
        "Opt3": "\"RNNベース検出器\"",
        "Opt4": "\"ピクセルベース検出器\"",
        "Opt5": "\"全結合検出器\"",
        "Opt6": "\"ランダム検出器\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】画像を一括でスキャンして分類と回帰を同時に行うため、リアルタイム処理に向いています。\""
    },
    {
        "ID": "\"DL-5-137\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"強化学習において、エージェントが「あえて期待値の低い行動」を試すことで、より良い方策を見つける可能性を探ることを何というか？\"",
        "Opt1": "\"探索（Exploration）\"",
        "Opt2": "\"活用（Exploitation）\"",
        "Opt3": "\"再構成\"",
        "Opt4": "\"正規化\"",
        "Opt5": "\"蒸留\"",
        "Opt6": "\"量子化\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】「探索」なしでは、目先の利益に囚われてしまい、真の最適解に辿り着けません。\""
    },
    {
        "ID": "\"DL-5-138\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"GANの学習で、GeneratorとDiscriminatorのどちらか一方が強すぎると学習が進まなくなる理由として正しいのは？\"",
        "Opt1": "\"勾配消失（または学習の停滞）が起きるから\"",
        "Opt2": "\"メモリが不足するから\"",
        "Opt3": "\"画像が白黒になるから\"",
        "Opt4": "\"データが消去されるから\"",
        "Opt5": "\"GPUが熱くなるから\"",
        "Opt6": "\"正解率が100%になるから\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】特にDiscriminatorが完璧に見破るようになると、Generatorは改善のヒント（勾配）を得られなくなります。\""
    },
    {
        "ID": "\"DL-5-139\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"自然言語処理の評価指標「ROUGE」が主に使われるタスクは？\"",
        "Opt1": "\"翻訳\"",
        "Opt2": "\"文章要約\"",
        "Opt3": "\"画像キャプショニング\"",
        "Opt4": "\"音声認識\"",
        "Opt5": "\"感情分類\"",
        "Opt6": "\"物体検出\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】特に「再現率（Recall）」を重視し、生成された要約がどれだけ参照文の情報をカバーしているかを見ます。\""
    },
    {
        "ID": "\"DL-5-140\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"強化学習において、報酬の合計を計算する際に使われる「割引率 $ \\gamma $」の範囲は？\"",
        "Opt1": "\"$[0",
        "Opt2": "1]$\"",
        "Opt3": "\"$[ -1",
        "Opt4": "1 ]$\"",
        "Opt5": "\"$[ 1",
        "Opt6": "\\infty ]$\"",
        "Answer_Idx": "\"常に0\"",
        "Explanation": "\"常に1\""
    },
    {
        "ID": "\"DL-5-141\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"マルチモーダルモデル「DALL-E」は何を生成するAIか？\"",
        "Opt1": "\"テキストから画像を生成する\"",
        "Opt2": "\"画像からテキストを生成する\"",
        "Opt3": "\"音声からテキストを生成する\"",
        "Opt4": "\"テキストを別の言語に翻訳する\"",
        "Opt5": "\"動画を要約する\"",
        "Opt6": "\"画像を3Dにする\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】OpenAIが開発した、テキストの説明（プロンプト）を元に独創的な画像を生成するモデルです。\""
    },
    {
        "ID": "\"DL-5-142\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"モデルのパラメータ数を減らさずに、一部の層のみを共有したり特定のタスクに特化させたりする学習手法は？\"",
        "Opt1": "\"マルチタスク学習\"",
        "Opt2": "\"アンサンブル学習\"",
        "Opt3": "\"自己教師あり学習\"",
        "Opt4": "\"能動学習\"",
        "Opt5": "\"強化学習\"",
        "Opt6": "\"初期化\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】Multi-task learning。複数の関連するタスクを同時に学習させることで、モデルの汎化性能が高まることがあります。\""
    },
    {
        "ID": "\"DL-5-143\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"大規模モデルを小規模なデータに適応させる際、モデル全体のパラメータを凍結し、少量の「アダプタ層」のみを追加学習させる手法を何というか？\"",
        "Opt1": "\"アダプタ（Adapter）法\"",
        "Opt2": "\"フル・ファインチューニング\"",
        "Opt3": "\"初期化\"",
        "Opt4": "\"蒸留\"",
        "Opt5": "\"剪定\"",
        "Opt6": "\"量子化\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】計算コストとメモリを劇的に節約しつつ、高い精度でモデルをカスタマイズできます。\""
    },
    {
        "ID": "\"DL-5-144\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"生成された画像が「訓練データの丸パクリ」ではなく、新規性があるかを評価する際に関連する概念は？\"",
        "Opt1": "\"多様性（Diversity）\"",
        "Opt2": "\"忠実度（Fidelity）\"",
        "Opt3": "\"モード崩壊\"",
        "Opt4": "\"正則化\"",
        "Opt5": "\"ドロップアウト\"",
        "Opt6": "\"早期終了\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】高品質かつバリエーション豊か（多様）な画像を生成できるかが、生成モデルの性能評価の肝です。\""
    },
    {
        "ID": "\"DL-5-145\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"強化学習の「Q学習」はどの種類の学習に分類されるか？\"",
        "Opt1": "\"価値ベース（Value-based）手法\"",
        "Opt2": "\"方策ベース（Policy-based）手法\"",
        "Opt3": "\"モデルベース（Model-based）手法\"",
        "Opt4": "\"教師あり学習\"",
        "Opt5": "\"非教師あり学習\"",
        "Opt6": "\"能動学習\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】状態と行動の価値（Q値）を推定することを主目的とする手法です。\""
    },
    {
        "ID": "\"DL-5-146\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"物体検出の「アンカーボックス」について適切な記述は？\"",
        "Opt1": "\"物体の形状の目安として事前に用意された複数の矩形領域\"",
        "Opt2": "\"学習中に自動で生成されるノイズ\"",
        "Opt3": "\"背景と物体を区別するためのフィルタ\"",
        "Opt4": "\"全結合層の別名\"",
        "Opt5": "\"パディングの幅のこと\"",
        "Opt6": "\"損失関数の名前\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】SSDやFaster R-CNNなどで、様々なアスペクト比の枠を事前に定義しておくことで、検出を容易にします。\""
    },
    {
        "ID": "\"DL-5-147\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"「セマンティックセグメンテーション」では、画像内の「背景」はどのように扱われるか？\"",
        "Opt1": "\"1つのクラスとして分類される\"",
        "Opt2": "\"無視される\"",
        "Opt3": "\"物体検出の枠にされる\"",
        "Opt4": "\"黒塗りにされる\"",
        "Opt5": "\"平均値にされる\"",
        "Opt6": "\"削除される\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】背景も「建物」「道路」「空」などのクラスとして全ピクセル分類されます。\""
    },
    {
        "ID": "\"DL-5-148\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"時系列予測で、過去の一定期間（ウィンドウ）のデータだけを使って学習・予測する手法を何というか？\"",
        "Opt1": "\"スライディングウィンドウ法\"",
        "Opt2": "\"一括学習\"",
        "Opt3": "\"全件学習\"",
        "Opt4": "\"ランダムサンプリング\"",
        "Opt5": "\"ドロップアウト\"",
        "Opt6": "\"正規化\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】最新の状態を反映させるために、古いデータを捨てながら枠（ウィンドウ）をずらして処理します。\""
    },
    {
        "ID": "\"DL-5-149\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"ディープラーニングのモデルが、人間が意図した「本当の目的」ではなく、報酬を稼ぐための「ズル（抜け穴）」を見つけてしまう現象は？\"",
        "Opt1": "\"報酬ハッキング（Reward Hacking）\"",
        "Opt2": "\"勾配爆発\"",
        "Opt3": "\"モード崩壊\"",
        "Opt4": "\"過学習\"",
        "Opt5": "\"未学習\"",
        "Opt6": "\"正則化\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】強化学習で不適切な報酬設計（例：点数だけを追う）をした際に、バグを利用してハイスコアを出すなどの現象を指します。\""
    },
    {
        "ID": "\"DL-5-150\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"大規模言語モデルの「プロンプトエンジニアリング」において、数個の例示をプロンプトに含める手法は？\"",
        "Opt1": "\"Few-shotプロンプティング\"",
        "Opt2": "\"Zero-shotプロンプティング\"",
        "Opt3": "\"ファインチューニング\"",
        "Opt4": "\"蒸留\"",
        "Opt5": "\"量子化\"",
        "Opt6": "\"剪定\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】追加学習をせずに、指示文の中に見本（Shot）をいくつか見せるだけで、AIの出力精度を向上させるテクニックです。\""
    },
    {
        "ID": "\"DL-5-151\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"Transformerにおいて、複数のAttentionを並列に学習させることで、異なる観点（単語間の距離や意味的繋がりなど）から情報を抽出する仕組みは？\"",
        "Opt1": "\"シングルヘッド・アテンション\"",
        "Opt2": "\"マルチヘッド・アテンション（Multi-Head Attention）\"",
        "Opt3": "\"ソフトマックス・アテンション\"",
        "Opt4": "\"ドロップアウト・アテンション\"",
        "Opt5": "\"双方向アテンション\"",
        "Opt6": "\"クロスアテンション\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】入力を複数のヘッドに分割してAttentionを計算することで、アンサンブル学習のような効果が得られ、表現力が向上します。\""
    },
    {
        "ID": "\"DL-5-152\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"Transformerの学習時に使われる「ラベル平滑化（Label Smoothing）」の主な目的は？\"",
        "Opt1": "\"計算速度を上げること\"",
        "Opt2": "\"モデルが特定のクラスに過度に自信を持つことを防ぎ、汎化性能を高めること\"",
        "Opt3": "\"メモリを節約すること\"",
        "Opt4": "\"パディングを埋めること\"",
        "Opt5": "\"学習率を自動調整すること\"",
        "Opt6": "\"欠損値を補完すること\"",
        "Answer_Idx": 1,
        "Explanation": "\"【解説】正解ラベルを1.0ではなく0.9など少し控えめな値に調整することで、過学習を抑制し、モデルの柔軟性を保ちます。\""
    },
    {
        "ID": "\"DL-5-153\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"BERTの学習において、入力される2つの文章を区別するために、トークンの先頭や区切りに挿入される特殊なトークンは？\"",
        "Opt1": "\"[CLS] と [SEP]\"",
        "Opt2": "\"[START] と [END]\"",
        "Opt3": "\"[PAD] と [MASK]\"",
        "Opt4": "\"[UNK] と [NUM]\"",
        "Opt5": "\"[A] と [B]\"",
        "Opt6": "\"[SOS] と [EOS]\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】[CLS]は文全体の表現（分類用）、[SEP]は2つの文の区切りを示すために用いられます。\""
    },
    {
        "ID": "\"DL-5-154\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"GANの学習で、GeneratorとDiscriminatorの力の均衡が崩れ、勾配が消失して学習が進まなくなる現象を回避するために提案された、確率分布間の「距離」を再定義した手法は？\"",
        "Opt1": "\"WGAN（Wasserstein GAN）\"",
        "Opt2": "\"DCGAN\"",
        "Opt3": "\"CycleGAN\"",
        "Opt4": "\"CGAN\"",
        "Opt5": "\"Pix2Pix\"",
        "Opt6": "\"StyleGAN\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】EM距離（Earth Mover's Distance）を用いることで、勾配が飽和しにくくなり、学習の安定性が大幅に向上しました。\""
    },
    {
        "ID": "\"DL-5-155\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"強化学習のDQNにおいて、目的の値（教師信号）を計算する際に、更新対象とは別のネットワークを一定期間固定して使用する手法は？\"",
        "Opt1": "\"ターゲットネットワーク\"",
        "Opt2": "\"経験再生\"",
        "Opt3": "\"優先度付き経験再生\"",
        "Opt4": "\"二重Q学習（Double DQN）\"",
        "Opt5": "\"勾配クリッピング\"",
        "Opt6": "\"ドロップアウト\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】更新中の値自体を目標にすると学習が発散しやすいため、古い重みを持つネットワーク（Target Network）を別に用意します。\""
    },
    {
        "ID": "\"DL-5-156\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"「AlphaGo Master」で採用された、着手予想と勝敗予想を一つのネットワークで同時に行う構造を何というか？\"",
        "Opt1": "\"マルチタスク学習（Dual Pipeline）\"",
        "Opt2": "\"アンサンブル学習\"",
        "Opt3": "\"転移学習\"",
        "Opt4": "\"自己教師あり学習\"",
        "Opt5": "\"能動学習\"",
        "Opt6": "\"強化学習のみ\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】方策（Policy）と価値（Value）を単一のネットワークから出力させることで、効率的な学習と推論を実現しました。\""
    },
    {
        "ID": "\"DL-5-157\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"画像生成の「StyleGAN」において、異なる画像のスタイル（髪型や顔立ちなど）を合成する手法を何というか？\"",
        "Opt1": "\"スタイルミキシング（Style Mixing）\"",
        "Opt2": "\"アンサンブル\"",
        "Opt3": "\"データの水増し\"",
        "Opt4": "\"正規化\"",
        "Opt5": "\"量子化\"",
        "Opt6": "\"蒸留\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】中間的な潜在変数の一部を入れ替えることで、大まかな形状と細かいテクスチャを別々に制御・合成することが可能です。\""
    },
    {
        "ID": "\"DL-5-158\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"大規模言語モデルのGPT-3などで見られる、学習（重みの更新）を一切行わずにプロンプトの指示だけでタスクを遂行させる手法は？\"",
        "Opt1": "\"Zero-shot学習\"",
        "Opt2": "\"Few-shot学習\"",
        "Opt3": "\"ファインチューニング\"",
        "Opt4": "\"蒸留\"",
        "Opt5": "\"剪定\"",
        "Opt6": "\"量子化\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】「指示」のみを与えるのがZero-shot、数個の「例示」を含めるのがFew-shotです。\""
    },
    {
        "ID": "\"DL-5-159\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"物体検出において、背景を物体と誤認してしまう「偽陽性（False Positive）」を減らすために、誤検出した領域を重点的に再学習させる手法は？\"",
        "Opt1": "\"ハードネガティブマイニング（Hard Negative Mining）\"",
        "Opt2": "\"データ拡張\"",
        "Opt3": "\"バッチ正規化\"",
        "Opt4": "\"ドロップアウト\"",
        "Opt5": "\"平滑化\"",
        "Opt6": "\"早期終了\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】モデルが間違えやすい「紛らわしい背景」を負例として集中的に学習させることで、精度を高めます。\""
    },
    {
        "ID": "\"DL-5-160\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"GANの派生で、ペアとなるデータがなくても、AドメインからBドメイン（例：夏の風景から冬の風景）への変換を可能にする手法は？\"",
        "Opt1": "\"CycleGAN\"",
        "Opt2": "\"Pix2Pix\"",
        "Opt3": "\"SRGAN\"",
        "Opt4": "\"DCGAN\"",
        "Opt5": "\"StyleGAN\"",
        "Opt6": "\"BigGAN\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】一貫性（Cycle Consistency）損失を導入し、「A→B→A」と戻した時に元通りになるように学習させます。\""
    },
    {
        "ID": "\"DL-5-161\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"強化学習において、価値関数 $V(s)$ と行動価値関数 $Q(s",
        "Opt1": "a)$ の差（行動 $a$ が平均よりどれだけ優れているか）を何というか？\"",
        "Opt2": "\"アドバンテージ（Advantage）\"",
        "Opt3": "\"報酬\"",
        "Opt4": "\"損失\"",
        "Opt5": "\"誤差\"",
        "Opt6": "\"割引率\"",
        "Answer_Idx": "\"方策\"",
        "Explanation": 0
    },
    {
        "ID": "\"DL-5-162\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"自然言語処理のTransformerにおいて、RNNのように「前から順に」ではなく「全ての単語を同時に」処理することによる最大の利点は？\"",
        "Opt1": "\"並列化が可能になり、学習速度が飛躍的に向上すること\"",
        "Opt2": "\"メモリを全く消費しないこと\"",
        "Opt3": "\"日本語が読めなくなること\"",
        "Opt4": "\"GPUが不要になること\"",
        "Opt5": "\"文章を短くすること\"",
        "Opt6": "\"誤差が0になること\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】RNNの逐次処理の制約を取り払ったことが、近年のLLMの急速な発展の鍵となりました。\""
    },
    {
        "ID": "\"DL-5-163\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"異常検知において、正常なデータのみでオートエンコーダを学習させ、入力と復元結果の差（再構成誤差）が大きくなるものを異常とみなす手法は？\"",
        "Opt1": "\"教師なし異常検知\"",
        "Opt2": "\"教師あり異常検知\"",
        "Opt3": "\"強化学習\"",
        "Opt4": "\"物体検出\"",
        "Opt5": "\"画像分類\"",
        "Opt6": "\"回帰分析\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】正常データの「型」を覚えさせることで、そこから外れるものを異常として検知します。\""
    },
    {
        "ID": "\"DL-5-164\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"画像認識と自然言語処理を融合し、画像の内容に関する自由な質問に対して回答を生成するタスクを何というか？\"",
        "Opt1": "\"VQA（Visual Question Answering）\"",
        "Opt2": "\"画像キャプショニング\"",
        "Opt3": "\"物体検出\"",
        "Opt4": "\"セグメンテーション\"",
        "Opt5": "\"超解像\"",
        "Opt6": "\"文章要約\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】マルチモーダル学習の代表的なタスクの一つで、視覚と情報の両方の理解が求められます。\""
    },
    {
        "ID": "\"DL-5-165\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"強化学習の「A3C」において、3つ目の「A」が指す、複数のエージェントを並列に動かして学習を加速させる仕組みの名前は？\"",
        "Opt1": "\"Asynchronous（非同期）\"",
        "Opt2": "\"Advantage（アドバンテージ）\"",
        "Opt3": "\"Actor（俳優）\"",
        "Opt4": "\"Agent（エージェント）\"",
        "Opt5": "\"Automatic（自動）\"",
        "Opt6": "\"Active（能動）\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】Asynchronous Advantage Actor-Criticの略。非同期に並列学習を行うことで効率を高めています。\""
    },
    {
        "ID": "\"DL-5-166\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"BERTが「事前学習（Pre-training）」の後に、特定のタスク（感情分析など）に合わせて行う調整作業を何というか？\"",
        "Opt1": "\"ファインチューニング（Fine-tuning）\"",
        "Opt2": "\"蒸留\"",
        "Opt3": "\"量子化\"",
        "Opt4": "\"剪定\"",
        "Opt5": "\"初期化\"",
        "Opt6": "\"正則化\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】汎用的なモデルを専門分野に特化させる、転移学習の標準的なプロセスです。\""
    },
    {
        "ID": "\"DL-5-167\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"CNNの構造において、通常の畳み込み（Conv2D）の代わりに、空間方向とチャネル方向を分けて処理することで計算量を削減したものを何というか？\"",
        "Opt1": "\"Depthwise Separable Convolution\"",
        "Opt2": "\"Dilated Convolution\"",
        "Opt3": "\"Transposed Convolution\"",
        "Opt4": "\"Pointwise Convolutionのみ\"",
        "Opt5": "\"Grouping Convolution\"",
        "Opt6": "\"Pooling\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】MobileNetなどで採用されており、エッジデバイスでの高速動作を実現する重要な技術です。\""
    },
    {
        "ID": "\"DL-5-168\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"物体検出の「YOLO」シリーズにおいて、推論速度を保ちつつ精度を高めるために導入された、勾配の流れを制御するバックボーン構造は？\"",
        "Opt1": "\"CSPNet（Cross Stage Partial Network）\"",
        "Opt2": "\"ResNet\"",
        "Opt3": "\"VGG\"",
        "Opt4": "\"Inception\"",
        "Opt5": "\"AlexNet\"",
        "Opt6": "\"LeNet\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】計算量を抑えつつ、重複する勾配情報を削減して学習効率を高める構造です。\""
    },
    {
        "ID": "\"DL-5-169\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"GANの学習を評価する指標で、生成画像の鮮明さと多様性の両方を単一の数値で評価する指標は？\"",
        "Opt1": "\"Inception Score（IS）\"",
        "Opt2": "\"mAP\"",
        "Opt3": "\"IoU\"",
        "Opt4": "\"BLEU\"",
        "Opt5": "\"ROUGE\"",
        "Opt6": "\"WER\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】FIDと並んで有名なGANの評価指標ですが、最近はFIDの方が実際の見た目に近いと言われています。\""
    },
    {
        "ID": "\"DL-5-170\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"「拡散モデル」における学習のプロセス（順方向）として正しいものはどれか？\"",
        "Opt1": "\"画像に段階的にガウスノイズを加え、完全なノイズにする\"",
        "Opt2": "\"ノイズから画像を生成する\"",
        "Opt3": "\"画像を白黒にする\"",
        "Opt4": "\"画像のサイズを小さくする\"",
        "Opt5": "\"画像を分類する\"",
        "Opt6": "\"画像を回転させる\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】フォワード（拡散）プロセスでノイズを加え、逆（デノイジング）プロセスでノイズを除去する方法を学びます。\""
    },
    {
        "ID": "\"DL-5-171\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"強化学習において、エージェントが得られる報酬が非常に疎（たまにしか貰えない）な場合に、学習を助けるために人工的な中間報酬を与えることを何というか？\"",
        "Opt1": "\"報酬シェイピング（Reward Shaping）\"",
        "Opt2": "\"報酬ハッキング\"",
        "Opt3": "\"報酬消去\"",
        "Opt4": "\"報酬増加\"",
        "Opt5": "\"報酬固定\"",
        "Opt6": "\"報酬削除\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】「目的地に近づく」こと自体に小さな報酬を与えることで、学習の道標（ヒント）を作ります。\""
    },
    {
        "ID": "\"DL-5-172\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"ニューラルネットワークのパラメータのうち、値が非常に小さいものを0にして削除することで、モデルを軽量化する手法は？\"",
        "Opt1": "\"剪定（Pruning）\"",
        "Opt2": "\"量子化（Quantization）\"",
        "Opt3": "\"蒸留（Distillation）\"",
        "Opt4": "\"正則化\"",
        "Opt5": "\"バッチ正規化\"",
        "Opt6": "\"ドロップアウト\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】「プルーニング」とも呼ばれます。推論速度の向上とモデルサイズの削減に効果があります。\""
    },
    {
        "ID": "\"DL-5-173\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"画像分類タスクで、1枚の画像を複数のパッチ（断片）に分割し、Transformerに入力して処理するモデルは？\"",
        "Opt1": "\"ViT（Vision Transformer）\"",
        "Opt2": "\"CNN\"",
        "Opt3": "\"ResNet\"",
        "Opt4": "\"VGG\"",
        "Opt5": "\"GAN\"",
        "Opt6": "\"DQN\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】パッチを単語（トークン）のように扱うことで、画像認識にもTransformerが非常に有効であることを示しました。\""
    },
    {
        "ID": "\"DL-5-174\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"モデルの不確実性を評価する際、推論時にもドロップアウトを適用して複数の出力を得て、そのばらつきを測る手法は？\"",
        "Opt1": "\"MCドロップアウト（Monte Carlo Dropout）\"",
        "Opt2": "\"アンサンブル\"",
        "Opt3": "\"バッチ正規化\"",
        "Opt4": "\"早期終了\"",
        "Opt5": "\"勾配クリッピング\"",
        "Opt6": "\"L1正則化\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】AIが「どの程度自分の答えに自信がないか」を確率的に見積もることができます。\""
    },
    {
        "ID": "\"DL-5-175\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"音声処理の分野で、テキストから波形（音声）を生成する「TTS（Text-to-Speech）」において、GANを用いた高速な生成器の例は？\"",
        "Opt1": "\"MelGAN / HiFi-GAN\"",
        "Opt2": "\"WaveNet\"",
        "Opt3": "\"BERT\"",
        "Opt4": "\"Transformer\"",
        "Opt5": "\"YOLO\"",
        "Opt6": "\"ResNet\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】WaveNetよりも生成速度が非常に速く、高品質な音声を生成できるため、現代の主流となっています。\""
    },
    {
        "ID": "\"DL-5-176\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"「Attention Is All You Need」という論文で提案された、現在の自然言語処理の基礎となっているアーキテクチャは？\"",
        "Opt1": "\"Transformer\"",
        "Opt2": "\"LSTM\"",
        "Opt3": "\"RNN\"",
        "Opt4": "\"CNN\"",
        "Opt5": "\"ResNet\"",
        "Opt6": "\"VGG\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】再帰も畳み込みも使わず、Attention（注意機構）のみで系列を処理する革新的なモデルです。\""
    },
    {
        "ID": "\"DL-5-177\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"物体検出の「SSD」において、小さな物体の検出が苦手とされる主な原因は？\"",
        "Opt1": "\"浅い層の特徴マップの解像度が低すぎる（または情報の欠落）\"",
        "Opt2": "\"深い層を使いすぎる\"",
        "Opt3": "\"アンカーボックスがないから\"",
        "Opt4": "\"全結合層が多いから\"",
        "Opt5": "\"GPUが古いから\"",
        "Opt6": "\"バッチサイズが小さいから\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】浅い層には物体の詳細が残っていますが、意味的な特徴が乏しいため、小さい物体の誤検出が起きやすいです。\""
    },
    {
        "ID": "\"DL-5-178\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"強化学習の「Q学習」における『オフポリシー（方策外）』とはどういう意味か？\"",
        "Opt1": "\"現在の方策とは異なる（過去や他者の）経験から学習できること\"",
        "Opt2": "\"方策を一切使わないこと\"",
        "Opt3": "\"学習率を固定すること\"",
        "Opt4": "\"報酬を1にすること\"",
        "Opt5": "\"環境が変化しないこと\"",
        "Opt6": "\"人間が教えること\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】自分の最新の行動結果だけでなく、過去のログデータなどからも学べる柔軟性の高い手法です。\""
    },
    {
        "ID": "\"DL-5-179\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"大規模言語モデルにおいて、パラメータ数を減らすことなく、少量の追加パラメータ（LoRAなど）のみを更新して適応させる手法の総称は？\"",
        "Opt1": "\"PEFT（Parameter-Efficient Fine-Tuning）\"",
        "Opt2": "\"フル・ファインチューニング\"",
        "Opt3": "\"事前学習\"",
        "Opt4": "\"蒸留\"",
        "Opt5": "\"量子化\"",
        "Opt6": "\"剪定\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】少ないリソースで巨大なモデルを特定の用途に最適化するための必須技術です。\""
    },
    {
        "ID": "\"DL-5-180\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"生成モデルの「オートエンコーダ」の中間層で、入力を極端に低い次元に圧縮することを何というか？\"",
        "Opt1": "\"ボトルネック（Bottleneck）\"",
        "Opt2": "\"パディング\"",
        "Opt3": "\"ストライド\"",
        "Opt4": "\"ドロップアウト\"",
        "Opt5": "\"正規化\"",
        "Opt6": "\"平滑化\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】この制約により、モデルはデータの最も重要な特徴（本質）だけを抽出することを余儀なくされます。\""
    },
    {
        "ID": "\"DL-5-181\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"「CLIP」というモデルが採用している、画像とテキストを共通のベクトル空間に写像し、正解ペアの距離を近づけ、不正解ペアの距離を遠ざける学習手法は？\"",
        "Opt1": "\"対照学習（Contrastive Learning）\"",
        "Opt2": "\"教師あり学習\"",
        "Opt3": "\"強化学習\"",
        "Opt4": "\"回帰\"",
        "Opt5": "\"クラスタリング\"",
        "Opt6": "\"次元圧縮\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】正解・不正解のペアを「対照」させて比較することで、高度な概念理解を実現します。\""
    },
    {
        "ID": "\"DL-5-182\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"物体検出において、予測した境界ボックスが画像の外にはみ出さないようにクリッピングしたり調整したりする後処理を何というか？\"",
        "Opt1": "\"境界ボックスの正規化（または調整）\"",
        "Opt2": "\"NMS\"",
        "Opt3": "\"IoU計算\"",
        "Opt4": "\"量子化\"",
        "Opt5": "\"蒸留\"",
        "Opt6": "\"パディング\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】座標の予測値が画像の範囲を超えないよう、ポストプロセスで修正を行います。\""
    },
    {
        "ID": "\"DL-5-183\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"強化学習の「モンテカルロ法」において、価値を推定するために必要とされる最小の単位は？\"",
        "Opt1": "\"エピソード（開始から終了までのひとまとまり）\"",
        "Opt2": "\"1ステップ（行動1回分）\"",
        "Opt3": "\"1エポック\"",
        "Opt4": "\"1バッチ\"",
        "Opt5": "\"1秒\"",
        "Opt6": "\"無限回\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】エピソードが終わるまで待ってから累積報酬を確定させるため、TD学習に比べて学習が遅くなる傾向があります。\""
    },
    {
        "ID": "\"DL-5-184\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"自然言語処理の評価指標「ROUGE-L」の『L』が意味する、文の順序を考慮した共通部分を測定するアルゴリズムは？\"",
        "Opt1": "\"LCS（Longest Common Subsequence：最長共通部分列）\"",
        "Opt2": "\"Line（行）\"",
        "Opt3": "\"Length（長さ）\"",
        "Opt4": "\"Label（ラベル）\"",
        "Opt5": "\"Low（低い）\"",
        "Opt6": "\"Logical（論理）\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】文の構成がどれだけ似ているかを、単語の並び順を崩さずに評価します。\""
    },
    {
        "ID": "\"DL-5-185\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"「StyleGAN2」において、初期のStyleGANで発生していた「水滴のようなノイズ」を解消するために変更された手法は？\"",
        "Opt1": "\"AdaINの再設計（Weight Demodulation）\"",
        "Opt2": "\"バッチ正規化の導入\"",
        "Opt3": "\"学習率の増加\"",
        "Opt4": "\"ドロップアウトの追加\"",
        "Opt5": "\"画像の縮小\"",
        "Opt6": "\"層の削減\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】特徴量の正規化方法を見直すことで、不自然なアーティファクトを除去し、画質を大幅に改善しました。\""
    },
    {
        "ID": "\"DL-5-186\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"マルチモーダル学習において、テキストから画像を生成する際、プロンプトの内容にどれだけ忠実に従うかを制御するパラメータは？\"",
        "Opt1": "\"CFGスケール（Classifier-Free Guidance Scale）\"",
        "Opt2": "\"学習率\"",
        "Opt3": "\"バッチサイズ\"",
        "Opt4": "\"エポック数\"",
        "Opt5": "\"パディング幅\"",
        "Opt6": "\"ストライド\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】値を大きくするとプロンプトに忠実になりますが、やりすぎると画像が破綻することもあります。\""
    },
    {
        "ID": "\"DL-5-187\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"CNNにおける「転置畳み込み（Transposed Convolution）」を用いた際に発生しやすい、市松模様状のノイズを何というか？\"",
        "Opt1": "\"チェッカーボード・アーティファクト\"",
        "Opt2": "\"モード崩壊\"",
        "Opt3": "\"勾配消失\"",
        "Opt4": "\"オーバーフィッティング\"",
        "Opt5": "\"ジッタリング\"",
        "Opt6": "\"エイリアシング\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】カーネルサイズやストライドの設定が適切でない場合、重なりにムラが出てしまい模様が生じます。\""
    },
    {
        "ID": "\"DL-5-188\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"強化学習において、エージェントが「環境」のモデル（遷移確率など）を自ら持っており、それを使ってシミュレーションを行う手法は？\"",
        "Opt1": "\"モデルベース（Model-based）強化学習\"",
        "Opt2": "\"モデルフリー強化学習\"",
        "Opt3": "\"教師あり学習\"",
        "Opt4": "\"能動学習\"",
        "Opt5": "\"アンサンブル学習\"",
        "Opt6": "\"転移学習\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】環境をシミュレートすることで、実機での試行回数を減らせるメリットがありますが、モデルの精度に依存します。\""
    },
    {
        "ID": "\"DL-5-189\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"「BERT」において、文章内の特定の単語（トークン）を他の単語に置き換えたり、そのままにしたりする割合が含まれる学習手法は？\"",
        "Opt1": "\"Masked LM（15%のトークンのうち、80%をマスク、10%をランダム、10%をそのまま）\"",
        "Opt2": "\"次文予測\"",
        "Opt3": "\"蒸留\"",
        "Opt4": "\"量子化\"",
        "Opt5": "\"剪定\"",
        "Opt6": "\"ドロップアウト\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】全てをマスクするのではなく、一部をあえてそのままにすることで、推論時の「マスクがない状態」との乖離を抑えます。\""
    },
    {
        "ID": "\"DL-5-190\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"物体検出の「SSD」で採用されている「デフォルトボックス（Default Boxes）」の役割として正しいものは？\"",
        "Opt1": "\"各特徴マップの各位置において、サイズや比率の異なる物体の候補枠を提供すること\"",
        "Opt2": "\"画像を消去すること\"",
        "Opt3": "\"色を変えること\"",
        "Opt4": "\"背景を白くすること\"",
        "Opt5": "\"RNNの記憶を助けること\"",
        "Opt6": "\"損失を計算しないこと\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】アンカーボックスとも呼ばれます。これを基準に位置のズレ（オフセット）を学習します。\""
    },
    {
        "ID": "\"DL-5-191\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"GANの学習で、生成器が訓練データの一部（例えば数字の「1」だけ）しか生成しなくなる失敗状態を何というか？\"",
        "Opt1": "\"モード崩壊（Mode Collapse）\"",
        "Opt2": "\"過学習\"",
        "Opt3": "\"勾配爆発\"",
        "Opt4": "\"勾配消失\"",
        "Opt5": "\"正則化\"",
        "Opt6": "\"平滑化\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】多様性が失われ、特定のパターンしか作れなくなるGAN特有の致命的な現象です。\""
    },
    {
        "ID": "\"DL-5-192\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"Transformerの計算量は、入力系列の長さ $L$ に対してどの程度のオーダーで増加するか？（Self-Attention部分）\"",
        "Opt1": "\"$O(L^2)$（長さの2乗）\"",
        "Opt2": "\"$O(L)$（線形）\"",
        "Opt3": "\"$O(\\log L)$（対数）\"",
        "Opt4": "\"$O(1)$（一定）\"",
        "Opt5": "\"$O(L^3)$\"",
        "Opt6": "\"$O(2^L)$\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】全ての単語ペア間の関係を計算するため2乗に比例します。これが長文処理のボトルネックとなります。\""
    },
    {
        "ID": "\"DL-5-193\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"強化学習の「Q学習」において、学習が進むにつれてQ値が真の値よりも過大に評価されてしまう問題を解決した手法は？\"",
        "Opt1": "\"Double DQN\"",
        "Opt2": "\"Dueling DQN\"",
        "Opt3": "\"Prioritized Experience Replay\"",
        "Opt4": "\"A3C\"",
        "Opt5": "\"PPO\"",
        "Opt6": "\"SARSA\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】行動選択と価値評価でネットワーク（または重み）を分けることで、過大評価を抑制します。\""
    },
    {
        "ID": "\"DL-5-194\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"「CycleGAN」の損失関数において、元の画像に戻した際の一致度を測る損失の名前は？\"",
        "Opt1": "\"サイクル一貫性損失（Cycle Consistency Loss）\"",
        "Opt2": "\"二値交差エントロピー\"",
        "Opt3": "\"平均二乗誤差\"",
        "Opt4": "\"ヒンジ損失\"",
        "Opt5": "\"KLダイバージェンス\"",
        "Opt6": "\"知覚損失\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】A→B→Aと戻した時の「元通りさ」を強制することで、ペアデータなしの変換を実現します。\""
    },
    {
        "ID": "\"DL-5-195\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"自然言語処理において、大量のテキストから「単語の並びの確率」をモデル化したものを何というか？\"",
        "Opt1": "\"言語モデル（Language Model）\"",
        "Opt2": "\"物体検出モデル\"",
        "Opt3": "\"セグメンテーションモデル\"",
        "Opt4": "\"生成モデル\"",
        "Opt5": "\"分類モデル\"",
        "Opt6": "\"回帰モデル\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】次にどの単語が来るかを予測する仕組みが、文章生成や翻訳の基盤となります。\""
    },
    {
        "ID": "\"DL-5-196\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"強化学習の「Dueling DQN」において、ネットワークの出力を「状態の価値 $V(s)$」と「行動の有利さ $A(s",
        "Opt1": "a)$」に分離する目的は？\"",
        "Opt2": "\"特定の行動に依存しない状態そのものの価値を効率的に学習するため\"",
        "Opt3": "\"計算を複雑にするため\"",
        "Opt4": "\"メモリを倍使うため\"",
        "Opt5": "\"画像を明るくするため\"",
        "Opt6": "\"報酬を減らすため\"",
        "Answer_Idx": "\"環境を壊すため\"",
        "Explanation": 0
    },
    {
        "ID": "\"DL-5-197\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"物体検出の「mAP」計算において、各クラスのAP（Average Precision）を算出するために必要な処理は？\"",
        "Opt1": "\"適合率-再現率曲線（PR曲線）の下側面積の計算\"",
        "Opt2": "\"正解率の平均\"",
        "Opt3": "\"損失の合計\"",
        "Opt4": "\"GPUの使用率\"",
        "Opt5": "\"バッチサイズ\"",
        "Opt6": "\"データの数\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】信頼度しきい値を変化させた時のPrecisionとRecallの軌跡を評価します。\""
    },
    {
        "ID": "\"DL-5-198\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"ディープラーニングのモデルを圧縮する「知識蒸留」において、教師モデルの出力（確率分布）を滑らかにするために導入されるパラメータは？\"",
        "Opt1": "\"温度（Temperature）\"",
        "Opt2": "\"学習率\"",
        "Opt3": "\"モーメンタム\"",
        "Opt4": "\"重み減衰\"",
        "Opt5": "\"パディング\"",
        "Opt6": "\"ストライド\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】Softmax関数に温度を加えることで、正解以外のクラスが持つ「隠れた情報」を生徒に伝えやすくします。\""
    },
    {
        "ID": "\"DL-5-199\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"画像生成における「拡散モデル」の代表的なサンプラー手法（DDIMなど）が、初期の手法（DDPM）より優れている点は？\"",
        "Opt1": "\"少ないステップ数で高速に画像を生成できること\"",
        "Opt2": "\"画質が著しく低いこと\"",
        "Opt3": "\"計算が遅いこと\"",
        "Opt4": "\"メモリを大量に使うこと\"",
        "Opt5": "\"色が1色になること\"",
        "Opt6": "\"学習ができないこと\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】非マルコフ過程を導入することで、サンプリング（生成）の工程を大幅に短縮しました。\""
    },
    {
        "ID": "\"DL-200\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"強化学習において、エージェントが過去に訪れたことがない「未知の状態」を優先的に探索するように報酬を与える仕組みを何というか？\"",
        "Opt1": "\"内発的動機付け（Intrinsic Motivation）\"",
        "Opt2": "\"外発的報酬\"",
        "Opt3": "\"報酬削減\"",
        "Opt4": "\"ペナルティ\"",
        "Opt5": "\"正則化\"",
        "Opt6": "\"蒸留\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】「好奇心」のような報酬を自律的に生成させることで、報酬が稀な環境でも効率的に探索させます。\""
    },
    {
        "ID": "\"DL-5-201\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"TransformerのScaled Dot-Product Attentionにおいて、ドット積の結果を$\\sqrt{d_k}$で割るスケーリングの主な理由は？\"",
        "Opt1": "\"Softmaxの勾配が極端に小さくなるのを防ぎ、学習を安定させるため\"",
        "Opt2": "\"計算速度を高速化するため\"",
        "Opt3": "\"メモリ消費量を半分にするため\"",
        "Opt4": "\"バイアスを0に近づけるため\"",
        "Opt5": "\"パディングを無視するため\"",
        "Opt6": "\"層の数を増やすため\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】ドット積の値が大きくなりすぎるとSoftmax関数の勾配が消失し、学習が停滞します。次元の平方根で割ることで、これを適切な範囲に保ちます。\""
    },
    {
        "ID": "\"DL-5-202\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"BERTが採用している「双方向性」を実現するために使用されているTransformerのコンポーネントは？\"",
        "Opt1": "\"Encoder（エンコーダ）\"",
        "Opt2": "\"Decoder（デコーダ）のみ\"",
        "Opt3": "\"Encoder-Decoder両方\"",
        "Opt4": "\"RNN\"",
        "Opt5": "\"CNN\"",
        "Opt6": "\"全結合層のみ\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】BERTはTransformerのEncoder部分を多層に重ねています。Encoderは全単語間の関係を同時に見るため、双方向的な文脈を捉えることが可能です。\""
    },
    {
        "ID": "\"DL-5-203\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"物体検出の「YOLO v3」以降で採用された、異なる解像度の特徴マップを統合して小さな物体の検出精度を高める仕組みは？\"",
        "Opt1": "\"FPN（Feature Pyramid Network）\"",
        "Opt2": "\"Max Pooling\"",
        "Opt3": "\"全結合層\"",
        "Opt4": "\"ソフトマックス\"",
        "Opt5": "\"ドロップアウト\"",
        "Opt6": "\"早期終了\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】解像度の高い浅い層と、意味情報の強い深い層を統合することで、あらゆるサイズの物体に対して精度を高めています。\""
    },
    {
        "ID": "\"DL-5-204\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"GANの学習で、Discriminator（識別器）が本物と偽物を完全に見分けられない状態（正解率が約0.5）になった際のGenerator（生成器）の状態として適切なのは？\"",
        "Opt1": "\"理想的な生成（本物に近い画像を生成できている）\"",
        "Opt2": "\"学習の失敗（モード崩壊）\"",
        "Opt3": "\"勾配消失\"",
        "Opt4": "\"過学習\"",
        "Opt5": "\"未学習\"",
        "Opt6": "\"計算エラー\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】Discriminatorが「本物か偽物か迷う」状態こそがGANのゴールであり、Generatorが最も精巧な偽物を作れている証拠です。\""
    },
    {
        "ID": "\"DL-5-205\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"強化学習において、連続的な行動空間（例：ロボットの関節角度）を扱うのに適しており、Actor-Criticをベースにした手法は？\"",
        "Opt1": "\"DDPG（Deep Deterministic Policy Gradient）\"",
        "Opt2": "\"Q学習\"",
        "Opt3": "\"モンテカルロ法\"",
        "Opt4": "\"サラサ（SARSA）\"",
        "Opt5": "\"TD学習\"",
        "Opt6": "\"ε-greedy法\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】Q学習などの離散的な手法とは異なり、決定論的な方策勾配を用いることで連続的な値を出力・学習できます。\""
    },
    {
        "ID": "\"DL-5-206\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"自然言語処理のTransformerにおいて、固定のsin/cos関数ではなく、学習可能なパラメータとして単語の相対的な距離を考慮する手法を何というか？\"",
        "Opt1": "\"相対位置エンコーディング（Relative Positional Encoding）\"",
        "Opt2": "\"絶対位置エンコーディング\"",
        "Opt3": "\"単語埋め込み\"",
        "Opt4": "\"アテンション\"",
        "Opt5": "\"マスキング\"",
        "Opt6": "\"量子化\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】系列の長さが変化しても柔軟に対応でき、単語同士の「近さ」をより正確に捉えられる手法です。\""
    },
    {
        "ID": "\"DL-5-207\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"「StyleGAN」で生成される画像の属性（眼鏡の有無など）を、入力ノイズベクトルではなく、直接各層へ注入する仕組みを何というか？\"",
        "Opt1": "\"AdaIN（Adaptive Instance Normalization）\"",
        "Opt2": "\"バッチ正規化\"",
        "Opt3": "\"レイヤー正規化\"",
        "Opt4": "\"L2正則化\"",
        "Opt5": "\"ドロップアウト\"",
        "Opt6": "\"パディング\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】各層の統計量を中間ベクトルwによって制御することで、生成画像のスタイル（特徴）を階層的に操ります。\""
    },
    {
        "ID": "\"DL-5-208\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"強化学習の「SAC（Soft Actor-Critic）」において、報酬の最大化に加えて「エントロピー」を最大化する目的は？\"",
        "Opt1": "\"探索の促進と、多様な解の発見\"",
        "Opt2": "\"計算時間の短縮\"",
        "Opt3": "\"メモリの節約\"",
        "Opt4": "\"報酬の削減\"",
        "Opt5": "\"環境のリセット\"",
        "Opt6": "\"過学習の促進\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】エントロピーを最大化（＝行動のランダム性を保持）することで、特定の行動に固執せず、よりロバストで多様な探索が可能になります。\""
    },
    {
        "ID": "\"DL-5-209\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"物体検出の「Fast R-CNN」において、サイズの異なる候補領域から固定長の特徴ベクトルを取り出す操作を何というか？\"",
        "Opt1": "\"RoI Pooling（Region of Interest Pooling）\"",
        "Opt2": "\"Max Pooling\"",
        "Opt3": "\"Average Pooling\"",
        "Opt4": "\"NMS\"",
        "Opt5": "\"IoU\"",
        "Opt6": "\"Flatten\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】これにより、様々な大きさの候補領域を全結合層へ一律に入力できるようになり、計算効率が向上しました。\""
    },
    {
        "ID": "\"DL-5-110\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"言語モデルにおいて、前の単語から次の単語を1つずつ予測して文章を生成していく方式を何というか？\"",
        "Opt1": "\"自己回帰（Autoregressive）モデル\"",
        "Opt2": "\"自己符号化器\"",
        "Opt3": "\"分類モデル\"",
        "Opt4": "\"物体検出モデル\"",
        "Opt5": "\"並列生成モデル\"",
        "Opt6": "\"バッチ生成モデル\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】GPTなどのデコーダベースのモデルがこの方式を採用しており、文脈を維持した文章生成を可能にしています。\""
    },
    {
        "ID": "\"DL-5-211\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"GANの評価指標「FID（Fréchet Inception Distance）」において、スコアが「低い」ことは何を意味するか？\"",
        "Opt1": "\"生成画像が本物（訓練データ）の分布に近く、高品質であること\"",
        "Opt2": "\"生成画像が本物と全く似ていないこと\"",
        "Opt3": "\"計算が失敗していること\"",
        "Opt4": "\"生成画像が白黒であること\"",
        "Opt5": "\"多様性が全くないこと\"",
        "Opt6": "\"モデルが軽量であること\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】FIDは本物と偽物の特徴量分布の距離を測るため、値が小さい（距離が近い）ほど高品質とみなされます。\""
    },
    {
        "ID": "\"DL-5-212\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"Transformerにおいて、一度に処理できるトークンの最大数を制限している主な要因は？\"",
        "Opt1": "\"Attentionの計算量が系列長の2乗で増加するため\"",
        "Opt2": "\"メモリが無限にあるため\"",
        "Opt3": "\"CPUが使えないため\"",
        "Opt4": "\"日本語の文字数が多いため\"",
        "Opt5": "\"パディングの幅が固定だから\"",
        "Opt6": "\"ストライドが1だから\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】$O(L^2)$の問題により、文が長くなるほど計算負荷が指数関数的に増大するため、上限（例：512や2048）が設けられます。\""
    },
    {
        "ID": "\"DL-5-213\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"画像分類タスクで「ResNet」が高い精度を出せる理由は、残差学習（Residual Learning）によって何が解決されたからか？\"",
        "Opt1": "\"多層化に伴う勾配消失および劣化問題\"",
        "Opt2": "\"画像の解像度低下\"",
        "Opt3": "\"データの欠損\"",
        "Opt4": "\"計算速度の低下\"",
        "Opt5": "\"メモリ不足\"",
        "Opt6": "\"ラベルの誤り\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】スキップ接続により勾配が直接伝わるため、1000層を超えるような非常に深いネットワークでも学習が可能になりました。\""
    },
    {
        "ID": "\"DL-5-214\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"強化学習において、エージェントが「環境の状態（$s$）」だけを見て「次の行動（$a$）」を決定できる性質を何というか？\"",
        "Opt1": "\"マルコフ性（マルコフ決定過程）\"",
        "Opt2": "\"再帰性\"",
        "Opt3": "\"並列性\"",
        "Opt4": "\"独立性\"",
        "Opt5": "\"非負性\"",
        "Opt6": "\"線形性\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】「過去の履歴」を無視して「現在の状態」だけで将来の決定ができるという前提が、多くの強化学習アルゴリズムの基礎です。\""
    },
    {
        "ID": "\"DL-5-215\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"物体検出の「EfficientDet」が採用している、解像度・深さ・幅を効率的にスケーリングする手法は？\"",
        "Opt1": "\"Compound Scaling（複合スケーリング）\"",
        "Opt2": "\"ランダムスケーリング\"",
        "Opt3": "\"線形スケーリング\"",
        "Opt4": "\"固定スケーリング\"",
        "Opt5": "\"逆スケーリング\"",
        "Opt6": "\"指数スケーリング\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】これら3つの要素をバランスよく調整することで、計算リソースに対して最大の精度を引き出します。\""
    },
    {
        "ID": "\"DL-5-216\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"セグメンテーションモデル「DeepLab v3」で用いられる、複数の異なるレートのDilated Convolutionを並列に適用する機構は？\"",
        "Opt1": "\"ASPP（Atrous Spatial Pyramid Pooling）\"",
        "Opt2": "\"Max Pooling\"",
        "Opt3": "\"Global Average Pooling\"",
        "Opt4": "\"NMS\"",
        "Opt5": "\"Softmax\"",
        "Opt6": "\"ReLU\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】複数のスケールの特徴を同時に捉えることができ、物体の大きさの変化に強くなります。\""
    },
    {
        "ID": "\"DL-5-217\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"大規模言語モデルのGPT-3において、1",
        "Opt1": "750億個という膨大なパラメータ数を持つことで現れる、予期せぬ能力（例：簡単な数学や翻訳）を何というか？\"",
        "Opt2": "\"創発的性質（Emergent Abilities）\"",
        "Opt3": "\"過学習\"",
        "Opt4": "\"勾配消失\"",
        "Opt5": "\"モード崩壊\"",
        "Opt6": "\"正則化\"",
        "Answer_Idx": "\"量子化\"",
        "Explanation": 0
    },
    {
        "ID": "\"DL-5-218\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"GANの学習で用いられる「ラベル反転」や「片側のラベル平滑化」の主な目的は？\"",
        "Opt1": "\"Discriminatorが強くなりすぎるのを防ぎ、学習の安定性を高めること\"",
        "Opt2": "\"計算速度を上げること\"",
        "Opt3": "\"画像を反転させること\"",
        "Opt4": "\"ノイズを消すこと\"",
        "Opt5": "\"重みを0にすること\"",
        "Opt6": "\"損失を増やすこと\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】識別器が完璧になりすぎると生成器への勾配が消えるため、あえて難易度を調整して学習を継続させます。\""
    },
    {
        "ID": "\"DL-5-219\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"強化学習の「優先度付き経験再生（Prioritized Experience Replay）」において、サンプリングの優先順位を決める基準は？\"",
        "Opt1": "\"TD誤差（時間的差分誤差）の大きさ\"",
        "Opt2": "\"報酬の合計\"",
        "Opt3": "\"行動の回数\"",
        "Opt4": "\"ステップ数\"",
        "Opt5": "\"割引率\"",
        "Opt6": "\"学習率\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】「予測と実際の結果の差（TD誤差）」が大きい、つまり「まだ十分に学べていない驚きの大きいデータ」を優先的に再学習します。\""
    },
    {
        "ID": "\"DL-5-220\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"自己教師あり学習の「SimCLR」などが用いる、同一の画像に対して異なる加工（回転、切り抜きなど）を行い、それらの特徴を近づける手法は？\"",
        "Opt1": "\"データ拡張を利用した対照学習\"",
        "Opt2": "\"強化学習\"",
        "Opt3": "\"教師あり学習のみ\"",
        "Opt4": "\"転移学習\"",
        "Opt5": "\"知識蒸留\"",
        "Opt6": "\"剪定\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】正解ラベルがなくても、「同じ画像から作ったペアは似ているはずだ」という論理で汎用的な特徴を学習します。\""
    },
    {
        "ID": "\"DL-5-221\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"画像生成の「拡散モデル」において、逆拡散プロセスでノイズを予測するネットワークのバックボーンとして一般的に使われるのは？\"",
        "Opt1": "\"U-Net\"",
        "Opt2": "\"VGG\"",
        "Opt3": "\"AlexNet\"",
        "Opt4": "\"LeNet\"",
        "Opt5": "\"RNN\"",
        "Opt6": "\"MLP\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】入力と出力の解像度が同じであり、スキップ接続によって詳細な情報を保持できるため、画像の復元に適しています。\""
    },
    {
        "ID": "\"DL-5-222\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"自然言語処理において、単語（トークン）をさらに細かい単位に分割し、未知語（語彙にない単語）の問題を緩和する技術は？\"",
        "Opt1": "\"サブワード分割（Byte Pair Encodingなど）\"",
        "Opt2": "\"One-hotエンコーディング\"",
        "Opt3": "\"単語カウント\"",
        "Opt4": "\"ストップワード除去\"",
        "Opt5": "\"ステミング\"",
        "Opt6": "\"品詞タグ付け\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】「playing」を「play」と「##ing」に分けることで、語彙数を抑えつつ多様な語形に対応できます。\""
    },
    {
        "ID": "\"DL-5-223\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"強化学習において、エージェントが環境のダイナミクスを予測するモデルを学習し、その予測に基づいて計画を立てる手法は？\"",
        "Opt1": "\"モデルベース強化学習\"",
        "Opt2": "\"モデルフリー強化学習\"",
        "Opt3": "\"Q学習\"",
        "Opt4": "\"SARSA\"",
        "Opt5": "\"DQN\"",
        "Opt6": "\"PPO\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】「こうすれば、こうなるはずだ」というシミュレーションを頭の中で行い、効率的に行動を選択します。\""
    },
    {
        "ID": "\"DL-5-224\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"物体検出の「R-CNN」シリーズで、最後に開発された「Faster R-CNN」が「Fast R-CNN」から進化した点は？\"",
        "Opt1": "\"候補領域の抽出（RPN）をネットワーク内部に取り込み、高速化した\"",
        "Opt2": "\"全結合層をなくした\"",
        "Opt3": "\"画像を白黒にした\"",
        "Opt4": "\"計算をCPUのみにした\"",
        "Opt5": "\"RNNを導入した\"",
        "Opt6": "\"パディングをやめた\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】従来はSelective Searchなどの外部アルゴリズムに頼っていた領域抽出を深層学習化したのが最大の功績です。\""
    },
    {
        "ID": "\"DL-5-225\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"GANの派生で「StyleGAN」が顔画像の生成に非常に優れている理由の一つである、低解像度から徐々に高解像度へ学習を進める手法は？\"",
        "Opt1": "\"プログレッシブ学習（Progressive Growing）\"",
        "Opt2": "\"一括学習\"",
        "Opt3": "\"ランダム学習\"",
        "Opt4": "\"逆学習\"",
        "Opt5": "\"教師なし学習\"",
        "Opt6": "\"強化学習\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】いきなり高画質を狙わず、4x4から段階的に育てることで、安定して高精細な画像を生成できます。\""
    },
    {
        "ID": "\"DL-5-226\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"マルチモーダルモデルの評価において、テキストで指定した内容と生成された画像がどの程度一致しているかを測るスコアは？\"",
        "Opt1": "\"CLIP Score\"",
        "Opt2": "\"mAP\"",
        "Opt3": "\"IoU\"",
        "Opt4": "\"BLEU\"",
        "Opt5": "\"ROUGE\"",
        "Opt6": "\"WER\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】CLIPモデルを用いて画像とテキストの類似度を計算し、生成AIの指示への忠実度を評価します。\""
    },
    {
        "ID": "\"DL-5-227\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"Transformerにおける「Feed-Forward Network (FFN)」の役割として正しいものは？\"",
        "Opt1": "\"各単語ベクトルを個別に非線形変換し、表現力を高めること\"",
        "Opt2": "\"単語間のアテンションを計算すること\"",
        "Opt3": "\"文章を短くすること\"",
        "Opt4": "\"位置情報を加算すること\"",
        "Opt5": "\"パディングを埋めること\"",
        "Opt6": "\"損失を計算すること\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】Attention層で情報のやり取りをした後、FFN層で各単語が持つ情報を個別に加工・抽出します。\""
    },
    {
        "ID": "\"DL-5-228\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"物体検出において、物体の中心点（Keypoint）のみを予測することで、アンカーボックスを不要にした手法は？\"",
        "Opt1": "\"CenterNet（またはAnchor-free手法）\"",
        "Opt2": "\"SSD\"",
        "Opt3": "\"Faster R-CNN\"",
        "Opt4": "\"YOLO v2\"",
        "Opt5": "\"Mask R-CNN\"",
        "Opt6": "\"VGG\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】複雑なアンカー設定を省き、点としての検出を行うことで処理を簡略化しつつ精度を維持します。\""
    },
    {
        "ID": "\"DL-5-229\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"強化学習の「PPO（Proximal Policy Optimization）」で用いられる、方策の更新幅を制限する関数は？\"",
        "Opt1": "\"クリッピング関数（Clipped Surrogate Objective）\"",
        "Opt2": "\"シグモイド関数\"",
        "Opt3": "\"ReLU関数\"",
        "Opt4": "\"Softmax関数\"",
        "Opt5": "\"恒等関数\"",
        "Opt6": "\"ステップ関数\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】急激なパラメータ更新（＝方策の変化）を抑制することで、学習が不安定になるのを防ぎます。\""
    },
    {
        "ID": "\"DL-5-230\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"自然言語処理のBERTにおいて、1つの単語が複数の意味を持つ場合に、周囲の単語との関係性を考慮して得られるベクトルを何というか？\"",
        "Opt1": "\"文脈依存型単語埋め込み\"",
        "Opt2": "\"静的単語ベクトル\"",
        "Opt3": "\"One-hotベクトル\"",
        "Opt4": "\"ハッシュ値\"",
        "Opt5": "\"スカラー\"",
        "Opt6": "\"バイアス\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】同じ「Bank」でも、「銀行」か「川の土手」かを周囲から判断し、異なるベクトルとして表現します。\""
    },
    {
        "ID": "\"DL-5-231\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"GANの学習で、生成器が訓練データにある「特定の非常にきれいな画像」だけを繰り返し生成するようになる現象は？\"",
        "Opt1": "\"モード崩壊（Mode Collapse）\"",
        "Opt2": "\"過学習\"",
        "Opt3": "\"勾配消失\"",
        "Opt4": "\"正則化\"",
        "Opt5": "\"量子化\"",
        "Opt6": "\"蒸留\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】Discriminatorを騙すのに効率の良いパターンだけを作るようになり、生成の多様性が失われる現象です。\""
    },
    {
        "ID": "\"DL-5-232\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"「拡散モデル」における「サンプリング」とは、具体的にどのような作業を指すか？\"",
        "Opt1": "\"純粋なノイズから、学習した逆プロセスを繰り返して画像を生成すること\"",
        "Opt2": "\"訓練データを集めること\"",
        "Opt3": "\"画像を圧縮すること\"",
        "Opt4": "\"色を反転させること\"",
        "Opt5": "\"GPUを増設すること\"",
        "Opt6": "\"損失を計算すること\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】推論時の呼び名であり、ノイズを少しずつ取り除いて画像を作り出す「生成」の工程を指します。\""
    },
    {
        "ID": "\"DL-5-233\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"強化学習の「A2C（Advantage Actor-Critic）」と「A3C」の主な違いは？\"",
        "Opt1": "\"A2Cは同期更新、A3Cは非同期更新\"",
        "Opt2": "\"A2CはRNN、A3CはCNN\"",
        "Opt3": "\"A2Cは教師あり、A3Cは教師なし\"",
        "Opt4": "\"A2Cは画像のみ、A3Cはテキストのみ\"",
        "Opt5": "\"A2Cは1層、A3Cは100層\"",
        "Opt6": "\"両者は全く同じである\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】A3Cは複数の環境を非同期に動かしますが、A2Cはそれらを同期させてまとめて更新を行うため、実装が簡潔になります。\""
    },
    {
        "ID": "\"DL-5-234\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"セグメンテーションの評価指標である「Dice係数」と「IoU」の関係として正しいものは？\"",
        "Opt1": "\"Dice係数はIoUと正の相関があり、意味的にはほぼ同等である\"",
        "Opt2": "\"Dice係数はIoUの逆数である\"",
        "Opt3": "\"Dice係数は正解率と同じである\"",
        "Opt4": "\"Dice係数は計算できない\"",
        "Opt5": "\"両者は全く無関係である\"",
        "Opt6": "\"Dice係数は動画にしか使えない\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】計算式は異なりますが（Diceは積集合の2倍を分母の合計で割る）、どちらも領域の重なりを評価する指標です。\""
    },
    {
        "ID": "\"DL-5-235\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"大規模言語モデルのGPT-4などで、指示に従う能力を高めるために行われる「人間の評価に基づく強化学習」を何というか？\"",
        "Opt1": "\"RLHF（Reinforcement Learning from Human Feedback）\"",
        "Opt2": "\"自己教師あり学習\"",
        "Opt3": "\"能動学習\"",
        "Opt4": "\"アンサンブル学習\"",
        "Opt5": "\"知識蒸留\"",
        "Opt6": "\"剪定\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】人間が良いと感じる回答に高い報酬を与えることで、AIの回答をより安全で有益なものに調整します。\""
    },
    {
        "ID": "\"DL-5-236\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"「Vision Transformer（ViT）」において、画像の各パッチの順序をモデルに教えるために使われるベクトルは？\"",
        "Opt1": "\"Positional Embedding（位置埋め込み）\"",
        "Opt2": "\"Word Embedding\"",
        "Opt3": "\"Bias\"",
        "Opt4": "\"Weight\"",
        "Opt5": "\"Stride\"",
        "Opt6": "\"Padding\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】Transformerは並列処理のため、これがないとパッチがバラバラなパズルのピースのように扱われてしまいます。\""
    },
    {
        "ID": "\"DL-5-237\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"物体検出の「SSD」において、小さな特徴マップ（深い層）が得意とする物体のサイズは？\"",
        "Opt1": "\"大きな物体\"",
        "Opt2": "\"小さな物体\"",
        "Opt3": "\"非常に細かい点\"",
        "Opt4": "\"背景のみ\"",
        "Opt5": "\"存在しない物体\"",
        "Opt6": "\"動画のみ\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】深い層は受容野が広いため、画像全体に広がるような大きな物体を捉えるのに適しています。\""
    },
    {
        "ID": "\"DL-5-238\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"強化学習において、エージェントが過去に得た経験の中から「学習効果が高いもの」を選んで再学習させる手法は？\"",
        "Opt1": "\"優先度付き経験再生（Prioritized Experience Replay）\"",
        "Opt2": "\"ランダムサンプリング\"",
        "Opt3": "\"ドロップアウト\"",
        "Opt4": "\"バッチ正規化\"",
        "Opt5": "\"早期終了\"",
        "Opt6": "\"正則化\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】TD誤差が大きい経験は学ぶべきことが多いと判断し、学習の効率を大幅に向上させます。\""
    },
    {
        "ID": "\"DL-5-239\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"GANの「Pix2Pix」が、画像の変換精度を高めるためにGeneratorの損失に加えているものは？\"",
        "Opt1": "\"L1損失（ピクセルごとの差分）\"",
        "Opt2": "\"L2正則化\"",
        "Opt3": "\"ジニ係数\"",
        "Opt4": "\"エントロピー\"",
        "Opt5": "\"ハッシュ値\"",
        "Opt6": "\"相関係数\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】GANの損失（リアルさ）だけでなく、元画像とのピクセル的な一致（L1）を足すことで、構造のズレを防ぎます。\""
    },
    {
        "ID": "\"DL-5-240\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"自然言語処理の「Transformer」のデコーダにおいて、自分より後ろの単語を見ないようにする「Look-ahead Mask」の役割は？\"",
        "Opt1": "\"推論時と同様の制約を学習時に持たせ、未来予測を学習させるため\"",
        "Opt2": "\"計算を速くするため\"",
        "Opt3": "\"メモリを節約するため\"",
        "Opt4": "\"単語を消去するため\"",
        "Opt5": "\"文字を大きくするため\"",
        "Opt6": "\"GPUを冷やすため\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】これがないと、学習中に「答え（後ろの単語）」を見て予測するというカンニングが起きてしまいます。\""
    },
    {
        "ID": "\"DL-5-241\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"強化学習の「Q学習」において、報酬を即座に反映させるか、将来にわたって反映させるかを決定する $ \\alpha $ を何というか？\"",
        "Opt1": "\"学習率\"",
        "Opt2": "\"割引率\"",
        "Opt3": "\"探索率\"",
        "Opt4": "\"温度係数\"",
        "Opt5": "\"モーメンタム\"",
        "Opt6": "\"減衰率\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】割引率（$\\gamma$）と混同しやすいですが、$\\alpha$ は「新しい情報をどの程度今の価値に取り込むか」を決める学習率です。\""
    },
    {
        "ID": "\"DL-5-242\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"画像生成における「CLIP」の役割として、拡散モデル等と組み合わせた際の主な機能は？\"",
        "Opt1": "\"入力されたテキストをベクトル化し、生成の「ガイド」にすること\"",
        "Opt2": "\"画像を圧縮すること\"",
        "Opt3": "\"ノイズを取り除くこと\"",
        "Opt4": "\"計算を速くすること\"",
        "Opt5": "\"GPUの負荷を下げること\"",
        "Opt6": "\"画像を白黒にすること\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】プロンプトの意味を理解し、画像生成モデルが「何を作るべきか」を指示する羅針盤のような役割を果たします。\""
    },
    {
        "ID": "\"DL-5-243\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"物体検出において、予測結果の信頼度が低いものを切り捨てるための基準値を何というか？\"",
        "Opt1": "\"信頼度しきい値（Confidence Threshold）\"",
        "Opt2": "\"IoU\"",
        "Opt3": "\"mAP\"",
        "Opt4": "\"学習率\"",
        "Opt5": "\"バッチサイズ\"",
        "Opt6": "\"エポック数\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】この値を高くすると誤検出（偽陽性）は減りますが、見逃し（偽陰性）が増えるトレードオフがあります。\""
    },
    {
        "ID": "\"DL-5-244\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"「Attention」機構の計算において、Query（クエリ）とKey（キー）の一致度を測るために一般的に使われる演算は？\"",
        "Opt1": "\"内積（ドット積）\"",
        "Opt2": "\"外積\"",
        "Opt3": "\"足し算\"",
        "Opt4": "\"割り算\"",
        "Opt5": "\"微分\"",
        "Opt6": "\"積分\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】ベクトル同士の内積をとることで、どれだけ似ているか（関連が深いか）をスカラー値で算出します。\""
    },
    {
        "ID": "\"DL-5-245\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"「オートエンコーダ」を異常検知に使う際、異常データの再構成誤差はどうなると期待されるか？\"",
        "Opt1": "\"正常データよりも大きくなる\"",
        "Opt2": "\"正常データよりも小さくなる\"",
        "Opt3": "\"正常データと全く同じになる\"",
        "Opt4": "\"常に0になる\"",
        "Opt5": "\"常に無限大になる\"",
        "Opt6": "\"計算不能になる\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】モデルは正常データしか学習していないため、未知の「異常な形」を上手く復元できず、誤差が大きくなります。\""
    },
    {
        "ID": "\"DL-5-246\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"強化学習の「Sarsa」が「Q学習」と異なる点は？\"",
        "Opt1": "\"実際にとった行動に基づいてQ値を更新する（オンポリシー）\"",
        "Opt2": "\"過去の最高の行動で更新する（オフポリシー）\"",
        "Opt3": "\"ニューラルネットワークを使わない\"",
        "Opt4": "\"報酬を使わない\"",
        "Opt5": "\"環境がない\"",
        "Opt6": "\"エージェントがいない\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】Sarsaは「今の方策」に従って選んだ次の行動を使って更新するため、Q学習よりも慎重な（安全な）学習になる傾向があります。\""
    },
    {
        "ID": "\"DL-5-247\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"Transformerにおいて、Attention層の各ヘッドが出力した複数のベクトルを、最終的に1つにまとめる方法は？\"",
        "Opt1": "\"連結（Concatenate）した後に全結合層を通す\"",
        "Opt2": "\"全ての合計をとる\"",
        "Opt3": "\"平均をとる\"",
        "Opt4": "\"最大値をとる\"",
        "Opt5": "\"ランダムに選ぶ\"",
        "Opt6": "\"削除する\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】複数の視点（ヘッド）からの情報をすべて繋ぎ合わせ、1つの表現に統合します。\""
    },
    {
        "ID": "\"DL-5-248\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"「画像キャプショニング」のモデルにおいて、画像の各部分に注目しながら文章を生成するために使われる技術は？\"",
        "Opt1": "\"アテンション（Attention）\"",
        "Opt2": "\"ドロップアウト\"",
        "Opt3": "\"バッチ正規化\"",
        "Opt4": "\"パディング\"",
        "Opt5": "\"ストライド\"",
        "Opt6": "\"プーリング\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】「犬が走っている」という文を作る際、AIが画像の「犬」の部分を重点的に見て処理できるようにします。\""
    },
    {
        "ID": "\"DL-5-249\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"物体検出の「YOLO」が「Faster R-CNN」よりも圧倒的に速い理由は？\"",
        "Opt1": "\"画像を格子状に分割し、一度のスキャンで位置とクラスを同時予測するから\"",
        "Opt2": "\"画像を1ピクセルずつ処理するから\"",
        "Opt3": "\"RNNを重ねているから\"",
        "Opt4": "\"全結合層を持たないから\"",
        "Opt5": "\"CPUだけで動くから\"",
        "Opt6": "\"画像を事前に縮小するから\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】「領域提案」と「分類」を分けずに一発で行うシングルステージ方式のため、リアルタイム性に優れています。\""
    },
    {
        "ID": "\"DL-5-250\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"強化学習において、エージェントが「あえて期待値の低い行動」を試す「探索」を全く行わなくなった場合に起きる問題は？\"",
        "Opt1": "\"局所最適解（狭い範囲での正解）に陥り、真の最適解を見逃すこと\"",
        "Opt2": "\"計算が速くなりすぎること\"",
        "Opt3": "\"メモリが空くこと\"",
        "Opt4": "\"報酬が無限に増えること\"",
        "Opt5": "\"環境が壊れること\"",
        "Opt6": "\"GPUが止まること\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】今の知識だけで「活用」し続けると、もっと良い方法がある可能性を一生見つけられなくなります。\""
    },
    {
        "ID": "\"DL-5-251\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"Transformerの学習速度とメモリ効率を改善するために、アテンションの計算をタイル状に分割し、SRAMを活用して中間結果の書き出しを最小限に抑える手法は？\"",
        "Opt1": "\"FlashAttention\"",
        "Opt2": "\"Multi-Query Attention\"",
        "Opt3": "\"Sparse Attention\"",
        "Opt4": "\"Local Attention\"",
        "Opt5": "\"Linear Attention\"",
        "Opt6": "\"Dense Attention\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】FlashAttentionは、メモリアクセスのボトルネックを解消することで、計算の2乗オーダーの壁を実質的に高速化し、LLMの長文処理を可能にしました。\""
    },
    {
        "ID": "\"DL-5-252\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"複数のAttentionヘッドの間で「Key」と「Value」を共有することで、推論時のKVキャッシュのメモリ消費を劇的に削減する手法は？\"",
        "Opt1": "\"MQA（Multi-Query Attention） / GQA\"",
        "Opt2": "\"Multi-Head Attention\"",
        "Opt3": "\"Cross Attention\"",
        "Opt4": "\"Self Attention\"",
        "Opt5": "\"Hard Attention\"",
        "Opt6": "\"Soft Attention\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】MQAは全ヘッドで共通のKVを使います。GQA（Grouped Query Attention）はその中間で、ヘッドをグループ化して共有します。\""
    },
    {
        "ID": "\"DL-5-253\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"強化学習において、エージェントが収集した過去のデータセット（ログ）のみを用いて、環境との実相互作用なしに学習を行う手法は？\"",
        "Opt1": "\"オフライン強化学習（Offline RL）\"",
        "Opt2": "\"オンライン強化学習\"",
        "Opt3": "\"方策勾配法\"",
        "Opt4": "\"モンテカルロ法\"",
        "Opt5": "\"ε-greedy法\"",
        "Opt6": "\"モデルベース強化学習\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】実機を動かすリスクが高い産業ロボットや医療などの分野で注目されており、既存の固定データから最適方策を見つけ出します。\""
    },
    {
        "ID": "\"DL-5-254\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"GANの学習をさらに安定させる技術で、重みの移動平均（Exponential Moving Average）をとる手法の主な効果は？\"",
        "Opt1": "\"生成画像の品質を安定させ、アーティファクトを減らす\"",
        "Opt2": "\"学習速度を2倍にする\"",
        "Opt3": "\"メモリ消費を半分にする\"",
        "Opt4": "\"モード崩壊を完全に防ぐ\"",
        "Opt5": "\"識別器を強くする\"",
        "Opt6": "\"全結合層をなくす\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】最新の重みだけでなく、過去の重みの平均（EMA）を推論に使うことで、学習の揺らぎが抑えられ、より高品質な画像が得られます。\""
    },
    {
        "ID": "\"DL-5-255\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"画像認識モデルにおいて、画像の一部をランダムな矩形で「消去」して学習させることで、遮蔽物（オクルージョン）への耐性を高めるデータ拡張手法は？\"",
        "Opt1": "\"Random Erasing / Cutout\"",
        "Opt2": "\"Mixup\"",
        "Opt3": "\"CutMix\"",
        "Opt4": "\"AutoAugment\"",
        "Opt5": "\"Normalizing\"",
        "Opt6": "\"Pooling\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】画像の一部を隠すことで、モデルが特定の部分だけに頼らず、全体の整合性から判断するように促します。\""
    },
    {
        "ID": "\"DL-5-256\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"2つの異なる画像とそのラベルを「線形結合（混ぜ合わせ）」して、中間的な画像とラベルを生成するデータ拡張手法は？\"",
        "Opt1": "\"Mixup\"",
        "Opt2": "\"Cutout\"",
        "Opt3": "\"Crop\"",
        "Opt4": "\"Flip\"",
        "Opt5": "\"Rotate\"",
        "Opt6": "\"Standardization\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】画像AとBを透明度を変えて重ね、ラベルもそれに応じて（例：猫0.7"
    },
    {
        "ID": "\"DL-5-257\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"強化学習において、エージェントが予測したQ値と、実際に得られた報酬＋次状態のQ値の差（ベルマン誤差）の2乗和を最小化する学習を何というか？\"",
        "Opt1": "\"TD学習（時間的差分学習）\"",
        "Opt2": "\"モンテカルロ学習\"",
        "Opt3": "\"動的計画法\"",
        "Opt4": "\"教師なし学習\"",
        "Opt5": "\"自己回帰学習\"",
        "Opt6": "\"対照学習\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】Temporal Difference（TD）学習は、エピソードの終了を待たずに、次の1ステップの結果から逐次的に価値を更新します。\""
    },
    {
        "ID": "\"DL-5-258\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"物体検出の「Focal Loss」は、どのような課題を解決するために提案されたか？\"",
        "Opt1": "\"背景（背景クラス）が多すぎて、簡単な負例の損失が支配的になる問題（クラス不均衡）\"",
        "Opt2": "\"画像が暗すぎる問題\"",
        "Opt3": "\"物体が重なりすぎる問題\"",
        "Opt4": "\"計算が終わらない問題\"",
        "Opt5": "\"メモリが不足する問題\"",
        "Opt6": "\"GPUが熱くなる問題\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】簡単な負例（背景）の寄与度を下げ、学習が難しい例（境界付近など）に集中させる損失関数です。\""
    },
    {
        "ID": "\"DL-5-259\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"自然言語処理のTransformerにおいて、非常に長い系列を扱うために、過去の隠れ状態をキャッシュとして保持し、次のセグメントに引き継ぐモデルは？\"",
        "Opt1": "\"Transformer-XL\"",
        "Opt2": "\"BERT\"",
        "Opt3": "\"GPT-2\"",
        "Opt4": "\"RoBERTa\"",
        "Opt5": "\"ALBERT\"",
        "Opt6": "\"T5\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】固定長の窓（Window）を超えて情報を伝えることができ、従来のTransformerよりも遥かに長い長距離依存関係を扱えます。\""
    },
    {
        "ID": "\"DL-5-260\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"マルチモーダル学習において、動画像（ビデオ）を処理する際に、空間方向だけでなく「時間方向」にもAttentionを適用するモデルは？\"",
        "Opt1": "\"Video Vision Transformer (ViViT)\"",
        "Opt2": "\"YOLO\"",
        "Opt3": "\"SSD\"",
        "Opt4": "\"WaveNet\"",
        "Opt5": "\"DeepLab\"",
        "Opt6": "\"ResNet\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】動画を3Dパッチとして扱い、時間軸での変化も考慮することで、動作の認識や動画生成を実現します。\""
    },
    {
        "ID": "\"DL-5-261\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"強化学習の「方策勾配法」において、勾配の分散を抑えるために導入される「ベースライン」として最も一般的なものは？\"",
        "Opt1": "\"状態価値関数 V(s)\"",
        "Opt2": "\"報酬 r\"",
        "Opt3": "\"学習率 α\"",
        "Opt4": "\"割引率 γ\"",
        "Opt5": "\"エントロピー H\"",
        "Opt6": "\"アクション a\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】報酬から平均的な価値（ベースライン）を引くことで、勾配の更新を安定させます（Advantageの計算）。\""
    },
    {
        "ID": "\"DL-5-262\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"GANの「Mode Collapse（モード崩壊）」を検知または抑制するために、ミニバッチ内のデータの多様性を考慮する手法を何というか？\"",
        "Opt1": "\"Mini-batch Discrimination\"",
        "Opt2": "\"Virtual Batch Normalization\"",
        "Opt3": "\"Weight Normalization\"",
        "Opt4": "\"Spectral Normalization\"",
        "Opt5": "\"Instance Normalization\"",
        "Opt6": "\"Layer Normalization\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】バッチ内のサンプル同士が似すぎていないかをDiscriminatorに見させることで、同じような画像ばかり作るのを防ぎます。\""
    },
    {
        "ID": "\"DL-5-263\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"BERTの改善版で、学習時間を長くし、より大きなバッチサイズ、長いシーケンスで、かつNSP（次文予測）を廃止して性能を向上させたモデルは？\"",
        "Opt1": "\"RoBERTa\"",
        "Opt2": "\"ALBERT\"",
        "Opt3": "\"DistilBERT\"",
        "Opt4": "\"ELECTRA\"",
        "Opt5": "\"T5\"",
        "Opt6": "\"XLNet\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】Facebookによって提案されたモデルで、事前学習のハイパーパラメータを徹底的に最適化することで、BERTを凌駕しました。\""
    },
    {
        "ID": "\"DL-5-264\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"大規模言語モデルの「T5（Text-to-Text Transfer Transformer）」の最大の特徴は？\"",
        "Opt1": "\"翻訳、要約、分類などのあらゆるタスクを「テキスト入出力」形式で統一したこと\"",
        "Opt2": "\"画像のみを扱うこと\"",
        "Opt3": "\"RNNをベースにしていること\"",
        "Opt4": "\"パラメータが100個しかないこと\"",
        "Opt5": "\"学習をしないこと\"",
        "Opt6": "\"音声波形を直接出力すること\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】入力も出力も文字列（Text-to-Text）に統一することで、単一のモデルで多様なタスクを同じ枠組みで扱えます。\""
    },
    {
        "ID": "\"DL-5-265\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"強化学習において、エージェントが実際に行動した軌跡だけでなく、環境モデルを使って想像上の経験（仮想的な軌跡）を生成して学習する枠組みは？\"",
        "Opt1": "\"Dyna（ダイナ）アーキテクチャ\"",
        "Opt2": "\"Q学習\"",
        "Opt3": "\"モンテカルロ法\"",
        "Opt4": "\"サラサ\"",
        "Opt5": "\"Actor-Critic\"",
        "Opt6": "\"ε-greedy\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】モデルベース強化学習の一種で、現実の試行と頭の中のシミュレーションを組み合わせて効率を最大化します。\""
    },
    {
        "ID": "\"DL-5-266\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"画像生成の「Diffusion Model」において、ノイズを段階的に減らしていく際、微積分（スコアベース）の観点から解析を行うための理論的枠組みは？\"",
        "Opt1": "\"確率微分方程式（SDE）\"",
        "Opt2": "\"線形代数\"",
        "Opt3": "\"群論\"",
        "Opt4": "\"トポロジー\"",
        "Opt5": "\"量子力学\"",
        "Opt6": "\"フーリエ解析\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】拡散モデルはSDE（Stochastic Differential Equation）として記述でき、これが連続的な時間発展の理解に繋がっています。\""
    },
    {
        "ID": "\"DL-5-267\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"物体検出の「DETR（DEtection TRansformer）」が、従来のYOLOやSSDと決定的に異なる点は？\"",
        "Opt1": "\"アンカーボックスやNMS（非最大値抑制）などの手動の後処理を完全に排除したこと\"",
        "Opt2": "\"CNNのみで構成されていること\"",
        "Opt3": "\"計算が遅いこと\"",
        "Opt4": "\"1ピクセルずつ分類すること\"",
        "Opt5": "\"動画専用であること\"",
        "Opt6": "\"RNNを用いていること\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】Transformerを物体検出に導入し、集合予測として解くことで、複雑なヒューリスティクス（後処理）なしに検出を可能にしました。\""
    },
    {
        "ID": "\"DL-5-268\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"自然言語処理の事前学習タスクで、単語を隠す（Mask）代わりに、生成器が置き換えた「偽の単語」を識別器が当てる手法（ELECTRAで採用）を何というか？\"",
        "Opt1": "\"RTD（Replaced Token Detection）\"",
        "Opt2": "\"MLM\"",
        "Opt3": "\"NSP\"",
        "Opt4": "\"Seq2Seq\"",
        "Opt5": "\"CTC\"",
        "Opt6": "\"Attention\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】すべてのトークンに対して「本物か偽物か」を判定するため、BERT（15%のみ学習）よりも学習効率が高いとされます。\""
    },
    {
        "ID": "\"DL-5-269\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"強化学習の「Q学習」において、ベルマン等式による更新が「収束」するために必要な条件の一つは？\"",
        "Opt1": "\"すべての状態と行動のペアが無限回（十分に多く）訪問されること\"",
        "Opt2": "\"学習率が1であること\"",
        "Opt3": "\"報酬が常に正であること\"",
        "Opt4": "\"割引率が0であること\"",
        "Opt5": "\"環境が静止していること\"",
        "Opt6": "\"エージェントが1人であること\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】「探索」が不十分だと、最適な価値関数に収束しません。そのためε-greedy法などで網羅的に探索する必要があります。\""
    },
    {
        "ID": "\"DL-5-270\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"大規模モデルの知識を小規模モデルに継承する「蒸留（Distillation）」において、教師モデルが計算した「確率分布」を何というか？\"",
        "Opt1": "\"ソフトターゲット（Soft Target）\"",
        "Opt2": "\"ハードラベル\"",
        "Opt3": "\"損失\"",
        "Opt4": "\"バイアス\"",
        "Opt5": "\"重み\"",
        "Opt6": "\"バイナリ\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】単なる0/1の正解ラベルではなく、「猫である確率80%、犬である確率10%…」といった情報を生徒に教えます。\""
    },
    {
        "ID": "\"DL-5-271\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"物体検出において、バウンディングボックスの「中心座標のズレ」と「サイズのズレ」を最適化するための損失関数は？\"",
        "Opt1": "\"回帰損失（Smooth L1 Lossなど）\"",
        "Opt2": "\"分類損失\"",
        "Opt3": "\"ランキング損失\"",
        "Opt4": "\"クロスエントロピー\"",
        "Opt5": "\"ジニ係数\"",
        "Opt6": "\"ハッシュ損失\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】物体の種類を当てる「分類」とは別に、枠の座標（連続値）を当てる「回帰」の損失が必要です。\""
    },
    {
        "ID": "\"DL-5-272\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"GANの派生「Pix2Pix」の弱点である「同じ入力から同じ出力しか出ない（多様性の欠如）」を解決した、潜在変数を導入したモデルは？\"",
        "Opt1": "\"BicycleGAN\"",
        "Opt2": "\"CycleGAN\"",
        "Opt3": "\"StyleGAN\"",
        "Opt4": "\"DCGAN\"",
        "Opt5": "\"BigGAN\"",
        "Opt6": "\"SRGAN\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】対になる画像変換において、条件となる画像に加えて潜在ベクトルを入力することで、多様な出力を可能にします。\""
    },
    {
        "ID": "\"DL-5-273\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"強化学習において、ある状態 $s$ で最適な行動 $a$ を選んだ際に見込まれる将来の報酬の最大値を表す関数は？\"",
        "Opt1": "\"最適行動価値関数 $Q^*(s",
        "Opt2": "a)$\"",
        "Opt3": "\"方策関数 $\\pi(s)$\"",
        "Opt4": "\"状態遷移確率 $P$\"",
        "Opt5": "\"報酬関数 $R$\"",
        "Opt6": "\"損失関数 $L$\"",
        "Answer_Idx": "\"活性化関数\"",
        "Explanation": 0
    },
    {
        "ID": "\"DL-5-274\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"Transformerの学習を安定させるために、Layer Normalizationを各サブレイヤーの「前」に置く配置を何というか？\"",
        "Opt1": "\"Pre-LN\"",
        "Opt2": "\"Post-LN\"",
        "Opt3": "\"Internal-LN\"",
        "Opt4": "\"Global-LN\"",
        "Opt5": "\"None-LN\"",
        "Opt6": "\"Back-LN\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】初期のTransformerはPost-LN（後）でしたが、現在の多くの大規模モデルは勾配が安定しやすいPre-LNを採用しています。\""
    },
    {
        "ID": "\"DL-5-275\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"音声信号処理において、時間軸の波形を周波数成分に分解し、それを画像のように扱う形式を何というか？\"",
        "Opt1": "\"スペクトログラム（Spectrogram）\"",
        "Opt2": "\"ヒストグラム\"",
        "Opt3": "\"散布図\"",
        "Opt4": "\"折れ線グラフ\"",
        "Opt5": "\"円グラフ\"",
        "Opt6": "\"ヒートマップ\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】メル周波数スペクトログラムなどは、音声認識や生成においてCNNやTransformerの入力として多用されます。\""
    },
    {
        "ID": "\"DL-5-276\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"「拡散モデル（Diffusion Model）」において、画像を生成するステップ数を大幅に削減し、数ステップでの生成を可能にする「蒸留」技術は？\"",
        "Opt1": "\"一貫性モデル（Consistency Models / Distillation）\"",
        "Opt2": "\"量子化\"",
        "Opt3": "\"剪定\"",
        "Opt4": "\"ドロップアウト\"",
        "Opt5": "\"正規化\"",
        "Opt6": "\"正則化\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】学習済みの拡散プロセスをショートカットするように別のモデルを学習させることで、高速な生成を実現します。\""
    },
    {
        "ID": "\"DL-5-277\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"BERTの学習済みモデルを特定のタスクに使う際、[CLS]トークンの最終出力ベクトルをそのまま入力として使う層は？\"",
        "Opt1": "\"分類層（全結合層＋Softmax）\"",
        "Opt2": "\"畳み込み層\"",
        "Opt3": "\"プーリング層\"",
        "Opt4": "\"RNN層\"",
        "Opt5": "\"アテンション層\"",
        "Opt6": "\"埋め込み層\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】[CLS]トークンは文全体の情報を集約しているとみなされるため、そのまま文書分類などのヘッドに繋げます。\""
    },
    {
        "ID": "\"DL-5-278\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"強化学習の「SAC」で最大化を目指す対象（目的関数）として正しいものは？\"",
        "Opt1": "\"報酬 ＋ エントロピー\"",
        "Opt2": "\"報酬のみ\"",
        "Opt3": "\"エントロピーのみ\"",
        "Opt4": "\"損失の最大化\"",
        "Opt5": "\"探索回数の削減\"",
        "Opt6": "\"計算時間の短縮\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】報酬を稼ぎつつ、できるだけ「ランダムさ（エントロピー）」を保つことで、ロバストな学習を実現します。\""
    },
    {
        "ID": "\"DL-5-279\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"画像認識において、入力画像の一部を別の画像の一部で「置き換える」ことで、背景に依存しない学習を促す手法は？\"",
        "Opt1": "\"CutMix\"",
        "Opt2": "\"Mixup\"",
        "Opt3": "\"Cutout\"",
        "Opt4": "\"Crop\"",
        "Opt5": "\"Blur\"",
        "Opt6": "\"Color Jitter\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】一部を切り取って別の画像で埋めることで、モデルは「何がそこに埋まっていても周辺から物体を認識する」力を養います。\""
    },
    {
        "ID": "\"DL-5-280\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"大規模言語モデルのチューニングにおいて、既存の重みを一切変えず、入力文の先頭に「学習可能な連続ベクトル（疑似単語）」を付与する手法は？\"",
        "Opt1": "\"Prompt Tuning / Prefix Tuning\"",
        "Opt2": "\"LoRA\"",
        "Opt3": "\"ファインチューニング\"",
        "Opt4": "\"蒸留\"",
        "Opt5": "\"量子化\"",
        "Opt6": "\"アダプタ法\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】モデルの重みを固定したまま、先頭のベクトルだけをタスク最適化するように学習させる軽量な手法です。\""
    },
    {
        "ID": "\"DL-5-281\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"強化学習の「方策（Policy）」には、確率的に行動を選ぶものと、一意に行動が決まるものがある。後者を何というか？\"",
        "Opt1": "\"決定論的方策（Deterministic Policy）\"",
        "Opt2": "\"確率論的方策\"",
        "Opt3": "\"ランダム方策\"",
        "Opt4": "\"静的方策\"",
        "Opt5": "\"動的方策\"",
        "Opt6": "\"未知の方策\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】状態 $s$ に対して常に同じ行動 $a$ を返す方策です。DDPGなどのアルゴリズムで使用されます。\""
    },
    {
        "ID": "\"DL-5-282\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"自然言語処理の評価において、人間が作成した正解の「参照文」とAIが生成した「候補文」の間で、単語の重複度を測定する最も一般的な指標は？\"",
        "Opt1": "\"BLEU（Bilingual Evaluation Understudy）\"",
        "Opt2": "\"mAP\"",
        "Opt3": "\"IoU\"",
        "Opt4": "\"FID\"",
        "Opt5": "\"Inception Score\"",
        "Opt6": "\"Accuracy\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】主に機械翻訳の評価に使われ、N-gram（単語の繋がり）の一致率を計算します。\""
    },
    {
        "ID": "\"DL-5-283\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"GANの学習で、Discriminatorの出力が「0または1」に張り付いてしまい、Generatorに全く勾配が伝わらなくなる現象を何というか？\"",
        "Opt1": "\"勾配消失（Vanishing Gradients）\"",
        "Opt2": "\"勾配爆発\"",
        "Opt3": "\"モード崩壊\"",
        "Opt4": "\"過学習\"",
        "Opt5": "\"未学習\"",
        "Opt6": "\"正則化\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】Discriminatorが優秀すぎると、Generatorは「どう直せば良くなるか」というヒントを失い、学習が停止します。\""
    },
    {
        "ID": "\"DL-5-284\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"セマンティックセグメンテーションのモデル「U-Net」において、各解像度レベルで特徴量を結合（Concat）する経路の名称は？\"",
        "Opt1": "\"スキップ接続（Skip Connection）\"",
        "Opt2": "\"回帰経路\"",
        "Opt3": "\"残差経路\"",
        "Opt4": "\"プーリング経路\"",
        "Opt5": "\"ドロップアウト経路\"",
        "Opt6": "\"全結合経路\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】エンコーダ側の高解像度な位置情報を、デコーダ側に直接渡すことで、精細な境界を復元します。\""
    },
    {
        "ID": "\"DL-5-285\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"「AlphaZero」が自己対局による学習において、次の手を決める際に使用する探索アルゴリズムは？\"",
        "Opt1": "\"モンテカルロ木探索（MCTS）\"",
        "Opt2": "\"幅優先探索\"",
        "Opt3": "\"深さ優先探索\"",
        "Opt4": "\"A*アルゴリズム\"",
        "Opt5": "\"ダイクストラ法\"",
        "Opt6": "\"ランダム探索\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】ニューラルネットワークによる評価値と、シミュレーションによる探索結果を組み合わせて最適な手を選択します。\""
    },
    {
        "ID": "\"DL-5-286\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"大規模モデルをFP16（半精度浮動小数点）やINT8（8ビット整数）で動作させ、計算量とメモリを削減する技術を何というか？\"",
        "Opt1": "\"量子化（Quantization）\"",
        "Opt2": "\"剪定\"",
        "Opt3": "\"蒸留\"",
        "Opt4": "\"正規化\"",
        "Opt5": "\"正則化\"",
        "Opt6": "\"平滑化\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】精度の低下を最小限に抑えつつ、推論速度を数倍に高め、スマホなどのエッジデバイスでの実行を可能にします。\""
    },
    {
        "ID": "\"DL-5-287\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"自然言語処理のTransformerにおいて、単語の位置情報を表現するために加算されるベクトルは？\"",
        "Opt1": "\"位置エンコーディング（Positional Encoding）\"",
        "Opt2": "\"単語埋め込み\"",
        "Opt3": "\"バイアス\"",
        "Opt4": "\"重み\"",
        "Opt5": "\"ストライド\"",
        "Opt6": "\"パディング\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】Transformer自体は単語の並び順を理解できないため、sin/cos関数などを用いた位置情報を付与する必要があります。\""
    },
    {
        "ID": "\"DL-5-288\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"物体検出の性能向上において、データセット内の「小さな物体」の比率を人工的に増やすために画像をタイル状に並べる手法は？\"",
        "Opt1": "\"Mosaic（モザイク）データ拡張\"",
        "Opt2": "\"Mixup\"",
        "Opt3": "\"Cutout\"",
        "Opt4": "\"Normalization\"",
        "Opt5": "\"Pooling\"",
        "Opt6": "\"Padding\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】YOLO v4などで採用された手法で、1画面に複数の画像（物体）を詰め込むことで、小規模物体の学習効率を高めます。\""
    },
    {
        "ID": "\"DL-5-289\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"強化学習において、環境からの報酬を待たずに、エージェントが「自分の予測がどれだけ外れたか」を報酬として定義する考え方は？\"",
        "Opt1": "\"好奇心（Curiosity）ベースの学習\"",
        "Opt2": "\"外発的報酬\"",
        "Opt3": "\"TD学習\"",
        "Opt4": "\"Q学習\"",
        "Opt5": "\"サラサ\"",
        "Opt6": "\"モンテカルロ\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】内発的動機付けの一つで、予測困難な「未知の領域」へ向かうようにエージェントを誘導します。\""
    },
    {
        "ID": "\"DL-5-290\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"画像生成モデルの評価で、生成された画像の「意味的な特徴（セマンティクス）」が本物とどれだけ一致しているかを評価する損失は？\"",
        "Opt1": "\"知覚損失（Perceptual Loss / VGG Loss）\"",
        "Opt2": "\"ピクセル損失\"",
        "Opt3": "\"L1損失\"",
        "Opt4": "\"L2損失\"",
        "Opt5": "\"相関損失\"",
        "Opt6": "\"ハッシュ損失\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】学習済みのVGGなどの特徴量マップを比較することで、ピクセルの一致を超えた「見た目の正しさ」を評価します。\""
    },
    {
        "ID": "\"DL-5-291\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"「Vision Transformer（ViT）」において、画像の各パッチとは別に入力される、分類結果を出力するための特別なトークンは？\"",
        "Opt1": "\"[CLS] トークン\"",
        "Opt2": "\"[MASK] トークン\"",
        "Opt3": "\"[SEP] トークン\"",
        "Opt4": "\"[PAD] トークン\"",
        "Opt5": "\"[EOS] トークン\"",
        "Opt6": "\"[UNK] トークン\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】BERTの設計を模倣しており、このトークンの最終出力が画像全体のクラス分類に使用されます。\""
    },
    {
        "ID": "\"DL-5-292\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"強化学習において、エージェントが特定の状態 $s$ で行動 $a$ を選んだ直後に受け取る「環境からの値」を何というか？\"",
        "Opt1": "\"即時報酬（Immediate Reward）\"",
        "Opt2": "\"価値\"",
        "Opt3": "\"方策\"",
        "Opt4": "\"遷移確率\"",
        "Opt5": "\"割引率\"",
        "Opt6": "\"誤差\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】行動の結果として得られる最も基本的なフィードバック信号です。\""
    },
    {
        "ID": "\"DL-5-293\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"GANの学習で、生成器（Generator）の更新回数に対して、識別器（Discriminator）の更新回数を増やす（例：1対5）手法の目的は？\"",
        "Opt1": "\"Discriminatorを常に「理想的な状態（最強）」に近く保ち、正しい勾配をGeneratorに送るため\"",
        "Opt2": "\"計算速度を上げるため\"",
        "Opt3": "\"メモリを節約するため\"",
        "Opt4": "\"Generatorを休ませるため\"",
        "Opt5": "\"画像を白黒にするため\"",
        "Opt6": "\"誤差を増やすため\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】理論上、Generatorは「現在の最強のDiscriminator」を目標に学習するため、Discriminatorの学習を先行させることが有効な場合があります。\""
    },
    {
        "ID": "\"DL-5-294\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"自然言語処理の「Transformer」デコーダにおいて、各層で「過去の全トークン」のアテンションを計算し直す無駄を省くための仕組みは？\"",
        "Opt1": "\"KVキャッシュ（Key-Value Cache）\"",
        "Opt2": "\"ドロップアウト\"",
        "Opt3": "\"バッチ正規化\"",
        "Opt4": "\"パディング\"",
        "Opt5": "\"ストライド\"",
        "Opt6": "\"プーリング\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】推論時、過去に計算したKeyとValueのベクトルをメモリに保持しておくことで、毎ステップの計算量を劇的に削減します。\""
    },
    {
        "ID": "\"DL-5-295\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"物体検出の「IoU」を用いた評価において、正解との重なりが一定（例：0.5）以上であっても、クラス分類が間違っている場合は？\"",
        "Opt1": "\"不正解（FP：偽陽性）としてカウントされる\"",
        "Opt2": "\"正解としてカウントされる\"",
        "Opt3": "\"無視される\"",
        "Opt4": "\"0.5点として計算される\"",
        "Opt5": "\"エラーになる\"",
        "Opt6": "\"学習が止まる\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】物体検出は「位置」と「種類」の両方が合っていなければなりません。位置だけ合っていてもクラスが違えば誤検出です。\""
    },
    {
        "ID": "\"DL-5-296\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"強化学習において、行動の選択肢が「右・左・上・下」のように有限個である空間を何というか？\"",
        "Opt1": "\"離散的（Discrete）行動空間\"",
        "Opt2": "\"連続的行動空間\"",
        "Opt3": "\"無限行動空間\"",
        "Opt4": "\"多次元行動空間\"",
        "Opt5": "\"動的行動空間\"",
        "Opt6": "\"静的行動空間\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】DQNなどの手法はこの離散空間を前提としています。対して、ロボットの角度などは連続空間となります。\""
    },
    {
        "ID": "\"DL-5-297\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"「拡散モデル」の基盤となった考え方で、データ分布の勾配（スコア）を学習し、その勾配に従ってデータを変化させていく手法を何というか？\"",
        "Opt1": "\"スコアベース生成モデル（Score-based Generative Modeling）\"",
        "Opt2": "\"GAN\"",
        "Opt3": "\"VAE\"",
        "Opt4": "\"自己回帰モデル\"",
        "Opt5": "\"CNN\"",
        "Opt6": "\"RNN\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】エネルギー関数やスコアマッチングの理論を応用したもので、現在の拡散モデルと数学的に統合されています。\""
    },
    {
        "ID": "\"DL-5-298\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"大規模言語モデルの「幻覚（Hallucination）」を抑制するために、外部の信頼できる知識ソース（検索エンジン等）を組み合わせて回答させる手法は？\"",
        "Opt1": "\"RAG（Retrieval-Augmented Generation）\"",
        "Opt2": "\"ファインチューニングのみ\"",
        "Opt3": "\"RLHFのみ\"",
        "Opt4": "\"蒸留\"",
        "Opt5": "\"量子化\"",
        "Opt6": "\"剪定\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】モデル内の古い知識や曖昧な記憶に頼らず、最新の外部情報を「検索（Retrieve）」して回答を生成（Generate）します。\""
    },
    {
        "ID": "\"DL-5-299\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"Transformerの「Self-Attention」において、Query、Key、Valueのすべてのベクトルが「同じ入力」から生成される理由は？\"",
        "Opt1": "\"自分自身の文脈（単語間の関係）を理解するため\"",
        "Opt2": "\"計算量を増やすため\"",
        "Opt3": "\"外部の辞書を参照するため\"",
        "Opt4": "\"画像を読み込むため\"",
        "Opt5": "\"音声を変換するため\"",
        "Opt6": "\"エラーを防ぐため\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】「Self」という名の通り、自分の文章の中での各単語の役割を相互に評価するために、同じ入力から3つの役割を生成します。\""
    },
    {
        "ID": "\"DL-5-300\"",
        "Category": "\"5.ディープラーニング手法\"",
        "Question": "\"強化学習の「方策勾配法」を改善し、更新前後の方策の比率（Importance Sampling係数）を用いて、安全にパラメータを更新する最新の手法は？\"",
        "Opt1": "\"PPO（Proximal Policy Optimization）\"",
        "Opt2": "\"Q学習\"",
        "Opt3": "\"DQN\"",
        "Opt4": "\"SARSA\"",
        "Opt5": "\"モンテカルロ法\"",
        "Opt6": "\"線形回帰\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】現在の強化学習において最も標準的に使われる手法の一つで、実装が比較的容易でありながら非常に高い安定性を誇ります。\""
    },
    {
        "ID": "\"LE-6-001\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"日本の著作権法第30条の4において、AIの学習目的で著作物を利用する場合の規定として正しいものは？\"",
        "Opt1": "\"営利目的であっても、情報の解析を目的とするならば、原則として許諾なく著作物を利用できる。\"",
        "Opt2": "\"営利目的の場合は、いかなる理由があっても著作権者の許諾が必要である。\"",
        "Opt3": "\"インターネット上に公開されている著作物のみが対象であり、書籍などは対象外である。\"",
        "Opt4": "\"学習によって生成された画像が既存の著作物と類似していても、侵害にはならない。\"",
        "Opt5": "\"著作権者の利益を不当に害する場合であっても、研究目的であれば利用できる。\"",
        "Opt6": "\"海外の著作物は日本の著作権法第30条の4の適用対象外である。\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】日本は世界でも稀な「機械学習パラダイス」と呼ばれ、営利・非営利を問わず、享受を目的としない解析であれば許諾不要です。ただし「権利者の利益を不当に害する場合（例：特定のクリエイターの作風を模倣する目的でそのデータセットを販売する等）」は例外となります。\""
    },
    {
        "ID": "\"LE-6-002\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"個人情報保護法における「仮名加工情報」の説明として正しいものは？\"",
        "Opt1": "\"他の情報と照合しない限り特定の個人を識別できないように加工された情報であり、社内分析などの目的に限定される。\"",
        "Opt2": "\"特定の個人を完全に識別できないように加工された情報であり、第三者への自由な提供が可能である。\"",
        "Opt3": "\"氏名を削除するだけでよく、住所や生年月日はそのまま保持してよい情報のことである。\"",
        "Opt4": "\"個人情報には該当しないため、漏洩しても報告義務は一切発生しない。\"",
        "Opt5": "\"復元するための「対応表」を破棄しなければならない情報のことである。\"",
        "Opt6": "\"仮名加工情報は、本人の同意があれば第三者提供が制限なく認められる。\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】「仮名加工情報」は2020年改正で新設されました。社内での柔軟なデータ活用を目的としており、第三者提供は原則禁止、復元（照合）も禁止されています。一方、第三者提供が可能なのは「匿名加工情報」です。\""
    },
    {
        "ID": "\"LE-6-003\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"EUで合意された「AI法（AI Act）」において、最も厳しい『容認できないリスク』として禁止されている行為はどれか。\"",
        "Opt1": "\"公共の場でのリアルタイム遠隔生体識別や、社会的信用スコアリング。\"",
        "Opt2": "\"求人選考や学校の入学選考におけるAIの活用。\"",
        "Opt3": "\"チャットボットによるカスタマーサポート。\"",
        "Opt4": "\"AIを用いたスパムメールのフィルタリング。\"",
        "Opt5": "\"医療診断における画像解析支援。\"",
        "Opt6": "\"銀行の融資審査における与信管理。\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】EU AI法はリスクを4段階（容認できない、高、限定的、最小限）に分類しています。社会的信用スコアリングなどは人権侵害の恐れがあるとして原則「禁止」です。\""
    },
    {
        "ID": "\"LE-6-004\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"AIの開発において、開発者が特定のアルゴリズムを選択した理由や、判断の根拠を人間が理解できるように説明する性質を何というか。\"",
        "Opt1": "\"説明可能性（Explainability / XAI）\"",
        "Opt2": "\"ロバスト性（Robustness）\"",
        "Opt3": "\"スケーラビリティ（Scalability）\"",
        "Opt4": "\"公平性（Fairness）\"",
        "Opt5": "\"可用性（Availability）\"",
        "Opt6": "\"保守性（Maintainability）\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】ディープラーニングは「ブラックボックス」になりやすいため、なぜその結果になったかを説明する技術（LIMEやSHAPなど）が倫理・法律面で重視されています。\""
    },
    {
        "ID": "\"LE-6-005\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"経済産業省が策定した「AI・データの利用に関する契約ガイドライン」において、AI開発における「学習済みモデル」の権利帰属の考え方として推奨されているのは？\"",
        "Opt1": "\"開発への寄与度や対価の支払状況に応じて、利用権限を柔軟に設計（シェア）する。\"",
        "Opt2": "\"委託側（ユーザー）にすべての著作権が帰属することを大原則とする。\"",
        "Opt3": "\"受託側（ベンダー）にすべての著作権が帰属することを大原則とする。\"",
        "Opt4": "\"特許法に基づき、常に国が権利を管理する。\"",
        "Opt5": "\"オープンソースとして公開することを必須とする。\"",
        "Opt6": "\"学習済みモデルには権利が発生しないため、契約で定める必要はない。\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】モデルの権利を一律にどちらかに決めるのではなく、ノウハウの流出防止や将来の活用を考慮し、利用範囲や独占権などを個別契約で柔軟に定めることを推奨しています。\""
    },
    {
        "ID": "\"LE-6-006\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"AIが生成したコンテンツ（画像や文章）が、既存の著作物と類似しており、かつその著作物を学習したことが明らかな場合に発生する問題は？\"",
        "Opt1": "\"著作権侵害（依拠性があるため）\"",
        "Opt2": "\"特許権侵害\"",
        "Opt3": "\"意匠権侵害\"",
        "Opt4": "\"不正競争防止法違反\"",
        "Opt5": "\"公認会計士法違反\"",
        "Opt6": "\"無過失責任の追求\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】著作権侵害には「類似性」と「依拠性（既存の作品を知っていて利用したこと）」の2点が必要です。AIが学習データとしてその作品を読み込んでいた場合、依拠性が認められる可能性が高まります。\""
    },
    {
        "ID": "\"LE-6-007\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"個人情報保護法における「要配慮個人情報」に該当しないものはどれか。\"",
        "Opt1": "\"本人の過去の年収や購買履歴。\"",
        "Opt2": "\"信条（宗教、政治的見解など）。\"",
        "Opt3": "\"病歴や身体障害の有無。\"",
        "Opt4": "\"犯罪の経歴や、犯罪により害を被った事実。\"",
        "Opt5": "\"人種や社会的身分。\"",
        "Opt6": "\"医師等により行われた健康診断の結果。\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】年収、クレジットカード番号、購買履歴などは重要な情報ですが、法律上の「要配慮個人情報（不当な差別を生む恐れがある情報）」には含まれません。ひっかけの定番です。\""
    },
    {
        "ID": "\"LE-6-008\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"不当な偏見や差別を助長しないよう、AIの学習データに含まれる人種、性別、年齢などの偏りを修正し、公平な判定を目指す概念を何というか。\"",
        "Opt1": "\"AIの公平性（Algorithmic Fairness）\"",
        "Opt2": "\"AIの透明性\"",
        "Opt3": "\"AIの安全性\"",
        "Opt4": "\"AIのプライバシー保護\"",
        "Opt5": "\"AIの高速化\"",
        "Opt6": "\"AIの汎用化\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】米国の判決支援AI（COMPAS）で黒人の再犯リスクが高く見積もられた事例などが有名です。これを防ぐのが公平性の議論です。\""
    },
    {
        "ID": "\"LE-6-009\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"「ディープフェイク」による偽情報の拡散が懸念される中、AIが生成したコンテンツであることを証明するための技術的アプローチ（電子的な透かし等）を何というか。\"",
        "Opt1": "\"オリジネーター・プロファイル（OP）やウォーターマーク。\"",
        "Opt2": "\"ブロックチェーンによる暗号化。\"",
        "Opt3": "\"ファイアウォールの強化。\"",
        "Opt4": "\"CAPTCHAによる認証。\"",
        "Opt5": "\"二要素認証。\"",
        "Opt6": "\"VPNによる秘匿化。\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】コンテンツの出所（Origin）を明確にし、AI生成物であることを識別可能にすることで、偽情報対策とする動きが強まっています。\""
    },
    {
        "ID": "\"LE-6-010\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"自動運転レベル3において、事故が発生した場合の法的責任の原則的な考え方は？（日本における現時点の議論）\"",
        "Opt1": "\"システムが作動中であっても、運転者がオーバーライド（介入）要請に応じなかった場合は運転者が責任を負う。\"",
        "Opt2": "\"いかなる場合も自動車メーカーがすべての責任を負う。\"",
        "Opt3": "\"いかなる場合もソフトウェアの開発者が責任を負う。\"",
        "Opt4": "\"レベル3では運転者が存在しないため、責任を問うことはできない。\"",
        "Opt5": "\"被害者が全額負担する。\"",
        "Opt6": "\"国がすべての賠償を肩代わりする。\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】レベル3は「条件付き自動運転」であり、システムからの要請があれば人間が運転を交代しなければなりません。その交代に応じなかった場合の責任は人間に帰属します。\""
    },
    {
        "ID": "\"LE-6-011\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"特許法において、AI（プログラム）自体が発明者として認められるか？\"",
        "Opt1": "\"現状の日本の特許法では、発明者は自然人（人間）に限られ、AIは発明者になれない。\"",
        "Opt2": "\"AIが自律的に発明したものであれば、AIを発明者として登録できる。\"",
        "Opt3": "\"AIの開発会社が自動的に発明者となる。\"",
        "Opt4": "\"AIが学習に使用したデータの提供者が発明者となる。\"",
        "Opt5": "\"特許庁長官が発明者となる。\"",
        "Opt6": "\"発明者という概念自体がAI時代には撤廃された。\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】「ダブス（DABUS）」事件などの国際的な議論がありますが、日本を含む主要国では「発明者は人間であること」が要件となっています。\""
    },
    {
        "ID": "\"LE-6-012\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"不正競争防止法において、IDやパスワード等により管理され、特定の者に提供される「限定提供データ」の不正取得を防ぐ対象となっている3要件は？\"",
        "Opt1": "\"特定者提供性、電磁的管理性、相当蓄積性。\"",
        "Opt2": "\"非公知性、有用性、秘密管理性。\"",
        "Opt3": "\"新規性、進歩性、産業上の利用可能性。\"",
        "Opt4": "\"類似性、依拠性、創作性。\"",
        "Opt5": "\"生存性、識別性、個人情報性。\"",
        "Opt6": "\"公開性、共有性、安全性。\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】「限定提供データ」は2018年改正で追加されました。ちなみに、選択肢2は「営業秘密」の3要件です。混同に注意してください。\""
    },
    {
        "ID": "\"LE-6-013\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"AIの開発において、意図的にAIを誤認させるような特殊なノイズを加えた入力データを作成し、攻撃を行う手法を何というか。\"",
        "Opt1": "\"敵対的攻撃（Adversarial Attack）\"",
        "Opt2": "\"モデルポイズニング\"",
        "Opt3": "\"踏み台攻撃\"",
        "Opt4": "\"DDos攻撃\"",
        "Opt5": "\"SQLインジェクション\"",
        "Opt6": "\"フィッシング攻撃\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】道路標識に特殊なシールを貼って自動運転AIに「一時停止」を「速度制限」と誤認させるような攻撃が代表例です。\""
    },
    {
        "ID": "\"LE-6-014\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"日本政府が掲げる「人間中心のAI社会原則」の3つの基本理念はどれか。\"",
        "Opt1": "\"尊厳、多様性・包括性、持続可能性。\"",
        "Opt2": "\"自由、平等、博愛。\"",
        "Opt3": "\"効率、成長、革新。\"",
        "Opt4": "\"安全、安心、安価。\"",
        "Opt5": "\"教育、労働、福祉。\"",
        "Opt6": "\"民主主義、法の支配、基本的人権。\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】AIを道具として使いこなし、人間の尊厳を保ちながら多様性と持続可能性のある社会を目指すという指針です。\""
    },
    {
        "ID": "\"LE-6-015\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"AIの「倫理」に関する議論において、AIが特定の個人の趣味嗜好を学習し、その人に最適化された情報ばかりを表示することで、考え方が偏ってしまう現象を何というか。\"",
        "Opt1": "\"フィルターバブル（またはエコーチェンバー）\"",
        "Opt2": "\"オーバーフィッティング\"",
        "Opt3": "\"勾配消失\"",
        "Opt4": "\"モード崩壊\"",
        "Opt5": "\"次元の呪い\"",
        "Opt6": "\"内共変量シフト\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】SNSのレコメンドなどで、自分の意見に近い情報しか目に触れなくなり、社会の分断を招くリスクが指摘されています。\""
    },
    {
        "ID": "\"LE-6-051\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"個人情報保護法において、指紋データや顔認識データなど、それ単体で特定の個人を識別できる符号を何というか？\"",
        "Opt1": "\"個人識別符号\"",
        "Opt2": "\"個人情報の断片\"",
        "Opt3": "\"匿名識別子\"",
        "Opt4": "\"仮名符号\"",
        "Opt5": "\"バイオメトリクスキー\"",
        "Opt6": "\"生体管理番号\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】個人識別符号には、指紋、顔、虹彩などの生体情報（身体的特徴）や、マイナンバー、免許証番号などの公的な番号が含まれ、これら一つでも個人情報に該当します。\""
    },
    {
        "ID": "\"LE-6-052\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"AIを用いたレコメンデーション広告において、ユーザーが「自分のデータがどう使われているか」を制御できるようにする権利として、GDPRなどで重視されている概念は？\"",
        "Opt1": "\"データポータビリティ権および消去権（忘れられる権利）\"",
        "Opt2": "\"著作権\"",
        "Opt3": "\"排他的利用権\"",
        "Opt4": "\"知的財産権\"",
        "Opt5": "\"二次利用権\"",
        "Opt6": "\"無制限アクセス権\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】GDPR（欧州一般データ保護規則）では、データの持ち出し（ポータビリティ）や、自分に関するデータの削除を求める権利が明文化されています。\""
    },
    {
        "ID": "\"LE-6-053\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"AIによる自動判定によって差別的な結果が生じた際、そのモデルの内部構造やパラメータを公開するのではなく、入力と出力の関係から「なぜその判断に至ったか」を説明する手法を総称して何というか？\"",
        "Opt1": "\"事後的な説明可能性（Post-hoc Explainability）\"",
        "Opt2": "\"内部解釈性（Intrinsic Interpretability）\"",
        "Opt3": "\"ホワイトボックス化\"",
        "Opt4": "\"オープンソース化\"",
        "Opt5": "\"モデル蒸留\"",
        "Opt6": "\"透明性の完全保証\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】複雑なディープラーニングモデルの中身（ブラックボックス）を直接理解するのは困難なため、結果に対して「どの特徴量が寄与したか」を事後的に示すアプローチ（LIMEやSHAPなど）が主流です。\""
    },
    {
        "ID": "\"LE-6-054\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"「プロバイダ責任制限法」の文脈において、AIが生成した誹謗中傷コメントが投稿された際、プラットフォーム事業者が責任を問われないための条件として正しいものは？\"",
        "Opt1": "\"権利侵害を知ることができたと認めるに足りる相当の理由がない場合。\"",
        "Opt2": "\"いかなる場合もプラットフォーム側に責任がある。\"",
        "Opt3": "\"AIが書いたものであれば、常に責任は免除される。\"",
        "Opt4": "\"投稿者が匿名であれば、責任は免除される。\"",
        "Opt5": "\"AIの開発者のみが責任を負う。\"",
        "Opt6": "\"警察の許可を得ていれば責任は問われない。\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】プラットフォーム側が「権利侵害が行われていることを知っていた（または知るべきだった）」場合にのみ責任が生じるという、通知と削除の仕組みに基づいています。\""
    },
    {
        "ID": "\"LE-6-055\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"AIが自動生成した広告文において、根拠がないにもかかわらず「世界一の精度」「絶対に痩せる」といった表現を使用した場合、抵触する可能性が最も高い法律は？\"",
        "Opt1": "\"景品表示法（不当表示の禁止）\"",
        "Opt2": "\"著作権法\"",
        "Opt3": "\"不正競争防止法\"",
        "Opt4": "\"個人情報保護法\"",
        "Opt5": "\"特許法\"",
        "Opt6": "\"独占禁止法\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】事実と異なる優良誤認や有利誤認を招く表示は、景品表示法によって厳しく制限されています。AI生成物であっても広告主が責任を負います。\""
    },
    {
        "ID": "\"LE-6-056\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"OECD（経済協力開発機構）が発表した「AIに関する理事会勧告」において、中心的に据えられている概念は？\"",
        "Opt1": "\"人間中心のAI（Human-centric AI）\"",
        "Opt2": "\"AIによる軍事利用の推進\"",
        "Opt3": "\"データの国家管理\"",
        "Opt4": "\"AI開発の完全自由化\"",
        "Opt5": "\"モデルの共通化\"",
        "Opt6": "\"インターネットの廃止\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】世界的な指針として「人間中心」であることが強調されており、これが日本の「人間中心のAI社会原則」などの各国のガイドラインの基礎となりました。\""
    },
    {
        "ID": "\"LE-6-057\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"個人情報保護法において、個人情報を取得する際に必須とされる行為は？\"",
        "Opt1": "\"利用目的を特定し、原則として本人に通知または公表すること。\"",
        "Opt2": "\"取得したデータをすべて暗号化すること。\"",
        "Opt3": "\"取得後すぐに仮名加工情報にすること。\"",
        "Opt4": "\"データをすべてクラウドに保存すること。\"",
        "Opt5": "\"取得した瞬間に本人に電話で確認すること。\"",
        "Opt6": "\"データを第三者に販売することを明記すること。\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】「何に使うか」を明確に（特定）し、それを本人に知らせる（通知・公表）ことが、個人情報取扱いの大原則です。\""
    },
    {
        "ID": "\"LE-6-058\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"「AI利活用ガイドライン」において、AIの利用者が留意すべき点として、AIが出力した結果を鵜呑みにせず、人間が介在して最終判断を行うことを何というか？\"",
        "Opt1": "\"Human-in-the-Loop（HITL）\"",
        "Opt2": "\"AI-in-the-Loop\"",
        "Opt3": "\"自動化の原則\"",
        "Opt4": "\"計算資源の最適化\"",
        "Opt5": "\"エラーの無視\"",
        "Opt6": "\"データの選別\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】特に医療や裁判など、人権や生命に関わる重要な決定では、最終的に人間が確認（介入）する仕組みが必要とされています。\""
    },
    {
        "ID": "\"LE-6-059\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"複数の企業が協力してAIを開発する際、各社が持ち寄ったデータの「営業秘密」を守りつつ、共同で学習を行うための技術は？\"",
        "Opt1": "\"秘密計算（または連合学習：Federated Learning）\"",
        "Opt2": "\"一括学習\"",
        "Opt3": "\"公開データセット化\"",
        "Opt4": "\"パディング\"",
        "Opt5": "\"データクレンジング\"",
        "Opt6": "\"ラベリング\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】生データを一箇所に集めることなく（秘匿したまま）学習を進めることで、企業間のノウハウ流出を防ぎながらAIを強化できます。\""
    },
    {
        "ID": "\"LE-6-060\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"AIが生成した偽画像や偽動画（ディープフェイク）を見分けるために、画像の中に目に見えない情報を埋め込む技術を何というか？\"",
        "Opt1": "\"電子透かし（Digital Watermarking）\"",
        "Opt2": "\"フィルタリング\"",
        "Opt3": "\"暗号化\"",
        "Opt4": "\"ハッシング\"",
        "Opt5": "\"キャプチャ\"",
        "Opt6": "\"タグ付け\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】生成AIの透明性を確保するため、AIが作ったものであるという証跡をデータに埋め込む技術の導入が進められています。\""
    },
    {
        "ID": "\"LE-6-061\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"「AI・データエコシステム」の構築において、データの提供者が安心してデータを共有できるように、利用目的外の利用を技術的・契約的に制限する仕組みを何というか？\"",
        "Opt1": "\"データガバナンス\"",
        "Opt2": "\"データマイニング\"",
        "Opt3": "\"データクリーニング\"",
        "Opt4": "\"データ拡張\"",
        "Opt5": "\"データウェアハウス\"",
        "Opt6": "\"データレイク\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】データの品質維持や権利保護、安全な利活用のための統治体制を指します。\""
    },
    {
        "ID": "\"LE-6-062\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"総務省・経済産業省が統合して策定した「AI事業者ガイドライン（第1.0版）」において、AI開発者・提供者・利用者に共通して求められる「10の原則」に含まれないものは？\"",
        "Opt1": "\"利益の最大化\"",
        "Opt2": "\"適正な学習\"",
        "Opt3": "\"安全性\"",
        "Opt4": "\"透明性・説明責任\"",
        "Opt5": "\"プライバシー保護\"",
        "Opt6": "\"公平性\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】利益の最大化は企業の目的ではありますが、公的なガイドラインが定める「守るべき原則」には含まれません。倫理的側面が優先されます。\""
    },
    {
        "ID": "\"LE-6-063\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"AIが自律的に行った取引（例：株の自動売買）において、プログラムのバグによって他者に損害を与えた場合の賠償責任の考え方として、最も一般的なのは？\"",
        "Opt1": "\"原則として、そのAIを利用して利益を得ている者（利用者や所有者）が責任を負う。\"",
        "Opt2": "\"AIそのものが罰金刑を受ける。\"",
        "Opt3": "\"プログラミング言語の作成者が責任を負う。\"",
        "Opt4": "\"インターネット回線の提供者が責任を負う。\"",
        "Opt5": "\"損害を受けた側が全責任を負う。\"",
        "Opt6": "\"責任は誰にも発生しない。\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】「報償責任」や「危険責任」の考え方に基づき、その仕組みで便益を得ている者が、そこから生じるリスクについても責任を負うのが法理の基本です。\""
    },
    {
        "ID": "\"LE-6-064\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"EUの「GDPR」において、欧州域外（日本など）への個人データの移転が認められるための条件の一つで、欧州委員会がその国の保護水準が十分であると認める制度を何というか？\"",
        "Opt1": "\"十分性認定（Adequacy Decision）\"",
        "Opt2": "\"標準契約条項（SCC）\"",
        "Opt3": "\"拘束的企業ルール（BCR）\"",
        "Opt4": "\"データ保護影響評価（DPIA）\"",
        "Opt5": "\"特区認定\"",
        "Opt6": "\"輸出許可\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】日本は2019年にこの認定を受けており、日欧間でのスムーズなデータ移転が可能になっています。\""
    },
    {
        "ID": "\"LE-6-065\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"AIの学習に「他人の肖像（顔写真）」を勝手に使用した場合、著作権とは別に侵害する可能性が高い権利は？\"",
        "Opt1": "\"肖像権（およびパブリシティ権）\"",
        "Opt2": "\"特許権\"",
        "Opt3": "\"意匠権\"",
        "Opt4": "\"商標権\"",
        "Opt5": "\"鉱業権\"",
        "Opt6": "\"漁業権\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】著作権が画像そのものの権利であるのに対し、肖像権は「勝手に撮影されたり公開されたりしない」という個人のプライバシーや人格に関する権利です。\""
    },
    {
        "ID": "\"LE-6-066\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"AIモデルの性能を評価する際、特定の属性（例：特定の地域の人々）に対してだけ精度が極端に低い場合、どのような問題があると言えるか？\"",
        "Opt1": "\"データの代表性の欠如（不均衡データ）によるバイアス\"",
        "Opt2": "\"過学習\"",
        "Opt3": "\"勾配消失\"",
        "Opt4": "\"メモリ不足\"",
        "Opt5": "\"ネットワーク遅延\"",
        "Opt6": "\"入力形式の誤り\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】学習データが特定の層に偏っていると、マイノリティに対して不当な結果を出すAIになり、公平性の観点から問題となります。\""
    },
    {
        "ID": "\"LE-6-067\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"ソフトウェアのソースコードを公開し、誰でも自由に利用、改変、再配布ができるようにしたライセンス形態を何というか？\"",
        "Opt1": "\"オープンソース（OSS）\"",
        "Opt2": "\"クローズドソース\"",
        "Opt3": "\"シェアウェア\"",
        "Opt4": "\"パテント\"",
        "Opt5": "\"フリーウェア\"",
        "Opt6": "\"プロプライエタリ\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】TensorFlowやPyTorchなどの主要なライブラリはOSSとして公開されており、これが現在のAIの急速な発展を支えています。\""
    },
    {
        "ID": "\"LE-6-068\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"AI開発における「PoC（Proof of Concept）」段階での契約において、失敗した際の責任の所在をあらかじめ明確にする「多段階契約」が推奨される理由は？\"",
        "Opt1": "\"AI開発は結果が不確実であり、一括で全ての責任を負うことが困難だから。\"",
        "Opt2": "\"契約書を長くして弁護士費用を稼ぐため。\"",
        "Opt3": "\"行政の指導が義務化されているから。\"",
        "Opt4": "\"開発者の給与を下げるため。\"",
        "Opt5": "\"サーバー費用を節約するため。\"",
        "Opt6": "\"特許を自動的に取得するため\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】やってみないと精度が出るかわからないAI開発の特性上、工程ごとに区切って契約を結ぶことで、リスクを適切に分担します。\""
    },
    {
        "ID": "\"LE-6-069\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"インターネット上の情報を収集する「スクレイピング」行為が禁止される可能性があるのは、どのような場合か？\"",
        "Opt1": "\"Webサイトの利用規約で禁止されており、かつサーバーに過大な負荷をかける場合。\"",
        "Opt2": "\"いかなる場合もスクレイピングは合法である。\"",
        "Opt3": "\"1ページでも取得したら違法である。\"",
        "Opt4": "\"画像を取得したら違法だが、文字なら良い。\"",
        "Opt5": "\"昼間に実行したら違法である。\"",
        "Opt6": "\"自分のPCで実行したら違法である。\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】著作権法上は解析目的であれば可能ですが、サイトの規約違反や、偽計業務妨害（サーバーへの攻撃とみなされる）などの観点から違法性を問われることがあります。\""
    },
    {
        "ID": "\"LE-6-070\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"「AI倫理」の議論において、トロッコ問題（どちらかを選べば一方が犠牲になる状況）のような究極の選択をAIにさせるべきではないとされる主な理由は？\"",
        "Opt1": "\"AIには責任能力がなく、倫理的な判断の主体になれないから。\"",
        "Opt2": "\"AIの方が計算が速いから。\"",
        "Opt3": "\"AIは電気代がかかるから。\"",
        "Opt4": "\"AIは日本語が苦手だから。\"",
        "Opt5": "\"AIは色がわからないから。\"",
        "Opt6": "\"AIは感情があるから。\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】責任を負えない機械に、生命の選択のような価値判断を委ねることの是非が倫理的な大きな論点となっています。\""
    },
    {
        "ID": "\"LE-6-071\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"2022年施行の改正個人情報保護法で導入された、漏洩が発生した際の「個人情報保護委員会への報告」と「本人への通知」の義務化条件は？\"",
        "Opt1": "\"個人の権利利益を害するおそれがある重大な事態が発生した場合。\"",
        "Opt2": "\"漏洩件数が1件でもあれば。\"",
        "Opt3": "\"漏洩したのが住所だけであれば不要。\"",
        "Opt4": "\"社内で解決できれば不要。\"",
        "Opt5": "\"パスワードがかかっていれば不要。\"",
        "Opt6": "\"警察に届け出れば委員会への報告は不要。\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】改正により、一定の重大な漏洩（要配慮個人情報の漏洩や、1"
    },
    {
        "ID": "\"LE-6-072\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"AIモデルを販売（ライセンス供与）する際、そのモデルが他者の特許を侵害していないことを保証する条項を何というか？\"",
        "Opt1": "\"非侵害保証条項\"",
        "Opt2": "\"守秘義務条項\"",
        "Opt3": "\"損害賠償条項\"",
        "Opt4": "\"不可抗力条項\"",
        "Opt5": "\"準拠法条項\"",
        "Opt6": "\"誠実協議条項\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】買い手側が「安心して使える」ようにするための重要な条項ですが、AIの場合は予測不能な面があるため、保証の範囲が争点になります。\""
    },
    {
        "ID": "\"LE-6-073\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"学習用データセットを作成するために、クラウドソーシングなどを利用して大量の画像にラベルを付ける作業を何というか？\"",
        "Opt1": "\"アノテーション\"",
        "Opt2": "\"スクレイピング\"",
        "Opt3": "\"サンプリング\"",
        "Opt4": "\"オーグメンテーション\"",
        "Opt5": "\"正規化\"",
        "Opt6": "\"量子化\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】教師あり学習に欠かせない作業ですが、作業者のバイアスが入り込んだり、劣悪な労働環境が倫理的問題になったりすることがあります。\""
    },
    {
        "ID": "\"LE-6-074\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"「AI利活用ガイドライン」等において、AIシステムが停止したり誤動作したりした際の「バックアップ体制」や「手動への切り替え」を確保することを何というか？\"",
        "Opt1": "\"レジリエンス（回復力・強靭性）\"",
        "Opt2": "\"スループット\"",
        "Opt3": "\"レイテンシ\"",
        "Opt4": "\"冗長化\"",
        "Opt5": "\"バッチ処理\"",
        "Opt6": "\"スケーリング\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】システムが壊れても致命的な結果を招かないよう、粘り強く対応できる能力のことです。\""
    },
    {
        "ID": "\"LE-6-075\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"AIの軍事利用において、人間の介在なしに標的を選択し殺傷する兵器を何というか？\"",
        "Opt1": "\"LAWS（自律型致死兵器システム）\"",
        "Opt2": "\"ドローン\"",
        "Opt3": "\"ステルス戦闘機\"",
        "Opt4": "\"スマートミサイル\"",
        "Opt5": "\"防衛ロボット\"",
        "Opt6": "\"サイバー兵器\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】Lethal Autonomous Weapons Systemsの略。国際的にその禁止や規制が強く議論されています。\""
    },
    {
        "ID": "\"LE-6-076\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"個人情報保護法において、本人からの「利用停止・消去」の請求が認められるケースとして追加された（2020年改正）理由は？\"",
        "Opt1": "\"利用する必要がなくなった場合や、本人の権利利益が害される恐れがある場合。\"",
        "Opt2": "\"本人が単に気が変わった場合。\"",
        "Opt3": "\"AIが最新のモデルに更新された場合。\"",
        "Opt4": "\"企業の株価が下がった場合。\"",
        "Opt5": "\"データが1年以上経過した場合。\"",
        "Opt6": "\"本人がSNSを退会した場合。\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】以前は「法違反がある場合」のみでしたが、改正により「利用の必要性がなくなった」等の場合にも請求できるようになりました。\""
    },
    {
        "ID": "\"LE-6-077\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"「AIガバナンス」を実現するために、企業が社内に設置する倫理審査委員会などの組織的な枠組みを何というか？\"",
        "Opt1": "\"AI倫理委員会（またはAIガバナンス会議）\"",
        "Opt2": "\"取締役会\"",
        "Opt3": "\"人事部\"",
        "Opt4": "\"品質管理部\"",
        "Opt5": "\"情報システム部\"",
        "Opt6": "\"カスタマーサポート\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】技術者だけでなく、法務や外部の有識者を交えて多角的にリスクを評価する体制が求められています。\""
    },
    {
        "ID": "\"LE-6-078\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"生成AIが作成した「偽のニュース記事」によって特定企業の株価が暴落した場合、投稿者が問われる可能性がある罪は？\"",
        "Opt1": "\"偽計業務妨害罪（または風説の流布）\"",
        "Opt2": "\"窃盗罪\"",
        "Opt3": "\"住居侵入罪\"",
        "Opt4": "\"公務執行妨害罪\"",
        "Opt5": "\"贈収賄罪\"",
        "Opt6": "\"道路交通法違反\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】虚偽の情報（風説）を流して他人の業務を妨害したり、市場を混乱させたりする行為は刑事罰の対象です。\""
    },
    {
        "ID": "\"LE-6-079\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"AI開発における「著作権」の保護期間は、一般的に著作者の死後何年までか？（日本の原則）\"",
        "Opt1": "\"70年\"",
        "Opt2": "\"10年\"",
        "Opt3": "\"20年\"",
        "Opt4": "\"50年\"",
        "Opt5": "\"100年\"",
        "Opt6": "\"永久に保護される\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】かつては50年でしたが、現在は「死後70年」が原則です（法人著作の場合は公表後70年）。\""
    },
    {
        "ID": "\"LE-6-080\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"AIシステムにおいて、過去のデータに基づいた学習の結果、現状の社会にある「不平等」をそのまま再生産してしまうことを何というか？\"",
        "Opt1": "\"バイアスの増幅\"",
        "Opt2": "\"精度の向上\"",
        "Opt3": "\"学習の高速化\"",
        "Opt4": "\"汎用化\"",
        "Opt5": "\"最適化\"",
        "Opt6": "\"平滑化\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】過去に男性ばかりが採用されていたデータで学習したAIが、「女性であること」を減点対象にしてしまうような事象を指します。\""
    },
    {
        "ID": "\"LE-6-081\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"「匿名加工情報」を作成する際に、特定の個人を識別しにくくするために行われる「K-匿名化」の「K」が意味するものは？\"",
        "Opt1": "\"同一の属性を持つデータの最小個数\"",
        "Opt2": "\"加工する文字の数\"",
        "Opt3": "\"削除する列の数\"",
        "Opt4": "\"暗号化の鍵の長さ\"",
        "Opt5": "\"保存する年数\"",
        "Opt6": "\"担当者の人数\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】例えばK=3なら、同じ属性（年齢・性別など）の人が必ず3人以上存在するように加工し、特定を困難にします。\""
    },
    {
        "ID": "\"LE-6-082\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"AIを用いて「人の健康状態を予測」する場合、個人情報保護法以外に遵守すべき厚生労働省の指針は？\"",
        "Opt1": "\"医療・介護関係事業者における個人情報の適切な取扱いのためのガイダンス\"",
        "Opt2": "\"食品衛生ガイドライン\"",
        "Opt3": "\"労働安全衛生法\"",
        "Opt4": "\"薬機法\"",
        "Opt5": "\"医師法\"",
        "Opt6": "\"介護保険法\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】医療情報は極めて機微な「要配慮個人情報」を含むため、より厳格なセクター別のガイドラインが存在します。\""
    },
    {
        "ID": "\"LE-6-083\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"AIのアルゴリズムを特許出願する際、明細書に「どのようにその問題を解決するか」を具体的に記載しなければならない要件を何というか？\"",
        "Opt1": "\"実施可能要件（イネーブルメント要件）\"",
        "Opt2": "\"新規性要件\"",
        "Opt3": "\"進歩性要件\"",
        "Opt4": "\"産業上の利用可能性\"",
        "Opt5": "\"単一性要件\"",
        "Opt6": "\"優先権主張\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】他の技術者がその記載を読んで再現できるレベルで書かなければ、特許は認められません。\""
    },
    {
        "ID": "\"LE-6-084\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"AIガバナンスの国際的な枠組みである「G7広島AIプロセス」において、主に議論された対象は？\"",
        "Opt1": "\"生成AI（Generative AI）のリスクと機会\"",
        "Opt2": "\"顔認証技術の禁止\"",
        "Opt3": "\"インターネットの検閲\"",
        "Opt4": "\"ビットコインの規制\"",
        "Opt5": "\"5Gの普及\"",
        "Opt6": "\"OSの標準化\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】2023年に日本が議長国として主導し、生成AIに関する国際的な指針（行動規範）が策定されました。\""
    },
    {
        "ID": "\"LE-6-085\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"「AI倫理」において、AIが下した判断によって不利益を被った人が、その理由を問い質したり、異議を申し立てたりできる権利を何というか？\"",
        "Opt1": "\"意義申し立て権（または説明を求める権利）\"",
        "Opt2": "\"拒否権\"",
        "Opt3": "\"所有権\"",
        "Opt4": "\"商標権\"",
        "Opt5": "\"先占権\"",
        "Opt6": "\"地上権\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】特に「自動化された意思決定」に対して、人間が介入して再審査することを求める権利が重視されています。\""
    },
    {
        "ID": "\"LE-6-086\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"個人情報保護法において、本人の同意を得ずに個人データを第三者に提供できる「オプトアウト」規定の変更（2020年改正）点は？\"",
        "Opt1": "\"名簿業者などが不正に取得したデータや、他のオプトアウトデータは提供禁止になった。\"",
        "Opt2": "\"すべてのオプトアウトが禁止された。\"",
        "Opt3": "\"同意があれば報告は不要になった。\"",
        "Opt4": "\"ネットに載っていれば誰でも売って良くなった。\"",
        "Opt5": "\"海外への販売は自由になった。\"",
        "Opt6": "\"10歳未満のデータのみ禁止になった。\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】不正なデータ売買（いわゆる名簿屋対策）として、オプトアウトによる第三者提供のルールが厳格化されました。\""
    },
    {
        "ID": "\"LE-6-087\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"AIシステムの利用者が、システムの使用中に発見した「バグ」や「脆弱性」を開発者に報告する仕組みを整備することを何というか？\"",
        "Opt1": "\"責任ある開示（Responsible Disclosure）\"",
        "Opt2": "\"内部告発\"",
        "Opt3": "\"特許侵害\"",
        "Opt4": "\"営業妨害\"",
        "Opt5": "\"情報の隠蔽\"",
        "Opt6": "\"データの改ざん\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】セキュリティ上の欠陥を悪用される前に、適切に修正プロセスへ繋げるための善意の報告体制です。\""
    },
    {
        "ID": "\"LE-6-088\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"経済産業省のガイドラインにおいて、AIの品質を担保するために「外部の第三者」が評価・認証する仕組みを検討することを何というか？\"",
        "Opt1": "\"AIの第三者認証\"",
        "Opt2": "\"自己点検\"",
        "Opt3": "\"全数検査\"",
        "Opt4": "\"ランダム調査\"",
        "Opt5": "\"抜き打ち検査\"",
        "Opt6": "\"ユーザーアンケート\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】自社だけでなく外部の目を入れることで、AIの信頼性を客観的に証明しようとする動きです。\""
    },
    {
        "ID": "\"LE-6-089\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"「不正競争防止法」において、他人の商品やサービスと混同させるような「周知な表示（ロゴや名前）」を勝手に使用する行為を何というか？\"",
        "Opt1": "\"混同惹起行為\"",
        "Opt2": "\"特許侵害\"",
        "Opt3": "\"著作権侵害\"",
        "Opt4": "\"インサイダー取引\"",
        "Opt5": "\"背任罪\"",
        "Opt6": "\"公文書偽造\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】有名ブランドに似た名前をつけて消費者を騙す行為は、不正競争として差し止めや賠償の対象になります。\""
    },
    {
        "ID": "\"LE-6-090\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"AIの推論結果が正しいかどうかを確認するために、正解が既知のデータセットを用いてテストすることを何というか？\"",
        "Opt1": "\"精度評価（バリデーション / テスト）\"",
        "Opt2": "\"学習\"",
        "Opt3": "\"前処理\"",
        "Opt4": "\"デプロイ\"",
        "Opt5": "\"監視\"",
        "Opt6": "\"保守\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】社会実装においては、単に「精度が高い」だけでなく、「どのような条件下で間違いやすいか」を把握することが重要です。\""
    },
    {
        "ID": "\"LE-6-091\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"「AI倫理」の文脈で、AIの開発者が「自分の作ったAIが将来的にどのように使われるか」を予測し、悪用を防ぐ対策を講じる責任を何というか？\"",
        "Opt1": "\"設計時からの倫理（Ethics by Design）\"",
        "Opt2": "\"結果責任\"",
        "Opt3": "\"製造物責任（PL）\"",
        "Opt4": "\"契約責任\"",
        "Opt5": "\"刑事責任\"",
        "Opt6": "\"道義的責任\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】後から修正するのではなく、企画・設計の段階から倫理的リスクを織り込むという考え方です。\""
    },
    {
        "ID": "\"LE-6-092\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"日本において、AIが生成した「プログラムのコード」に著作権が発生するための条件は？\"",
        "Opt1": "\"人間にによる「創作的寄与」があること（AIを単なる道具として使った場合）。\"",
        "Opt2": "\"AIが実行ボタンを押しただけで発生する。\"",
        "Opt3": "\"コードの行数が1",
        "Opt4": "000行を超えていること。\"",
        "Opt5": "\"そのコードが実際に動作すること。\"",
        "Opt6": "\"コードの中にコメントがあること。\"",
        "Answer_Idx": "\"有料のAIツールを使った場合のみ発生する。\"",
        "Explanation": 0
    },
    {
        "ID": "\"LE-6-093\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"AIを用いた監視カメラシステムにおいて、プライバシー保護のために画像内の「顔」を自動的にぼかす技術を何というか？\"",
        "Opt1": "\"プライバシー保護画像処理（または匿名化処理）\"",
        "Opt2": "\"エッジ抽出\"",
        "Opt3": "\"コントラスト調整\"",
        "Opt4": "\"シャープネス\"",
        "Opt5": "\"色補正\"",
        "Opt6": "\"ノイズ除去\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】情報を有用に使いつつ、個人の特定を防ぐという「活用のための保護」技術です。\""
    },
    {
        "ID": "\"LE-6-094\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"「AIガバナンス」において、企業がAIに関する方針を外部に公表し、透明性を高める文書を何というか？\"",
        "Opt1": "\"AI倫理方針（またはAIポリシー）\"",
        "Opt2": "\"財務諸表\"",
        "Opt3": "\"定款\"",
        "Opt4": "\"雇用契約書\"",
        "Opt5": "\"領収書\"",
        "Opt6": "\"パンフレット\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】自社がAIをどのように、どのようなルールで使うかをステークホルダーに示す重要な宣言です。\""
    },
    {
        "ID": "\"LE-6-095\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"AIの開発プロジェクトが中断または終了した際、提供されたデータを返却または廃棄することを定める条項を何というか？\"",
        "Opt1": "\"データ返却・廃棄条項\"",
        "Opt2": "\"競業避止条項\"",
        "Opt3": "\"秘密保持条項\"",
        "Opt4": "\"譲渡条項\"",
        "Opt5": "\"解除条項\"",
        "Opt6": "\"存続条項\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】データの囲い込みや目的外利用を防ぐための、出口戦略（イグジット）における重要なルールです。\""
    },
    {
        "ID": "\"LE-6-096\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"AIが生成した「嘘の情報（もっともらしい嘘）」を、技術用語で何というか？\"",
        "Opt1": "\"ハルシネーション（Hallucination / 幻覚）\"",
        "Opt2": "\"バグ\"",
        "Opt3": "\"エラー\"",
        "Opt4": "\"ノイズ\"",
        "Opt5": "\"オーバーフィット\"",
        "Opt6": "\"アンダーフィット\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】大規模言語モデル（LLM）などで、事実に基づかない情報を生成してしまう現象を指します。\""
    },
    {
        "ID": "\"LE-6-097\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"「知的財産権」に含まれないものはどれか？\"",
        "Opt1": "\"基本的人権\"",
        "Opt2": "\"著作権\"",
        "Opt3": "\"特許権\"",
        "Opt4": "\"商標権\"",
        "Opt5": "\"実用新案権\"",
        "Opt6": "\"意匠権\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】知的財産権は「人間の知的活動によって生み出された財産」を守る権利。基本的人権は憲法上の権利です。\""
    },
    {
        "ID": "\"LE-6-098\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"個人情報保護法において、本人が「自分のデータを見せてほしい」と請求できる権利を何というか？\"",
        "Opt1": "\"開示請求権\"",
        "Opt2": "\"削除請求権\"",
        "Opt3": "\"修正請求権\"",
        "Opt4": "\"利用停止請求権\"",
        "Opt5": "\"所有権\"",
        "Opt6": "\"占有権\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】自分のデータがどのように扱われているかを確認し、透明性を確保するための基本権です。\""
    },
    {
        "ID": "\"LE-6-099\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"AIを用いた「自動採用システム」の利用について、求職者に対してAIを使用していることを事前に伝えるべきとされる理由は？\"",
        "Opt1": "\"透明性と公平性を確保するため。\"",
        "Opt2": "\"AIの方が面接官より怖いから。\"",
        "Opt3": "\"受験料を上げるため。\"",
        "Opt4": "\"PCの操作に慣れてもらうため。\"",
        "Opt5": "\"不合格にした時の言い訳にするため。\"",
        "Opt6": "\"法律で100%禁止されているから。\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】「知らないうちに機械に人生を左右される」ことを防ぎ、信頼関係を築くための倫理的要請です。\""
    },
    {
        "ID": "\"LE-6-100\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"AIの学習における「著作権法第30条の4」の規定において、利用が「制限」されるケースとして挙げられている例は？\"",
        "Opt1": "\"情報解析を目的とせず、鑑賞など（享受）を目的とする場合。\"",
        "Opt2": "\"営利企業が利用する場合。\"",
        "Opt3": "\"海外のサーバーで学習する場合。\"",
        "Opt4": "\"1",
        "Opt5": "000枚以上の画像を学習する場合。\"",
        "Opt6": "\"学習した後にモデルを消去する場合。\"",
        "Answer_Idx": "\"特定の有名人の名前を検索する場合。\"",
        "Explanation": 0
    },
    {
        "ID": "\"LE-6-101\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"2023年のG7広島サミットを機に発足した、生成AIに関する国際的な指針や行動規範を策定するための枠組みを何というか？\"",
        "Opt1": "\"G7広島AIプロセス\"",
        "Opt2": "\"OECD AI勧告\"",
        "Opt3": "\"GPAI（人工知能に関するグローバル・パートナーシップ）\"",
        "Opt4": "\"AI法（AI Act）\"",
        "Opt5": "\"トロント宣言\"",
        "Opt6": "\"アジロマ・AI原則\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】日本が議長国として主導し、生成AIのリスク管理や高度なAIシステムを開発する組織向けの国際的な行動規範などが合意されました。\""
    },
    {
        "ID": "\"LE-6-102\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"AIを含むソフトウェアに欠陥があり、それによってユーザーの「生命、身体または財産」に損害が生じた場合、製造物責任法（PL法）はどのように適用されるか？\"",
        "Opt1": "\"現状の日本のPL法では「有体物」が対象であり、ソフトウェア単体は原則対象外だが、組み込まれた機器の欠陥として責任を問われる可能性がある。\"",
        "Opt2": "\"ソフトウェアは常に「物」とみなされるため、アプリ単体でもPL法の対象となる。\"",
        "Opt3": "\"AIが書いたコードであれば、PL法は一切適用されない。\"",
        "Opt4": "\"損害額が1億円を超えない限り、PL法は適用されない。\"",
        "Opt5": "\"製造者が海外企業であれば、日本のPL法は適用されない。\"",
        "Opt6": "\"PL法はAI時代に合わせて廃止された。\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】日本のPL法における「製造物」は「製造又は加工された動産（有体物）」を指します。無形物であるソフトウェア単体での適用は議論が続いていますが、ハードウェアと一体化した製品（自動運転車やロボット）では適用対象となります。\""
    },
    {
        "ID": "\"LE-6-103\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"AIが過去の採用データから「特定の大学出身者」を優遇するように学習してしまった。これはどの倫理的原則に対する侵害か？\"",
        "Opt1": "\"公平性（Fairness）\"",
        "Opt2": "\"ロバスト性（Robustness）\"",
        "Opt3": "\"説明可能性（Explainability）\"",
        "Opt4": "\"プライバシー\"",
        "Opt5": "\"持続可能性\"",
        "Opt6": "\"収益性\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】過去のデータに含まれる社会的偏見をAIが再生産してしまう「アルゴリズム・バイアス」の問題であり、機会の平等を損なうため公平性の欠如とみなされます。\""
    },
    {
        "ID": "\"LE-6-104\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"科学技術の社会実装において、倫理的（Ethical）、法的（Legal）、社会的（Social）な課題を総称して何というか？\"",
        "Opt1": "\"ELSI（エルシー）\"",
        "Opt2": "\"SDGs\"",
        "Opt3": "\"ESG\"",
        "Opt4": "\"KPI\"",
        "Opt5": "\"BCP\"",
        "Opt6": "\"DX\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】AIなどの新技術が社会に浸透する際に生じる、技術以外の多角的な課題を指す言葉として、研究開発の初期段階から考慮することが求められています。\""
    },
    {
        "ID": "\"LE-6-105\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"大規模言語モデル（LLM）の学習データに、他人の氏名や住所などの個人情報が含まれていた場合、個人情報保護法における「利用目的の特定」との関係で留意すべき点は？\"",
        "Opt1": "\"あらかじめ個人情報の収集を目的としていない場合でも、学習・利用の段階で目的外利用にならないよう配慮が必要。\"",
        "Opt2": "\"インターネット上の情報であれば、個人情報保護法は適用されない。\"",
        "Opt3": "\"AIの学習であれば、いかなる個人情報も自由に使ってよいという特例がある。\"",
        "Opt4": "\"10年以上前の古い情報であれば、通知なしで利用できる。\"",
        "Opt5": "\"実名が含まれていなければ、住所や電話番号は個人情報ではない。\"",
        "Opt6": "\"学術研究目的であれば、営利企業の利用でも常に免除される。\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】スクレイピング等で意図せず個人情報を取得した場合でも、それをAIモデルの一部として保持・利用する際には、適切な安全管理措置や利用目的の範囲内での運用が求められます。\""
    },
    {
        "ID": "\"LE-6-106\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"プラットフォーマーが自社のAIアルゴリズムを操作して、自社サービスを他社よりも有利に表示させる行為（自己優遇）が抵触する可能性が高い法律は？\"",
        "Opt1": "\"独占禁止法\"",
        "Opt2": "\"著作権法\"",
        "Opt3": "\"特許法\"",
        "Opt4": "\"景品表示法\"",
        "Opt5": "\"労働基準法\"",
        "Opt6": "\"不動産登記法\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】市場における支配的な地位を利用して公正な競争を妨げる行為は、独占禁止法（優越的地位の濫用など）によって規制される対象となります。\""
    },
    {
        "ID": "\"LE-6-107\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"AIを導入した企業が、従業員のキーボード入力や視線をAIで常時監視し、生産性を評価する場合に最も配慮すべき権利は？\"",
        "Opt1": "\"プライバシー権および労働者の人格権\"",
        "Opt2": "\"特許権\"",
        "Opt3": "\"商標権\"",
        "Opt4": "\"著作権\"",
        "Opt5": "\"団結権\"",
        "Opt6": "\"参政権\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】過度な監視は労働者の精神的負担となり、人格権の侵害やプライバシーの逸脱とみなされるリスクがあるため、事前の説明や同意、必要最小限の範囲での運用が求められます。\""
    },
    {
        "ID": "\"LE-6-108\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"AIが生成した文章をそのまま「自分が書いた」と偽って発表した場合、著作権法上のどの権利に抵触する可能性があるか？\"",
        "Opt1": "\"著作者人格権（氏名表示権）\"",
        "Opt2": "\"複製権\"",
        "Opt3": "\"翻案権\"",
        "Opt4": "\"公衆送信権\"",
        "Opt5": "\"展示権\"",
        "Opt6": "\"譲渡権\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】AI生成物に人間が創作的寄与をしていない場合、そもそも著作権は発生しませんが、あたかも自分が作者であると偽る行為は、道義的な問題に加え、著作者名義の詐称に関わる論点となります。\""
    },
    {
        "ID": "\"LE-6-109\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"自動運転車が「歩行者を避ければ乗員が死亡し、そのまま進めば歩行者が死亡する」という回避不能な状況に陥った際の倫理的ジレンマを何というか？\"",
        "Opt1": "\"トロッコ問題（トロリー問題）\"",
        "Opt2": "\"囚人のジレンマ\"",
        "Opt3": "\"アビリーンのパラドックス\"",
        "Opt4": "\"共有地の悲劇\"",
        "Opt5": "\"ハルシネーション\"",
        "Opt6": "\"モード崩壊\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】AIに生命の価値判断をあらかじめプログラムすることの是非を問う、AI倫理における有名な思考実験です。\""
    },
    {
        "ID": "\"LE-6-110\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"AIの開発において、企画段階からプライバシーやセキュリティのリスクを想定し、その対策をシステム設計に組み込む考え方を何というか？\"",
        "Opt1": "\"Privacy by Design / Security by Design\"",
        "Opt2": "\"After the fact（事後対応）\"",
        "Opt3": "\"Black box design\"",
        "Opt4": "\"Profit first design\"",
        "Opt5": "\"Move fast and break things\"",
        "Opt6": "\"Cloud native\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】後から修正するのではなく、最初から「守り」を設計に組み込むことで、手戻りを防ぎ、信頼性の高いシステムを構築する手法です。\""
    },
    {
        "ID": "\"LE-6-111\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"AIが生成した架空の人物の顔写真を、あたかも実在する「お客様の声」として広告に使用した場合、法的・倫理的にどのような問題があるか？\"",
        "Opt1": "\"景品表示法（優良誤認）および消費者の信頼を裏切る倫理的問題。\"",
        "Opt2": "\"著作権法違反（AIに著作権があるため）。\"",
        "Opt3": "\"肖像権侵害（実在しない人物のため発生しないが）。\"",
        "Opt4": "\"特許法違反。\"",
        "Opt5": "\"商標法違反。\"",
        "Opt6": "\"特に問題はない。\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】実在しない実績や評価を捏造して表示することは、消費者を欺く行為（不当表示）に該当する可能性が非常に高いです。\""
    },
    {
        "ID": "\"LE-6-112\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"企業がAIを導入する際、そのAIの挙動によって生じ得るリスクを特定・評価し、適切な管理策を講じるプロセスを何というか？\"",
        "Opt1": "\"AIリスクアセスメント（AIガバナンス）\"",
        "Opt2": "\"AIベンチマーク\"",
        "Opt3": "\"AIマーケティング\"",
        "Opt4": "\"AIプログラミング\"",
        "Opt5": "\"AIハッカソン\"",
        "Opt6": "\"AIコンサルティング\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】企業としての社会的責任（CSR）を果たし、法的リスクを回避するために、AIのライフサイクル全体でリスクを管理する体制が必要です。\""
    },
    {
        "ID": "\"LE-6-113\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"「プロバイダ責任制限法」において、AIによる自動投稿が他者の名誉を毀損した場合、投稿の削除（送信防止措置）を求めることができる相手は？\"",
        "Opt1": "\"ウェブサイトの管理者や掲示板運営者（プロバイダ）。\"",
        "Opt2": "\"AIの開発言語（Pythonなど）のコミュニティ。\"",
        "Opt3": "\"サーバーの電気代を支払っている会社。\"",
        "Opt4": "\"PCの製造メーカー。\"",
        "Opt5": "\"インターネットエクスチェンジ（IX）。\"",
        "Opt6": "\"電力会社。\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】情報の流通を管理しているプラットフォームやサイト運営者に対して、削除依頼や発信者情報の開示請求を行うことができます。\""
    },
    {
        "ID": "\"LE-6-114\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"AIが作成した翻訳物について、人間が内容を確認し、大幅に修正・加筆して完成させた場合、その完成物の著作権はどうなるか？\"",
        "Opt1": "\"修正・加筆した人間に著作権が発生する。\"",
        "Opt2": "\"AIの提供会社に著作権が帰属する。\"",
        "Opt3": "\"著作権は発生せず、パブリックドメインとなる。\"",
        "Opt4": "\"最初にプロンプトを入力した人間のみが権利を持つ。\"",
        "Opt5": "\"翻訳元の著作者にすべての権利が移転する。\"",
        "Opt6": "\"修正した人間とAIの共同著作物となる。\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】人間が「創作的寄与」を行った部分は、その人間の著作物として保護されます。AIはあくまで補助道具（ツール）としての扱いです。\""
    },
    {
        "ID": "\"LE-6-115\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"ディープラーニングモデルを軽量化するために、精度への影響が少ない重みを削除する「剪定（プルーニング）」を行う際、ライセンス面で注意すべき点は？\"",
        "Opt1": "\"元のモデルのライセンス（MIT",
        "Opt2": "Apache等）が継承されるかどうか。\"",
        "Opt3": "\"剪定した後は常に「自作」と言い張ってよい。\"",
        "Opt4": "\"剪定作業自体に特許が必要である。\"",
        "Opt5": "\"剪定したモデルは個人情報保護法の対象外になる。\"",
        "Opt6": "\"剪定すると著作権が消滅する。\"",
        "Answer_Idx": "\"剪定後は商標登録が必須である。\"",
        "Explanation": 0
    },
    {
        "ID": "\"LE-6-116\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"2022年の日本の個人情報保護法改正で導入された、本人の権利利益を保護しつつデータの利活用を促進するための「仮名加工情報」の利用目的として、認められていないものはどれか？\"",
        "Opt1": "\"本人へのダイレクトメールの送付や、個別のサービス勧誘。\"",
        "Opt2": "\"社内における新商品の研究開発。\"",
        "Opt3": "\"特定の疾患に関するデータの統計分析。\"",
        "Opt4": "\"機械学習モデルの精度向上。\"",
        "Opt5": "\"不正アクセスのパターン検知。\"",
        "Opt6": "\"社内の業務効率化のためのデータ分析。\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】仮名加工情報は「特定の個人を識別すること」を目的とした利用（連絡や勧誘など）が厳格に禁止されています。\""
    },
    {
        "ID": "\"LE-6-117\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"AIによる「信用スコアリング」において、居住地域や人種を判定基準に含めた結果、特定グループの融資審査が通りにくくなる現象を何というか？\"",
        "Opt1": "\"不当な差別（ディスパレート・インパクト）\"",
        "Opt2": "\"モデルの汎化\"",
        "Opt3": "\"データの圧縮\"",
        "Opt4": "\"精度のパレート最適\"",
        "Opt5": "\"次元の削減\"",
        "Opt6": "\"ラベリングの効率化\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】意図的な差別でなくても、結果として特定の属性に不利益が生じることを指し、公平性の観点から強く批判される対象となります。\""
    },
    {
        "ID": "\"LE-6-118\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"AI開発における「著作権」の議論で、AIが学習を通じて「特定の画家の作風」を完全に模倣できるようになった場合、現行法での解釈は？\"",
        "Opt1": "\"「作風（アイデア）」そのものは著作権の保護対象ではないため、直ちに侵害とは言えない。\"",
        "Opt2": "\"作風を真似ることは常に著作権侵害である。\"",
        "Opt3": "\"模倣された画家は、AI会社から自動的にロイヤリティを受け取れる。\"",
        "Opt4": "\"作風を真似るAIの開発は法律で全面禁止されている。\"",
        "Opt5": "\"10作品以上真似たら侵害、それ以下なら合法である。\"",
        "Opt6": "\"AIが学習した時点で、画家の著作権は消滅する。\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】著作権は「具体的な表現」を保護するものであり、筆致や色使いといった「作風・アイデア」自体は保護されないのが原則です。ただし、類似性が高ければ侵害のリスクは残ります。\""
    },
    {
        "ID": "\"LE-6-119\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"「AI利活用ガイドライン」において、AIの判断によって人命に関わる重大な決定がなされる際、AIに全権を委ねるのではなく人間が最終確認することを何というか？\"",
        "Opt1": "\"Human-in-the-Loop（HITL）\"",
        "Opt2": "\"Automatic bypass\"",
        "Opt3": "\"Machine first\"",
        "Opt4": "\"Error rejection\"",
        "Opt5": "\"Full automation\"",
        "Opt6": "\"Human replacement\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】「人間が輪の中にいる（介入する）」という意味で、AIの暴走や誤判断を防ぐための重要なガバナンスの考え方です。\""
    },
    {
        "ID": "\"LE-6-120\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"AIによる意思決定の透明性を確保するために、モデルの予測根拠を可視化する技術（LIMEやSHAPなど）を総称して何というか？\"",
        "Opt1": "\"XAI（説明可能なAI）\"",
        "Opt2": "\"AGI（汎用人工知能）\"",
        "Opt3": "\"NLP（自然言語処理）\"",
        "Opt4": "\"GAN（敵対的生成ネットワーク）\"",
        "Opt5": "\"CNN（畳み込みニューラルネットワーク）\"",
        "Opt6": "\"RNN（回帰型ニューラルネットワーク）\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】Explainable AIの略。ブラックボックス化したAIの判断理由を人間にわかる形で示す技術や取り組みを指します。\""
    },
    {
        "ID": "\"LE-6-121\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"日本の不正競争防止法において、他人のWebサイトからデータを収集（スクレイピング）して自社のサービスで利用する際、違法となる可能性があるケースは？\"",
        "Opt1": "\"サーバーに過大な負荷をかけて業務を妨害したり、ID・パスワードによる管理を回避して取得した場合。\"",
        "Opt2": "\"いかなるスクレイピングも合法である。\"",
        "Opt3": "\"1秒に1回以上のアクセスをしたら常に死刑である。\"",
        "Opt4": "\"画像を取得したら違法だが、数値なら問題ない。\"",
        "Opt5": "\"ブラウザで表示できる情報なら、どのように取得しても自由である。\"",
        "Opt6": "\"データをUSBメモリに保存してはいけない。\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】データの取得自体は著作権法上（30条の4）で許容されやすいですが、アクセス方法がサーバー攻撃とみなされたり、管理措置を突破したりすると不正競争防止法や業務妨害罪に触れる可能性があります。\""
    },
    {
        "ID": "\"LE-6-122\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"AIが生成したニュース記事が、特定の人物に関する虚偽の内容であった場合、成立する可能性がある罪は？\"",
        "Opt1": "\"名誉毀損罪（または侮辱罪）\"",
        "Opt2": "\"建造物侵入罪\"",
        "Opt3": "\"強盗罪\"",
        "Opt4": "\"器物損壊罪\"",
        "Opt5": "\"公文書偽造罪\"",
        "Opt6": "\"スピード違反\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】AIが作ったものであっても、それを公開・拡散した人間が、他人の社会的評価を下げる虚偽の事実を広めた責任を問われます。\""
    },
    {
        "ID": "\"LE-6-123\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"個人情報保護法において、本人の同意を得ずに個人データを海外の第三者に提供する場合に必要となる、相手国に関する要件は？\"",
        "Opt1": "\"欧州委員会による十分性認定を受けているか、日本の基準と同等の保護体制（SCC等）を整えていること。\"",
        "Opt2": "\"相手国のGDPが日本より高いこと。\"",
        "Opt3": "\"相手国に日本語を話せる担当者がいること。\"",
        "Opt4": "\"相手国が島国であること。\"",
        "Opt5": "\"相手国の首相が承認していること。\"",
        "Opt6": "\"特に要件はなく、自由に提供できる。\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】日本の個人情報保護水準と同等の保護が受けられない国への持ち出しは厳しく制限されています（十分性認定はEUなどが該当）。\""
    },
    {
        "ID": "\"LE-6-124\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"AIの学習データを集めるために、クラウドソーシングサイトで「1枚1円」などの極端に低い報酬で労働者に大量のタグ付けをさせることに対する倫理的懸念は？\"",
        "Opt1": "\"デジタル・スウェットショップ（デジタル搾取工場）問題\"",
        "Opt2": "\"データの過学習問題\"",
        "Opt3": "\"サーバーの電気代問題\"",
        "Opt4": "\"通信の遅延問題\"",
        "Opt5": "\"AIの著作権問題\"",
        "Opt6": "\"OSのアップデート問題\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】新興国などの労働力を安価に買い叩く構造が、現代の搾取であるとしてELSIの観点から問題視されています。\""
    },
    {
        "ID": "\"LE-125\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"AIのガバナンスにおいて、企業が「うちは倫理的なAIを使っています」とアピールするだけで実態が伴っていない状態を揶揄する言葉は？\"",
        "Opt1": "\"倫理ウォッシング（Ethics Washing）\"",
        "Opt2": "\"グリーンウォッシング\"",
        "Opt3": "\"ホワイトウォッシング\"",
        "Opt4": "\"ブルーウォッシング\"",
        "Opt5": "\"マネーロンダリング\"",
        "Opt6": "\"ブラッシング\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】環境配慮を偽装する「グリーンウォッシング」になぞらえた言葉で、実効性のない倫理指針の公表を批判する際に使われます。\""
    },
    {
        "ID": "\"LE-6-126\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"AIが自律的に学習し、当初の開発目的とは異なる「有害な挙動」を始めた場合、開発者が責任を問われる可能性がある法理は？\"",
        "Opt1": "\"製造物責任（PL法）や不法行為責任\"",
        "Opt2": "\"一事不再理\"",
        "Opt3": "\"罪刑法定主義\"",
        "Opt4": "\"契約自由の原則\"",
        "Opt5": "\"善管注意義務の免除\"",
        "Opt6": "\"自己責任の原則\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】開発段階で予測・回避可能だったリスクに対して対策を怠った場合、過失責任やPL法上の欠陥として責任を問われる可能性があります。\""
    },
    {
        "ID": "\"LE-6-127\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"生成AIがユーザーの入力（プロンプト）に応じて機密情報を出力してしまった。この「情報の漏洩」を防ぐために、入力されたデータを学習に利用しない設定を何というか？\"",
        "Opt1": "\"オプトアウト設定（またはAPIの非利用学習設定）\"",
        "Opt2": "\"オプトイン設定\"",
        "Opt3": "\"リセット設定\"",
        "Opt4": "\"初期化\"",
        "Opt5": "\"フォーマット\"",
        "Opt6": "\"デリート\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】企業の機密情報や個人情報がAIの再学習に使われ、他者への回答に混入するのを防ぐために必須のセキュリティ対策です。\""
    },
    {
        "ID": "\"LE-6-128\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"AIによる自動運転において、事故の瞬間のデータを記録し、原因究明を可能にする装置を何というか？\"",
        "Opt1": "\"イベント・データ・レコーダー（EDR）\"",
        "Opt2": "\"ドライブレコーダー（映像のみ）\"",
        "Opt3": "\"フライトレコーダー（飛行機用）\"",
        "Opt4": "\"スピードメーター\"",
        "Opt5": "\"タコグラフ\"",
        "Opt6": "\"GPS\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】事故前後の速度、ブレーキ、自動運転の状態などを詳細に記録することで、法的責任の所在を明らかにするために重要です。\""
    },
    {
        "ID": "\"LE-6-129\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"「AI倫理指針」において、AIが特定の文化や宗教に対して不適切な回答をしないよう、あらかじめフィルタリングや調整を行うことを何というか？\"",
        "Opt1": "\"ガードレール（またはセーフティ・アライメント）\"",
        "Opt2": "\"オーバーフロー\"",
        "Opt3": "\"アンダーカット\"",
        "Opt4": "\"バックドア\"",
        "Opt5": "\"サイドステップ\"",
        "Opt6": "\"デッドロック\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】AIが社会規範に沿った安全な回答をするように「道」を外れないための柵（ガードレール）を設けるイメージです。\""
    },
    {
        "ID": "\"LE-6-130\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"AIが生成したソースコードが、既存のオープンソースソフトウェア（GPL等）と酷似しており、そのライセンス条件（ソースコードの公開義務）を無視して商用利用した場合の問題は？\"",
        "Opt1": "\"ライセンス違反（契約違反および著作権侵害）\"",
        "Opt2": "\"商標権侵害\"",
        "Opt3": "\"意匠権侵害\"",
        "Opt4": "\"不動産登記法違反\"",
        "Opt5": "\"電波法違反\"",
        "Opt6": "\"公職選挙法違反\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】OSSのライセンスには「派生物も公開せよ」といった条件がある場合があり、AIがそれを学習・出力した際もその条件が適用される可能性が高いです。\""
    },
    {
        "ID": "\"LE-6-131\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"個人情報保護法において、本人が自分の個人データの「利用停止」を請求できる条件が緩和された（2020年改正）のはなぜか？\"",
        "Opt1": "\"個人の権利をより手厚く保護し、データ利用の透明性を高めるため。\"",
        "Opt2": "\"企業の事務作業を増やすため。\"",
        "Opt3": "\"郵便局の利益を守るため。\"",
        "Opt4": "\"AIの学習を止めるため。\"",
        "Opt5": "\"サーバーを冷やすため。\"",
        "Opt6": "\"スマホの普及率を下げるため。\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】「法違反がある場合」だけでなく、「利用する必要がなくなった場合」などでも請求できるようになり、個人の自己決定権が強化されました。\""
    },
    {
        "ID": "\"LE-6-132\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"AIを用いた動画監視において、特定の人物を追跡し続ける際、その人物の許可なく「歩き方の特徴（歩容）」を記録・利用することの是非は？\"",
        "Opt1": "\"プライバシー侵害や個人情報保護法違反になる可能性があるため、適切な告知や同意が必要。\"",
        "Opt2": "\"歩き方は誰でも見られるので、自由に記録してよい。\"",
        "Opt3": "\"顔を隠していれば、歩き方は個人情報ではない。\"",
        "Opt4": "\"警察であれば、いかなる場合も無許可で利用できる。\"",
        "Opt5": "\"夜間であれば記録してもよい。\"",
        "Opt6": "\"10秒以内の動画なら問題ない。\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】顔だけでなく、歩容などの身体的特徴も個人を特定できる情報（個人識別符号等）に含まれるため、慎重な取り扱いが求められます。\""
    },
    {
        "ID": "\"LE-6-133\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"AIモデルをAPI経由で提供する際、SLA（サービス品質保証）で定めるべき主な項目は？\"",
        "Opt1": "\"稼働率（アップタイム）や応答速度。\"",
        "Opt2": "\"開発者の氏名。\"",
        "Opt3": "\"AIの好きな色。\"",
        "Opt4": "\"物理的なサーバーの重量。\"",
        "Opt5": "\"オフィスの広さ。\"",
        "Opt6": "\"会社の株価。\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】ビジネス利用において、AIが常に安定して使えること（可用性）を保証することは極めて重要です。\""
    },
    {
        "ID": "\"LE-6-134\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"AIの開発において、開発者が自分の意図を反映させてしまい、結果としてAIが特定の方向に偏ってしまうことを何というか？\"",
        "Opt1": "\"設計者のバイアス（認知バイアス）\"",
        "Opt2": "\"過学習\"",
        "Opt3": "\"勾配爆発\"",
        "Opt4": "\"モード崩壊\"",
        "Opt5": "\"データの不均衡\"",
        "Opt6": "\"通信エラー\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】データだけでなく、開発者が「何を重要とするか」を決める際にも主観が入り込み、AIの判断に影響を与えることがあります。\""
    },
    {
        "ID": "\"LE-6-135\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"AIによる「自動ニュース作成」において、誤った医療情報を生成し、それを信じた読者が健康を害した場合、誰が法的責任を負う可能性があるか？\"",
        "Opt1": "\"そのAIを運用・公開した組織や、内容の確認を怠った編集責任者。\"",
        "Opt2": "\"AIに使われている数学（線形代数）の考案者。\"",
        "Opt3": "\"電気通信事業者。\"",
        "Opt4": "\"ブラウザの開発元。\"",
        "Opt5": "\"キーボードのメーカー。\"",
        "Opt6": "\"読者本人のみの責任。\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】情報を発信する主体（媒体社など）には、内容の正確性を確認する注意義務があり、それを怠れば過失責任を問われる可能性があります。\""
    },
    {
        "ID": "\"LE-6-136\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"AIが生成した「音楽」が、特定の有名アーティストの歌声と酷似している場合、現行の日本の議論で侵害の可能性があるのは？\"",
        "Opt1": "\"パブリシティ権（または人格権的な利益）\"",
        "Opt2": "\"特許権\"",
        "Opt3": "\"商標権\"",
        "Opt4": "\"実用新案権\"",
        "Opt5": "\"意匠権\"",
        "Opt6": "\"土地収用権\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】著作権では「声」そのものは保護されませんが、有名人の顧客吸引力を利用する行為としてパブリシティ権の観点から問題視されることがあります。\""
    },
    {
        "ID": "\"LE-6-137\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"EUの「AI法（AI Act）」が定める『高リスクAI』に該当し、厳しい管理が求められる分野はどれか？\"",
        "Opt1": "\"医療機器、教育、求人、法執行（犯罪捜査）など。\"",
        "Opt2": "\"ビデオゲームの背景描画。\"",
        "Opt3": "\"スパムメールの自動振り分け。\"",
        "Opt4": "\"個人の日記作成支援。\"",
        "Opt5": "\"天気予報の解析。\"",
        "Opt6": "\"ペットの自動給餌器。\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】人権や安全に重大な影響を与える可能性がある分野は「高リスク」と定義され、技術文書の作成や適合性評価が義務付けられます。\""
    },
    {
        "ID": "\"LE-6-138\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"AIの「説明責任（Accountability）」を果たすために、AIの学習に使用したデータセットの構成や、性能テストの結果などをまとめた文書を何というか？\"",
        "Opt1": "\"モデルカード（Model Cards）\"",
        "Opt2": "\"年賀状\"",
        "Opt3": "\"企画書\"",
        "Opt4": "\"領収書\"",
        "Opt5": "\"誓約書\"",
        "Opt6": "\"履歴書\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】Googleなどが提唱した形式で、モデルの特性や制限事項を透明化し、利用者が適切に判断できるようにするための文書です。\""
    },
    {
        "ID": "\"LE-6-139\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"AIの学習における「著作権法第30条の4」の例外として、「著作権者の利益を不当に害することとなる場合」に該当する可能性が高い行為は？\"",
        "Opt1": "\"情報解析用として販売されているデータベースを、AI学習のためにコピーして使用する。\"",
        "Opt2": "\"ネット上の画像を100万枚学習する。\"",
        "Opt3": "\"営利目的で風景写真を学習する。\"",
        "Opt4": "\"10年前の新聞記事を学習する。\"",
        "Opt5": "\"外国の小説を学習する。\"",
        "Opt6": "\"自分の飼い猫の写真を学習する。\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】著作権者が「データセットとして売る」ことで利益を得ている市場を奪う行為は、権利者の利益を不当に害するとみなされます。\""
    },
    {
        "ID": "\"LE-6-140\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"AIを用いた「予測ポリシング（犯罪予測）」において、特定の地域に警察官を集中配備した結果、その地域の逮捕者が増え、さらにAIが「その地域は危険だ」と学習を強めてしまう悪循環を何というか？\"",
        "Opt1": "\"フィードバックループ（自己強化ループ）\"",
        "Opt2": "\"正則化\"",
        "Opt3": "\"ドロップアウト\"",
        "Opt4": "\"バッチ正規化\"",
        "Opt5": "\"早期終了\"",
        "Opt6": "\"アンサンブル学習\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】AIの出力が将来のデータに影響を与え、偏見が固定化・増幅されてしまう現象を指します。\""
    },
    {
        "ID": "\"LE-6-141\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"AIシステムの利用規約において、AIが生成した内容によって発生した損害について、提供側が一切の責任を負わないとする条項を何というか？\"",
        "Opt1": "\"免責条項\"",
        "Opt2": "\"守秘義務条項\"",
        "Opt3": "\"準拠法条項\"",
        "Opt4": "\"誠実協議条項\"",
        "Opt5": "\"契約解除条項\"",
        "Opt6": "\"有効期間条項\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】提供側がリスクを回避するための条項ですが、消費者契約法などにより、重大な過失がある場合にまで免責されるとは限りません。\""
    },
    {
        "ID": "\"LE-6-142\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"AIの「ブラックボックス問題」を解決するために、モデルの入力データを少しだけ変えてみて、出力がどう変わるかを観察し、判断の境界線を探る手法は？\"",
        "Opt1": "\"感度分析（サロゲートモデルの構築等）\"",
        "Opt2": "\"一括学習\"",
        "Opt3": "\"バックプロパゲーション\"",
        "Opt4": "\"活性化関数\"",
        "Opt5": "\"プーリング\"",
        "Opt6": "\"パディング\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】中身がわからなくても、外側からの反応を見ることで「AIが何を重視しているか」を推測するアプローチです。\""
    },
    {
        "ID": "\"LE-6-143\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"「知的財産推進計画」など日本政府の施策において、AI生成物の権利関係を検討する際に重視されているバランスは？\"",
        "Opt1": "\"クリエイターの権利保護と、AI開発の促進の両立。\"",
        "Opt2": "\"AI開発を完全に禁止すること。\"",
        "Opt3": "\"著作権をすべて廃止すること。\"",
        "Opt4": "\"すべてのAI生成物を国営にすること。\"",
        "Opt5": "\"海外のAIをすべて排除すること。\"",
        "Opt6": "\"紙の書籍の出版を止めること。\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】文化の発展（クリエイター保護）と産業の発展（AI活用）のどちらも損なわないような、柔軟な法整備やガイドライン作りが進められています。\""
    },
    {
        "ID": "\"LE-6-144\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"AIの開発・利用において、特定の企業が市場を独占し、不当に高い価格を設定したり他社の参入を妨げたりすることを監視する機関は？\"",
        "Opt1": "\"公正取引委員会\"",
        "Opt2": "\"文化庁\"",
        "Opt3": "\"気象庁\"",
        "Opt4": "\"観光庁\"",
        "Opt5": "\"消防庁\"",
        "Opt6": "\"スポーツ庁\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】独占禁止法の番人として、巨大IT企業（プラットフォーマー）による市場の歪みを監視・是正する役割を担います。\""
    },
    {
        "ID": "\"LE-6-145\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"個人情報保護法において、本人が「自分の個人データを別のサービスに移行させてほしい」と請求できる権利（欧州のGDPRで明文化）を何というか？\"",
        "Opt1": "\"データポータビリティ権\"",
        "Opt2": "\"開示請求権\"",
        "Opt3": "\"削除請求権\"",
        "Opt4": "\"修正請求権\"",
        "Opt5": "\"占有権\"",
        "Opt6": "\"所有権\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】特定のプラットフォームへのデータの囲い込みを防ぎ、ユーザーが自由にサービスを選べるようにするための権利です。\""
    },
    {
        "ID": "\"LE-6-146\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"AIの開発受託契約において、完成したAIモデルが目標の精度に達しなかった場合でも、受託側が責任を問われないように結ぶ形式は？\"",
        "Opt1": "\"準委任契約（善管注意義務を負うが完成義務はない）\"",
        "Opt2": "\"請負契約（完成させる義務がある）\"",
        "Opt3": "\"贈与契約\"",
        "Opt4": "\"売買契約\"",
        "Opt5": "\"賃貸借契約\"",
        "Opt6": "\"質権設定契約\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】AI開発はやってみないとわからない不確実性が高いため、特定の作業を行うことを約束する「準委任契約」が選ばれることが多いです。\""
    },
    {
        "ID": "\"LE-6-147\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"AIガバナンスにおける「ステークホルダー」に含まれるのは誰か？\"",
        "Opt1": "\"開発者、利用者、提供者、規制当局、および影響を受ける一般市民すべて。\"",
        "Opt2": "\"AI開発者のみ。\"",
        "Opt3": "\"株主のみ。\"",
        "Opt4": "\"総理大臣のみ。\"",
        "Opt5": "\"電気代の支払い担当者のみ。\"",
        "Opt6": "\"AIというプログラム自体。\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】AIは社会全体に影響を与えるため、直接の契約関係にない市民や社会全体をステークホルダー（利害関係者）として捉える必要があります。\""
    },
    {
        "ID": "\"LE-6-148\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"AIが生成した「偽の動画」で、政治家の発言を捏造し選挙に影響を与えようとする行為に対する、最も直接的な対策技術は？\"",
        "Opt1": "\"真正性証明（デジタル署名やオリジネーター・プロファイル）\"",
        "Opt2": "\"暗号化\"",
        "Opt3": "\"データ圧縮\"",
        "Opt4": "\"フォーマット変換\"",
        "Opt5": "\"ノイズ除去\"",
        "Opt6": "\"画面の明るさ調整\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】コンテンツが「本物（信頼できる発信元）」であることを証明する技術を普及させることが、偽情報対策の柱となっています。\""
    },
    {
        "ID": "\"LE-6-149\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"AIを用いた「チャットボット」が、ユーザーを自殺や犯罪に誘導するような回答をした場合、企業が負うべき責任の根拠は？\"",
        "Opt1": "\"安全配慮義務や説明責任（ガバナンスの欠如）\"",
        "Opt2": "\"製造物責任（物理的損害がない場合は難しいが議論がある）\"",
        "Opt3": "\"不当利得返還義務\"",
        "Opt4": "\"善意無過失の原則\"",
        "Opt5": "\"時効の援用\"",
        "Opt6": "\"信義誠実の原則の逸脱\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】ユーザーに危害が及ばないよう、AIを適切に制御・監視する責任（安全配慮義務等）が運営企業には求められます。\""
    },
    {
        "ID": "\"LE-6-150\"",
        "Category": "\"6.社会実装\"",
        "Question": "\"G検定の合格に向けた学習を通じて、最も重要となる「社会実装」のスタンスは？\"",
        "Opt1": "\"技術の進歩を理解しつつ、倫理・法律・社会との調和を常に考えること。\"",
        "Opt2": "\"法律を無視して技術開発を優先すること。\"",
        "Opt3": "\"AIを一切使わないこと。\"",
        "Opt4": "\"すべてのAI開発を国に任せること。\"",
        "Opt5": "\"他人のデータをすべて無料で奪うこと。\"",
        "Opt6": "\"自分の利益だけを追求すること。\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】技術的な知識（ディープラーニングの手法）と、それをどう社会に正しく適用するか（社会実装）の両輪が揃って初めて、AIの真の価値が発揮されます。\""
    },
    {
        "ID": "\"7-001\"",
        "Category": "\"数理・統計\"",
        "Question": "\"行列 $A$ とベクトル $x$ に対して $Ax = \\lambda x$ となる $\\lambda$ を何と呼ぶか？\"",
        "Opt1": "\"固有値\"",
        "Opt2": "\"固有ベクトル\"",
        "Opt3": "\"行列式\"",
        "Opt4": "\"逆行列\"",
        "Opt5": "\"転置行列\"",
        "Opt6": "\"トレース\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】$Ax = \\lambda x$ において、スカラー $\\lambda$ を固有値、ベクトル $x$ を固有ベクトルと呼びます。ディープラーニングの主成分分析（PCA）などで重要な概念です。\""
    },
    {
        "ID": "\"7-002\"",
        "Category": "\"数理・統計\"",
        "Question": "\"線形代数において、行列 $A$ とその転置行列 $A^T$ が $AA^T = A^T A = I$ （$I$は単位行列）を満たすとき、行列 $A$ を何と呼ぶか？\"",
        "Opt1": "\"直交行列\"",
        "Opt2": "\"正則行列\"",
        "Opt3": "\"対角行列\"",
        "Opt4": "\"対称行列\"",
        "Opt5": "\"三角行列\"",
        "Opt6": "\"零行列\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】逆行列が転置行列と一致するような行列を直交行列と呼びます。直交行列による変換は、ベクトルの長さを変えない性質があり、ニューラルネットワークの重みの初期化などで議論されます。\""
    },
    {
        "ID": "\"7-003\"",
        "Category": "\"数理・統計\"",
        "Question": "\"確率論において、事象 $A$ が発生したという条件の下で事象 $B$ が発生する確率 $P(B|A)$ を求める際に用いられる定理は？\"",
        "Opt1": "\"ベイズの定理\"",
        "Opt2": "\"中心極限定理\"",
        "Opt3": "\"大数の法則\"",
        "Opt4": "\"三平方の定理\"",
        "Opt5": "\"平均値の定理\"",
        "Opt6": "\"ガウスの定理\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】$P(B|A) = \\frac{P(A|B)P(B)}{P(A)}$ で表されるベイズの定理は、AIのベイズ分類器などの基礎となる最重要項目です。\""
    },
    {
        "ID": "\"7-004\"",
        "Category": "\"数理・統計\"",
        "Question": "\"統計学において、データの散らばり具合を表す指標のうち、単位が元のデータと同じになるものはどれか？\"",
        "Opt1": "\"標準偏差\"",
        "Opt2": "\"分散\"",
        "Opt3": "\"共分散\"",
        "Opt4": "\"相関係数\"",
        "Opt5": "\"平均値\"",
        "Opt6": "\"中央値\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】分散は偏差の「2乗」の平均であるため単位が2乗になりますが、その平方根である「標準偏差」は元のデータと単位が一致するため、解釈が容易になります。\""
    },
    {
        "ID": "\"7-005\"",
        "Category": "\"数理・統計\"",
        "Question": "\"2つの変数 $X$ と $Y$ の関係について、相関係数が 0 に近い場合に断定できることは？\"",
        "Opt1": "\"直線的な相関関係がない。\"",
        "Opt2": "\"因果関係がない。\"",
        "Opt3": "\"非線形な関係も存在しない。\"",
        "Opt4": "\"データが完全にランダムである。\"",
        "Opt5": "\"一方が増えるともう一方が必ず減る。\"",
        "Opt6": "\"$X$ と $Y$ は独立である。\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】相関係数はあくまで「直線的な関連性」を示す指標です。0に近い場合でも、曲線的な関係（2次関数的など）が存在する可能性があるため、「独立である」や「関係がない」と断定するのはひっかけです。\""
    },
    {
        "ID": "\"7-006\"",
        "Category": "\"数理・統計\"",
        "Question": "\"多変量解析において、説明変数同士に強い相関がある場合に、回帰係数の推定が不安定になる現象を何というか？\"",
        "Opt1": "\"多重共線性（マルチコ）\"",
        "Opt2": "\"過学習\"",
        "Opt3": "\"勾配消失\"",
        "Opt4": "\"次元の呪い\"",
        "Opt5": "\"局所最適解\"",
        "Opt6": "\"疑似相関\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】Multicollinearity（マルチコリニアリティ）と呼ばれます。重回帰分析を行う際、相関の強い変数を同時に入れると正しく推定できなくなるため、変数選択や主成分分析による圧縮が必要です。\""
    },
    {
        "ID": "\"7-007\"",
        "Category": "\"数理・統計\"",
        "Question": "\"標準正規分布において、平均値から標準偏差の2倍（$2\\sigma$）の範囲内に含まれるデータの割合はおよそどのくらいか？\"",
        "Opt1": "\"約95%\"",
        "Opt2": "\"約68%\"",
        "Opt3": "\"約50%\"",
        "Opt4": "\"約99%\"",
        "Opt5": "\"約32%\"",
        "Opt6": "\"約5%\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】$1\\sigma$以内が約68%、$2\\sigma$以内が約95%、$3\\sigma$以内が約99.7%となる「68-95-99.7の法則」は、異常検知などの理解に必須です。\""
    },
    {
        "ID": "\"7-008\"",
        "Category": "\"数理・統計\"",
        "Question": "\"機械学習の最適化で用いられる「勾配降下法」において、関数の傾きが 0 になる点を求めるために計算する多変数関数の微分を何というか？\"",
        "Opt1": "\"偏微分\"",
        "Opt2": "\"全微分\"",
        "Opt3": "\"積分\"",
        "Opt4": "\"行列式\"",
        "Opt5": "\"固有値分解\"",
        "Opt6": "\"特異値分解\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】特定の変数以外を定数とみなして微分する「偏微分」を用います。全ての変数に対する偏微分を並べたベクトルが「勾配（グラディエント）」です。\""
    },
    {
        "ID": "\"7-009\"",
        "Category": "\"数理・統計\"",
        "Question": "\"L1正則化（Lasso回帰）を用いた際の特徴として、正しいものはどれか？\"",
        "Opt1": "\"一部の重みが完全に 0 になりやすく、変数選択の効果がある。\"",
        "Opt2": "\"重みの2乗和をペナルティとして加える。\"",
        "Opt3": "\"重みが全体的に小さくなるが、0 にはなりにくい。\"",
        "Opt4": "\"外れ値の影響を強く受けるようになる。\"",
        "Opt5": "\"学習速度が大幅に向上する。\"",
        "Opt6": "\"必ず大域最適解に収束するようになる。\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】L1正則化は絶対値を加えるため、解が座標軸上で 0 になりやすい（スパース性）という特徴があります。2乗和を加えるのはL2正則化（Ridge）です。\""
    },
    {
        "ID": "\"7-010\"",
        "Category": "\"数理・統計\"",
        "Question": "\"「標本サイズを大きくすると、標本平均は母平均に近づく」という統計学上の法則を何というか？\"",
        "Opt1": "\"大数の法則\"",
        "Opt2": "\"中心極限定理\"",
        "Opt3": "\"ベイズの定理\"",
        "Opt4": "\"べき乗則\"",
        "Opt5": "\"パレートの法則\"",
        "Opt6": "\"ジップの法則\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】大数の法則は「平均」が近づくことを指します。一方、中心極限定理は「和（や平均）の分布が正規分布に近づく」ことを指すので、区別が必要です。\""
    },
    {
        "ID": "\"7-011\"",
        "Category": "\"数理・統計\"",
        "Question": "\"確率論において、ある期間内に平均 $\\lambda$ 回発生する事象が、$k$ 回発生する確率を表す分布はどれか？\"",
        "Opt1": "\"ポアソン分布\"",
        "Opt2": "\"ベルヌーイ分布\"",
        "Opt3": "\"二項分布\"",
        "Opt4": "\"正規分布\"",
        "Opt5": "\"幾何分布\"",
        "Opt6": "\"一様分布\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】「単位時間あたりに平均 $\\lambda$ 回起こる事象」の発生回数を表す離散確率分布をポアソン分布と呼びます。交通事故の発生件数やコールセンターへの着信数などのモデル化に使われます。\""
    },
    {
        "ID": "\"7-012\"",
        "Category": "\"数理・統計\"",
        "Question": "\"統計的仮説検定において、「実際には正しい対立仮説を棄却してしまう（誤って帰無仮説を採択してしまう）」誤りを何というか？\"",
        "Opt1": "\"第二種の過誤\"",
        "Opt2": "\"第一種の過誤\"",
        "Opt3": "\"標準誤差\"",
        "Opt4": "\"サンプリングバイアス\"",
        "Opt5": "\"過学習\"",
        "Opt6": "\"検定力\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】「真実を見逃す」誤りが第二種の過誤（$\\beta$）です。逆に「何もないのに差があると判断してしまう」誤りは第一種の過誤（$\\alpha$）と呼ばれます。G検定ではこの2つの区別が頻出です。\""
    },
    {
        "ID": "\"7-013\"",
        "Category": "\"数理・統計\"",
        "Question": "\"線形代数において、正方行列ではない任意の行列 $M$ を $U \\Sigma V^T$ の形に分解する手法を何というか？\"",
        "Opt1": "\"特異値分解（SVD）\"",
        "Opt2": "\"固有値分解\"",
        "Opt3": "\"QR分解\"",
        "Opt4": "\"LU分解\"",
        "Opt5": "\"コレスキー分解\"",
        "Opt6": "\"スペクトル分解\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】固有値分解が正方行列のみを対象とするのに対し、特異値分解は任意の形状の行列に適用可能です。次元圧縮や推薦システムなどで広く利用される非常に強力な数学的手法です。\""
    },
    {
        "ID": "\"7-014\"",
        "Category": "\"数理・統計\"",
        "Question": "\"情報理論において、事象 $x$ が発生する確率を $P(x)$ としたとき、$-\\log_2 P(x)$ で定義される値を何というか？\"",
        "Opt1": "\"自己情報量\"",
        "Opt2": "\"平均情報量（エントロピー）\"",
        "Opt3": "\"相互情報量\"",
        "Opt4": "\"KLダイバージェンス\"",
        "Opt5": "\"ジニ係数\"",
        "Opt6": "\"クロスエントロピー\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】個別の事象が持つ情報の驚き具合を表すのが「自己情報量」です。その期待値（平均）が「エントロピー」となります。定義式における対数の底が2の場合は単位がビット（bit）となります。\""
    },
    {
        "ID": "\"7-015\"",
        "Category": "\"数理・統計\"",
        "Question": "\"2つの確率分布 $P$ と $Q$ の間の「隔たり」を測定するために用いられ、機械学習の損失関数としても利用される指標は？\"",
        "Opt1": "\"KLダイバージェンス（カルバック・ライブラー情報量）\"",
        "Opt2": "\"ハミング距離\"",
        "Opt3": "\"ユークリッド距離\"",
        "Opt4": "\"マハラノビス距離\"",
        "Opt5": "\"相関係数\"",
        "Opt6": "\"平均絶対誤差\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】2つの分布がどれだけ似ているか（あるいは異なるか）を測る指標です。GANやVAEなどの生成モデル、または分類問題のクロスエントロピー誤差の理解において不可欠な概念です。\""
    },
    {
        "ID": "\"7-016\"",
        "Category": "\"数理・統計\"",
        "Question": "\"主成分分析（PCA）において、元のデータの情報の何％を保持しているかを示す指標を何というか？\"",
        "Opt1": "\"寄与率\"",
        "Opt2": "\"決定係数\"",
        "Opt3": "\"相関係数\"",
        "Opt4": "\"自由度\"",
        "Opt5": "\"有意確率（p値）\"",
        "Opt6": "\"再現率\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】各主成分がデータ全体の分散のどれくらいを説明しているかを示すのが寄与率です。第1主成分から順に足し合わせた「累積寄与率」を見て、何次元まで圧縮するかを判断します。\""
    },
    {
        "ID": "\"7-017\"",
        "Category": "\"数理・統計\"",
        "Question": "\"「ある事象が起こる確率」と「その事象が起こらない確率」の比（$\\frac{p}{1-p}$）を何というか？\"",
        "Opt1": "\"オッズ\"",
        "Opt2": "\"尤度\"",
        "Opt3": "\"事後確率\"",
        "Opt4": "\"ハザード比\"",
        "Opt5": "\"信頼区間\"",
        "Opt6": "\"期待値\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】ギャンブルやロジスティック回帰分析の文脈でよく登場します。このオッズの対数をとったものが「対数オッズ（ロジット）」であり、ロジスティック回帰の基本式となります。\""
    },
    {
        "ID": "\"7-018\"",
        "Category": "\"数理・統計\"",
        "Question": "\"正規分布において、平均と中央値と最頻値の関係について正しいものはどれか？\"",
        "Opt1": "\"3つの値がすべて一致する。\"",
        "Opt2": "\"平均 ＞ 中央値 ＞ 最頻値 となる。\"",
        "Opt3": "\"最頻値 ＞ 中央値 ＞ 平均 となる。\"",
        "Opt4": "\"中央値が常に 0 となる。\"",
        "Opt5": "\"分散の値によって3つの位置関係が変わる。\"",
        "Opt6": "\"正規分布に最頻値は存在しない。\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】正規分布は左右対称の山型分布であるため、中心に位置する平均値、中央値、最頻値がすべて一致します。これが成立しない（歪んでいる）分布との比較が重要です。\""
    },
    {
        "ID": "\"7-019\"",
        "Category": "\"数理・統計\"",
        "Question": "\"多変量解析において、複数の説明変数間の相関が高いことで回帰分析の結果が不安定になる「多重共線性」の回避策として不適切なものはどれか？\"",
        "Opt1": "\"より多くの相関の高い変数を追加する。\"",
        "Opt2": "\"相関の高い変数の一方を削除する。\"",
        "Opt3": "\"主成分分析を行い、変数を圧縮する。\"",
        "Opt4": "\"リッジ回帰（L2正則化）を用いる。\"",
        "Opt5": "\"ステップワイス法などで変数選択を行う。\"",
        "Opt6": "\"学習データ数を大幅に増やす。\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】相関の高い変数をさらに追加することは、多重共線性の問題を悪化させます。変数を減らす、あるいは正則化によって重みを抑制するのが正しいアプローチです。\""
    },
    {
        "ID": "\"7-020\"",
        "Category": "\"数理・統計\"",
        "Question": "\"連続確率変数 $X$ がある値 $x$ 以下をとる確率 $P(X \\le x)$ を関数としたものを何というか？\"",
        "Opt1": "\"累積分布関数\"",
        "Opt2": "\"確率密度関数\"",
        "Opt3": "\"確率質量関数\"",
        "Opt4": "\"一様分布関数\"",
        "Opt5": "\"シグモイド関数\"",
        "Opt6": "\"ソフトマックス関数\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】確率密度関数をマイナス無限大から $x$ まで積分したものが累積分布関数です。離散型の場合は確率質量関数に対応します。グラフの右端は必ず 1 に収束します。\""
    },
    {
        "ID": "\"7-021\"",
        "Category": "\"数理・統計\"",
        "Question": "\"行列の積（乗算）に関する性質として、一般に成り立つものはどれか？（行列 A",
        "Opt1": "B は積が定義可能な正方行列とする）\"",
        "Opt2": "\"AB = BA は必ずしも成り立たない（非可換）。\"",
        "Opt3": "\"AB = BA が常に成り立つ。\"",
        "Opt4": "\"行列の積において、結合法則（ (AB)C = A(BC) ）は成り立たない。\"",
        "Opt5": "\"行列 A が 0 でなければ、常に逆行列 A⁻¹ が存在する。\"",
        "Opt6": "\"行列 A と B の積が 0 ならば、A または B のどちらかが 0 である。\"",
        "Answer_Idx": "\"行列の積の結果は、必ず元の行列よりもランク（階数）が大きくなる。\"",
        "Explanation": 0
    },
    {
        "ID": "\"7-022\"",
        "Category": "\"数理・統計\"",
        "Question": "\"ニューラルネットワークの誤差逆伝播法（バックプロパゲーション）において、合成関数の微分を計算するために用いられる数学的原理は？\"",
        "Opt1": "\"連鎖律（チェインルール）\"",
        "Opt2": "\"平均値の定理\"",
        "Opt3": "\"ロピタルの定理\"",
        "Opt4": "\"テーラー展開\"",
        "Opt5": "\"部分積分\"",
        "Opt6": "\"ラグランジュの未定乗数法\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】複雑に重なった関数の微分を、各層の微分の積として分解して計算する仕組みが「連鎖律」です。これにより、出力層の誤差から入力層方向へ重みの修正量を効率的に計算できます。\""
    },
    {
        "ID": "\"7-023\"",
        "Category": "\"数理・統計\"",
        "Question": "\"統計学における「不偏分散」が、通常の分散（標本分散）の計算式において分母を n ではなく n-1 にする理由は？\"",
        "Opt1": "\"標本から母分散を推定する際、標本分散は母分散を過小評価する傾向があるため。\"",
        "Opt2": "\"計算を簡単にするため。\"",
        "Opt3": "\"外れ値の影響を完全に取り除くため。\"",
        "Opt4": "\"データの個数が少ない時は、n-1 にしたほうが 0 になりにくいから。\"",
        "Opt5": "\"正規分布の定義に合わせるため。\"",
        "Opt6": "\"n-1 にすることで必ず整数になるから。\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】標本から得られる情報の偏り（自由度が1減る）を補正し、期待値が母分散と一致するように調整したものが不偏分散です。G検定では「自由度」という言葉と共に頻出します。\""
    },
    {
        "ID": "\"7-024\"",
        "Category": "\"数理・統計\"",
        "Question": "\"確率分布において、平均値の周りにデータが集中しており、裾（スソ）が左右に非常に長く伸びている（尖っている）度合いを表す指標は？\"",
        "Opt1": "\"尖度（せんど）\"",
        "Opt2": "\"歪度（わいど）\"",
        "Opt3": "\"四分位範囲\"",
        "Opt4": "\"変動係数\"",
        "Opt5": "\"マハラノビス距離\"",
        "Opt6": "\"決定係数\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】分布がどれだけ尖っているかを示すのが「尖度」です。一方、分布が左右どちらに偏っているか（非対称性）を示すのが「歪度」です。正規分布の尖度は3（または定義により0）となります。\""
    },
    {
        "ID": "\"7-025\"",
        "Category": "\"数理・統計\"",
        "Question": "\"線形代数において、行列の対角要素の和（a₁₁ + a₂₂ + ... + aₙₙ）を何と呼ぶか？\"",
        "Opt1": "\"トレース（跡）\"",
        "Opt2": "\"行列式\"",
        "Opt3": "\"ノルム\"",
        "Opt4": "\"ランク\"",
        "Opt5": "\"転置\"",
        "Opt6": "\"アダマール積\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】対角成分の合計をトレースと呼び、$Tr(A)$ と表記します。これは固有値の合計とも一致するという非常に重要な性質を持っており、行列の特性把握に使われます。\""
    },
    {
        "ID": "\"7-026\"",
        "Category": "\"数理・統計\"",
        "Question": "\"点推定において、推定量の期待値が真のパラメータ（母数）と一致する性質を何というか？\"",
        "Opt1": "\"不偏性\"",
        "Opt2": "\"一致性\"",
        "Opt3": "\"有効性\"",
        "Opt4": "\"頑健性（ロバスト性）\"",
        "Opt5": "\"収束性\"",
        "Opt6": "\"線形性\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】推定量の平均がぴったり母数になることを不偏性と言います。サンプルサイズを大きくした時に母数に近づく性質は「一致性」と呼び、区別が必要です。\""
    },
    {
        "ID": "\"7-027\"",
        "Category": "\"数理・統計\"",
        "Question": "\"ベイズ統計学において、データを観測する前にあらかじめ設定しておく確率分布を何というか？\"",
        "Opt1": "\"事前分布\"",
        "Opt2": "\"事後分布\"",
        "Opt3": "\"尤度（ゆうど）関数\"",
        "Opt4": "\"周辺尤度\"",
        "Opt5": "\"共役分布\"",
        "Opt6": "\"正規分布\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】経験や過去の知識に基づいた確率が「事前分布」です。これに「尤度（データによる修正）」を掛け合わせることで、データ観測後の確率である「事後分布」を導き出します。\""
    },
    {
        "ID": "\"7-028\"",
        "Category": "\"数理・統計\"",
        "Question": "\"多変数関数 f(x",
        "Opt1": "y) において、特定の変数 x だけを動かした時の変化率を求める操作を何というか？\"",
        "Opt2": "\"偏微分\"",
        "Opt3": "\"全微分\"",
        "Opt4": "\"勾配計算\"",
        "Opt5": "\"ラプラス変換\"",
        "Opt6": "\"フーリエ変換\"",
        "Answer_Idx": "\"定積分\"",
        "Explanation": 0
    },
    {
        "ID": "\"7-029\"",
        "Category": "\"数理・統計\"",
        "Question": "\"2つのベクトル a",
        "Opt1": "b のなす角を θ としたとき、|a||b|cosθ で計算される値を何というか？\"",
        "Opt2": "\"内積（ドット積）\"",
        "Opt3": "\"外積（クロス積）\"",
        "Opt4": "\"テンソル積\"",
        "Opt5": "\"アダマール積\"",
        "Opt6": "\"ユークリッド距離\"",
        "Answer_Idx": "\"コサイン類似度\"",
        "Explanation": 0
    },
    {
        "ID": "\"7-030\"",
        "Category": "\"数理・統計\"",
        "Question": "\"情報理論において、ある事象が起こる確率が低いほど、その事象が起こった時に得られる情報の価値は高くなる。この情報の大きさを定義する式は？\"",
        "Opt1": "\"$-\\log P(x)$\"",
        "Opt2": "\"$\\exp(P(x))$\"",
        "Opt3": "\"$\\sqrt{P(x)}$\"",
        "Opt4": "\"$1 / P(x)$\"",
        "Opt5": "\"$1 - P(x)$\"",
        "Opt6": "\"$P(x) \\log P(x)$\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】自己情報量の定義式です。対数をとることで、独立な事象が同時に起こる情報の大きさを「和」で計算できるようになります。\""
    },
    {
        "ID": "\"7-031\"",
        "Category": "\"数理・統計\"",
        "Question": "\"行列 A の各要素 aᵢⱼ に対して、行と列を入れ替えた aⱼᵢ を要素に持つ行列を何というか？\"",
        "Opt1": "\"転置行列\"",
        "Opt2": "\"逆行列\"",
        "Opt3": "\"単位行列\"",
        "Opt4": "\"対角行列\"",
        "Opt5": "\"随伴行列\"",
        "Opt6": "\"直交行列\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】行と列を反転させる操作が転置（Transpose）です。ニューラルネットワークの計算（特に行列の次元調整）で頻繁に登場します。\""
    },
    {
        "ID": "\"7-032\"",
        "Category": "\"数理・統計\"",
        "Question": "\"確率変数 X が「1回起こるか起こらないか」という試行（例：コイン投げ）に従うとき、この分布を何というか？\"",
        "Opt1": "\"ベルヌーイ分布\"",
        "Opt2": "\"ポアソン分布\"",
        "Opt3": "\"二項分布\"",
        "Opt4": "\"正規分布\"",
        "Opt5": "\"カイ二乗分布\"",
        "Opt6": "\"t分布\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】1回限りの成功・失敗（1/0）を扱うのがベルヌーイ分布です。これを n 回繰り返したときの成功回数が「二項分布」になります。\""
    },
    {
        "ID": "\"7-033\"",
        "Category": "\"数理・統計\"",
        "Question": "\"データの集合において、(最大値 - 最小値) で算出される散らばりの指標を何というか？\"",
        "Opt1": "\"範囲（レンジ）\"",
        "Opt2": "\"標準偏差\"",
        "Opt3": "\"分散\"",
        "Opt4": "\"四分位範囲\"",
        "Opt5": "\"平均偏差\"",
        "Opt6": "\"最頻値\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】最も単純な散らばりの指標ですが、外れ値に極めて弱いという欠点があります。試験では「分散」などの定義と混同しないよう注意が必要です。\""
    },
    {
        "ID": "\"7-034\"",
        "Category": "\"数理・統計\"",
        "Question": "\"回帰分析において、予測値と実測値の差（残差）の2乗の合計を最小にするように係数を決定する手法は？\"",
        "Opt1": "\"最小二乗法\"",
        "Opt2": "\"最尤推定法\"",
        "Opt3": "\"ベイズ推定\"",
        "Opt4": "\"勾配降下法\"",
        "Opt5": "\"主成分分析\"",
        "Opt6": "\"k-近傍法\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】線形回帰の最も基本的な推定法です。誤差を2乗することで正負の影響をなくし、大きな誤差に重い罰則を与えます。\""
    },
    {
        "ID": "\"7-035\"",
        "Category": "\"数理・統計\"",
        "Question": "\"n個の要素から k個を選ぶ「組み合わせ（Combination）」の数を求める式は？\"",
        "Opt1": "\"$\\frac{n!}{k!(n-k)!}$\"",
        "Opt2": "\"$n^k$\"",
        "Opt3": "\"$n! / (n-k)!$\"",
        "Opt4": "\"$n \\times k$\"",
        "Opt5": "\"$n! / k!$\"",
        "Opt6": "\"$(n+k)! / n!$\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】順序を考慮しない選び方が組み合わせです。AIのハイパーパラメータの組み合わせ数や、アンサンブル学習のパターン計算などで使われます。\""
    },
    {
        "ID": "\"7-036\"",
        "Category": "\"数理_統計\"",
        "Question": "\"行列 A が正則行列（逆行列を持つ）であるための条件として正しいものは？\"",
        "Opt1": "\"行列式 det(A) が 0 ではない。\"",
        "Opt2": "\"行列 A が左右対称である。\"",
        "Opt3": "\"行列 A のすべての要素が正である。\"",
        "Opt4": "\"行列 A が対角行列である。\"",
        "Opt5": "\"行列 A のランクが 0 である。\"",
        "Opt6": "\"行列 A の行数と列数が異なる。\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】行列式が 0 の場合、その行列は「特異行列」と呼ばれ、逆行列が存在しません。これは「逆行列による変換が空間を潰してしまう」ことを意味します。\""
    },
    {
        "ID": "\"7-037\"",
        "Category": "\"数理・統計\"",
        "Question": "\"情報の乱雑さや不確実性を表す「エントロピー」が最大になるのはどのような状態か？\"",
        "Opt1": "\"すべての事象が等確率で起こる（一様分布）とき。\"",
        "Opt2": "\"特定の事象が必ず起こる（確率1）とき。\"",
        "Opt3": "\"データの個数が無限大のとき。\"",
        "Opt4": "\"平均値と分散が等しいとき。\"",
        "Opt5": "\"すべてのデータが 0 のとき。\"",
        "Opt6": "\"分布が正規分布に従うとき。\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】何が起こるか全く予測がつかない（どの事象も同じくらい起こりうる）状態が、最も情報量（驚き）の期待値が高くなります。\""
    },
    {
        "ID": "\"7-038\"",
        "Category": "\"数理・統計\"",
        "Question": "\"2つの変数の相関関係を調べる際、外れ値の影響を受けにくい（順位に基づいた）相関係数は？\"",
        "Opt1": "\"スピアマンの順位相関係数\"",
        "Opt2": "\"ピアソンの積率相関係数\"",
        "Opt3": "\"偏相関係数\"",
        "Opt4": "\"自己相関係数\"",
        "Opt5": "\"決定係数\"",
        "Opt6": "\"共分散\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】一般的な相関係数（ピアソン）は数値そのものを使いますが、スピアマンは「順位」に変換してから計算するため、極端な値（外れ値）に強いという特徴があります。\""
    },
    {
        "ID": "\"7-039\"",
        "Category": "\"数理・統計\"",
        "Question": "\"ベクトル v の長さ（原点からの距離）を測る指標を総称して何というか？\"",
        "Opt1": "\"ノルム\"",
        "Opt2": "\"トレース\"",
        "Opt3": "\"ランク\"",
        "Opt4": "\"次元\"",
        "Opt5": "\"カーネル\"",
        "Opt6": "\"バイアス\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】L1ノルム（絶対値の和）やL2ノルム（2乗和の平方根）などがあります。正則化（過学習抑制）の文脈で非常に重要な用語です。\""
    },
    {
        "ID": "\"7-040\"",
        "Category": "\"数理・統計\"",
        "Question": "\"統計的な推測において、母集団から標本を抽出する際に生じる、母集団の真の姿とのズレを何というか？\"",
        "Opt1": "\"標本誤差（サンプリング誤差）\"",
        "Opt2": "\"第一種の過誤\"",
        "Opt3": "\"過学習\"",
        "Opt4": "\"未学習\"",
        "Opt5": "\"系統誤差\"",
        "Opt6": "\"測定誤差\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】全数調査でない限り、標本から得た結果には必ず「たまたまそのメンバーが選ばれたことによる偏り」が生じます。これが標本誤差です。\""
    },
    {
        "ID": "\"7-041\"",
        "Category": "\"数理・統計\"",
        "Question": "\"確率変数 X",
        "Opt1": "Y に対して、期待値の性質 E[X + Y] = E[X] + E[Y] が成り立つ条件は？\"",
        "Opt2": "\"X と Y が独立でなくても常に成り立つ。\"",
        "Opt3": "\"X と Y が独立であるときのみ成り立つ。\"",
        "Opt4": "\"X と Y が正規分布に従うときのみ成り立つ。\"",
        "Opt5": "\"X と Y の分散が等しいときのみ成り立つ。\"",
        "Opt6": "\"X と Y が互いに排反であるときのみ成り立つ。\"",
        "Answer_Idx": "\"X と Y が連続型確率変数であるときのみ成り立つ。\"",
        "Explanation": 0
    },
    {
        "ID": "\"7-042\"",
        "Category": "\"数理・統計\"",
        "Question": "\"ある病気の罹患率が 1% の集団において、検査の陽性的中率（陽性と判定されたときに実際に病気である確率）を計算する際、分子に来るべき値は？\"",
        "Opt1": "\"(罹患率) × (感度)\"",
        "Opt2": "\"(罹患率) × (特異度)\"",
        "Opt3": "\"(1 - 罹患率) × (1 - 特異度)\"",
        "Opt4": "\"感度 + 特異度\"",
        "Opt5": "\"1 - 感度\"",
        "Opt6": "\"(1 - 罹患率) × (感度)\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】ベイズの定理の応用です。陽性的中率の分子は「実際に病気で(1%)、かつ検査で正しく陽性と出る(感度)」確率となります。分母は「病気で陽性」と「健康なのに誤って陽性」の合計です。\""
    },
    {
        "ID": "\"7-043\"",
        "Category": "\"数理・統計\"",
        "Question": "\"複数の変数のバラつきと、変数間の相関関係を網羅した行列を何というか？\"",
        "Opt1": "\"分散共分散行列\"",
        "Opt2": "\"相関行列\"",
        "Opt3": "\"単位行列\"",
        "Opt4": "\"対角行列\"",
        "Opt5": "\"逆行列\"",
        "Opt6": "\"ヘッセ行列\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】対角成分に各変数の「分散」が、非対角成分に変数間の「共分散」が配置された行列です。マハラノビス距離の計算や主成分分析において中心的な役割を果たします。\""
    },
    {
        "ID": "\"7-044\"",
        "Category": "\"数理・統計\"",
        "Question": "\"正規分布（ガウス分布）の確率密度関数において、分布の「広がり」を決定するパラメータはどれか？\"",
        "Opt1": "\"標準偏差（または分散）\"",
        "Opt2": "\"平均値\"",
        "Opt3": "\"中央値\"",
        "Opt4": "\"歪度\"",
        "Opt5": "\"尖度\"",
        "Opt6": "\"自由度\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】正規分布は平均 $\\mu$ で位置が、標準偏差 $\\sigma$ で山の高さと広がりが決まります。平均を変えると左右に平行移動し、標準偏差を大きくすると山が低く平べったくなります。\""
    },
    {
        "ID": "\"7-045\"",
        "Category": "\"数理・統計\"",
        "Question": "\"独立な n 個のベルヌーイ試行において、成功回数 k が従う分布を二項分布というが、この n を大きくしたとき（かつ成功確率 p が小さいとき）、二項分布が近似的に従う分布は？\"",
        "Opt1": "\"ポアソン分布\"",
        "Opt2": "\"正規分布\"",
        "Opt3": "\"カイ二乗分布\"",
        "Opt4": "\"t分布\"",
        "Opt5": "\"F分布\"",
        "Opt6": "\"一様分布\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】「稀にしか起こらない事象」の回数を扱うポアソン分布は、二項分布の極限（n大、p小）として導かれます。一方、pが小さくない場合にnを大きくすると正規分布に近似します。\""
    },
    {
        "ID": "\"7-046\"",
        "Category": "\"数理・統計\"",
        "Question": "\"仮説検定において、帰無仮説が正しいとした場合に、観測されたデータ（またはそれ以上に極端なデータ）が得られる確率を何というか？\"",
        "Opt1": "\"p値（有意確率）\"",
        "Opt2": "\"有意水準（α）\"",
        "Opt3": "\"検定力（1-β）\"",
        "Opt4": "\"信頼係数\"",
        "Opt5": "\"標準誤差\"",
        "Opt6": "\"寄与率\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】p値が小さいほど「めったに起こらないことが起きた」と判断し、帰無仮説を棄却します。よく「p値＝対立仮説が正しい確率」と誤解されますが、それはひっかけです。\""
    },
    {
        "ID": "\"7-047\"",
        "Category": "\"数理・統計\"",
        "Question": "\"情報理論におけるエントロピー H(X) の式 $H(X) = - \\sum P(x) \\log P(x)$ において、対数の底を 2 とした場合、単位は何になるか？\"",
        "Opt1": "\"ビット (bit)\"",
        "Opt2": "\"ナット (nat)\"",
        "Opt3": "\"ディット (dit)\"",
        "Opt4": "\"バイト (byte)\"",
        "Opt5": "\"デシベル (dB)\"",
        "Opt6": "\"パーセント (%)\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】底が 2 のときは bit、底が $e$（自然対数）のときは nat と呼ばれます。G検定ではこの単位の違いも稀に問われます。\""
    },
    {
        "ID": "\"7-048\"",
        "Category": "\"数理・統計\"",
        "Question": "\"確率変数 X の分散 V[X] を期待値 E を用いて表す式として正しいものは？\"",
        "Opt1": "\"$E[X^2] - (E[X])^2$\"",
        "Opt2": "\"$E[X^2] + (E[X])^2$\"",
        "Opt3": "\"$E[(X - E[X])]$\"",
        "Opt4": "\"$(E[X])^2 - E[X^2]$\"",
        "Opt5": "\"$E[X^2] / E[X]$\"",
        "Opt6": "\"$\\sqrt{E[X^2] - E[X]}$\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】「2乗の平均 引く 平均の2乗」という有名な公式です。定義式 $E[(X - \\mu)^2]$ を展開することで導かれます。計算問題で非常によく使います。\""
    },
    {
        "ID": "\"7-049\"",
        "Category": "\"数理・統計\"",
        "Question": "\"母集団が正規分布に従うと仮定できない場合や、サンプルサイズが小さい場合に、2つの群の「中央値」に差があるかを調べるノンパラメトリック検定は？\"",
        "Opt1": "\"マン・ホイットニーのU検定\"",
        "Opt2": "\"t検定\"",
        "Opt3": "\"ウェルチのt検定\"",
        "Opt4": "\"カイ二乗検定\"",
        "Opt5": "\"F検定\"",
        "Opt6": "\"アンサンス・コンボ検定\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】データの分布を限定しない（ノンパラメトリック）手法の代表例です。t検定などは平均値を比較しますが、U検定は順位和に基づき中央値的な差を評価します。\""
    },
    {
        "ID": "\"7-050\"",
        "Category": "\"数理・統計\"",
        "Question": "\"自由度が大きくなるにつれて、標準正規分布（N(0",
        "Opt1": "1)）に形が近づいていく確率分布はどれか？\"",
        "Opt2": "\"t分布\"",
        "Opt3": "\"ポアソン分布\"",
        "Opt4": "\"指数分布\"",
        "Opt5": "\"ベータ分布\"",
        "Opt6": "\"コーシー分布\"",
        "Answer_Idx": "\"対数正規分布\"",
        "Explanation": 0
    },
    {
        "ID": "\"7-051\"",
        "Category": "\"数理・統計\"",
        "Question": "\"ベクトル a = (3",
        "Opt1": "4) の L2ノルム（ユークリッド距離）の値は？\"",
        "Opt2": "\"5\"",
        "Opt3": "\"7\"",
        "Opt4": "\"12\"",
        "Opt5": "\"25\"",
        "Opt6": "\"1\"",
        "Answer_Idx": "\"0\"",
        "Explanation": 0
    },
    {
        "ID": "\"7-052\"",
        "Category": "\"数理・統計\"",
        "Question": "\"条件付き確率 P(A|B) と P(B|A) の関係について、正しい記述は？\"",
        "Opt1": "\"一般に P(A|B) と P(B|A) は等しくない。\"",
        "Opt2": "\"常に P(A|B) = P(B|A) が成り立つ。\"",
        "Opt3": "\"P(A) = P(B) のときのみ、両者は等しくなる。\"",
        "Opt4": "\"両者の和は必ず 1 になる。\"",
        "Opt5": "\"一方が決まれば、もう一方は計算なしで求まる。\"",
        "Opt6": "\"積 P(A|B)P(B|A) は常に 1 以下である。\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】ベイズの定理 $P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$ からわかる通り、事前確率 $P(A)$ と $P(B)$ が等しくない限り、逆の条件付き確率は一致しません。\""
    },
    {
        "ID": "\"7-053\"",
        "Category": "\"数理・統計\"",
        "Question": "\"統計学における「第一種の過誤（α）」を小さくしようとすると、一般にどのような現象が起きるか？\"",
        "Opt1": "\"第二種の過誤（β）が大きくなる。\"",
        "Opt2": "\"第二種の過誤（β）も小さくなる。\"",
        "Opt3": "\"検定力（1-β）が大きくなる。\"",
        "Opt4": "\"p値が大きくなる。\"",
        "Opt5": "\"サンプルサイズが自動的に増える。\"",
        "Opt6": "\"有意水準が 100% に近づく。\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】α（早とちり）を警戒して厳しく判定するほど、β（見逃し）が発生しやすくなるというトレードオフの関係にあります。\""
    },
    {
        "ID": "\"7-054\"",
        "Category": "\"数理・統計\"",
        "Question": "\"線形代数において、行列 A の逆行列 $A^{-1}$ が存在するための必要十分条件は？\"",
        "Opt1": "\"det(A) ≠ 0\"",
        "Opt2": "\"det(A) = 0\"",
        "Opt3": "\"A が正方行列ではないこと\"",
        "Opt4": "\"A のすべての固有値が 0 であること\"",
        "Opt5": "\"A のランクが 0 であること\"",
        "Opt6": "\"A が単位行列であること\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】行列式（デターミナント）が 0 でないことが、逆行列が存在する（正則である）ための条件です。\""
    },
    {
        "ID": "\"7-055\"",
        "Category": "\"数理・統計\"",
        "Question": "\"確率変数の期待値について、E[aX + b] （a",
        "Opt1": "bは定数）を計算するとどうなるか？\"",
        "Opt2": "\"aE[X] + b\"",
        "Opt3": "\"aE[X]\"",
        "Opt4": "\"E[X] + b\"",
        "Opt5": "\"a^2E[X] + b\"",
        "Opt6": "\"E[X]\"",
        "Answer_Idx": "\"(a+b)E[X]\"",
        "Explanation": 0
    },
    {
        "ID": "\"7-056\"",
        "Category": "\"数理・統計\"",
        "Question": "\"相関係数 r (-1 ≦ r ≦ 1) について、r = -0.9 である状態を正しく表したものは？\"",
        "Opt1": "\"強い負の相関がある。\"",
        "Opt2": "\"強い正の相関がある。\"",
        "Opt3": "\"ほとんど相関がない。\"",
        "Opt4": "\"因果関係が逆転している。\"",
        "Opt5": "\"計算ミスである。\"",
        "Opt6": "\"非線形な関係が 90% ある。\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】1 または -1 に近いほど直線的な相関が強く、-0.9 は一方が増えればもう一方が減るという「負の相関」が非常に強いことを示します。\""
    },
    {
        "ID": "\"7-057\"",
        "Category": "\"数理・統計\"",
        "Question": "\"データの分布が右に裾を引いている（左側に山が偏っている）状態のとき、歪度（わいど）の符号はどうなるか？\"",
        "Opt1": "\"正（プラス）\"",
        "Opt2": "\"負（マイナス）\"",
        "Opt3": "\"0\"",
        "Opt4": "\"1\"",
        "Opt5": "\"判定不能\"",
        "Opt6": "\"常に複素数になる\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】右に裾が長い分布を「正の歪み」と言います。平均値が中央値よりも右側（大きい方）に引っ張られるのが特徴です。\""
    },
    {
        "ID": "\"7-058\"",
        "Category": "\"数理・統計\"",
        "Question": "\"推測統計において、母集団から標本を抽出する際、どの個体も等しい確率で選ばれるように抽出する方法を何というか？\"",
        "Opt1": "\"無作為抽出（ランダムサンプリング）\"",
        "Opt2": "\"有意抽出\"",
        "Opt3": "\"層化抽出\"",
        "Opt4": "\"多段抽出\"",
        "Opt5": "\"スノーボールサンプリング\"",
        "Opt6": "\"系統抽出\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】統計的推論の妥当性を支える最も基本的な抽出法です。作為が入るとサンプリングバイアスが生じ、結果が歪みます。\""
    },
    {
        "ID": "\"7-059\"",
        "Category": "\"数理・統計\"",
        "Question": "\"行列 A とその逆行列 $A^{-1}$ を掛け合わせた結果（$AA^{-1}$）はどうなるか？\"",
        "Opt1": "\"単位行列 I\"",
        "Opt2": "\"零行列 O\"",
        "Opt3": "\"元の行列 A\"",
        "Opt4": "\"逆行列 $A^{-1}$\"",
        "Opt5": "\"行列式 det(A)\"",
        "Opt6": "\"スカラー 1\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】逆行列とは、掛けると単位行列（対角成分が 1、他が 0 の行列）になる行列のことです。数値計算における「逆数」のような役割です。\""
    },
    {
        "ID": "\"7-060\"",
        "Category": "\"数理・統計\"",
        "Question": "\"二項分布 B(n",
        "Opt1": "p) の期待値（平均）を求める式はどれか？\"",
        "Opt2": "\"np\"",
        "Opt3": "\"np(1-p)\"",
        "Opt4": "\"n/p\"",
        "Opt5": "\"p/n\"",
        "Opt6": "\"$\\sqrt{np}$\"",
        "Answer_Idx": "\"$n^2p$\"",
        "Explanation": 0
    },
    {
        "ID": "\"7-061\"",
        "Category": "\"数理・統計\"",
        "Question": "\"ベイズの定理において、新たなデータを得るたびに事後確率を次の推論の事前確率として使い、確率を更新していくプロセスを何というか？\"",
        "Opt1": "\"ベイズ更新\"",
        "Opt2": "\"最尤推定\"",
        "Opt3": "\"ラプラス平滑化\"",
        "Opt4": "\"マルコフ連鎖\"",
        "Opt5": "\"EMアルゴリズム\"",
        "Opt6": "\"モンテカルロ法\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】観測データが増えるごとに確信度を高めていくプロセスです。スパムメール判定や自動運転の自己位置推定などでリアルタイムに確率を書き換える手法として重要です。\""
    },
    {
        "ID": "\"7-062\"",
        "Category": "\"数理・統計\"",
        "Question": "\"行列 A の「ランク（階数）」が意味するものとして、最も適切な説明は？\"",
        "Opt1": "\"行列の中で互いに線形独立な行（または列）の最大数。\"",
        "Opt2": "\"行列に含まれる 0 ではない要素の総数。\"",
        "Opt3": "\"行列の行数と列数を掛け合わせた値。\"",
        "Opt4": "\"行列式 det(A) の値。\"",
        "Opt5": "\"固有値の合計値。\"",
        "Opt6": "\"逆行列が存在するかどうかを示す 0 か 1 のフラグ。\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】ランクは、その行列が「何次元分の情報を保持しているか」を表します。ランクがフルランク（行・列の数と一致）でない場合、その行列による変換は空間を押し潰してしまい、情報の損失が発生します。\""
    },
    {
        "ID": "\"7-063\"",
        "Category": "\"数理・統計\"",
        "Question": "\"独立した 2 つの正規分布に従う群において、それぞれの「分散」が等しいかどうかを調べるために用いられる検定統計量は？\"",
        "Opt1": "\"F分布\"",
        "Opt2": "\"カイ二乗分布\"",
        "Opt3": "\"t分布\"",
        "Opt4": "\"標準正規分布\"",
        "Opt5": "\"ポアソン分布\"",
        "Opt6": "\"二項分布\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】「分散の比」が従う分布がF分布です。2群の平均の差を調べるt検定（等分散性の仮定が必要な場合）の前段階として行われることが多い検定です。\""
    },
    {
        "ID": "\"7-064\"",
        "Category": "\"数理・統計\"",
        "Question": "\"標準正規分布に従う $k$ 個の独立な確率変数の「2乗の和」が従う確率分布はどれか？\"",
        "Opt1": "\"カイ二乗分布\"",
        "Opt2": "\"t分布\"",
        "Opt3": "\"F分布\"",
        "Opt4": "\"指数分布\"",
        "Opt5": "\"対数正規分布\"",
        "Opt6": "\"ガンマ分布\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】分散の推定や「適合度検定」「独立性検定」に用いられる重要な分布です。自由度 $k$ が大きくなるほど、分布の形状は右に裾を引いた形から正規分布に近づきます。\""
    },
    {
        "ID": "\"7-065\"",
        "Category": "\"数理・統計\"",
        "Question": "\"線形代数において、行列 A の固有値 $\\lambda_1",
        "Opt1": "\\lambda_2",
        "Opt2": "\\dots",
        "Opt3": "\\lambda_n$ の積（$\\prod \\lambda_i$）と等しくなる指標はどれか？\"",
        "Opt4": "\"行列式 det(A)\"",
        "Opt5": "\"トレース Tr(A)\"",
        "Opt6": "\"ランク rank(A)\"",
        "Answer_Idx": "\"ノルム ||A||\"",
        "Explanation": "\"転置行列 $A^T$\""
    },
    {
        "ID": "\"7-066\"",
        "Category": "\"数理・統計\"",
        "Question": "\"機械学習の評価指標「決定係数（$R^2$）」の説明として正しいものは？\"",
        "Opt1": "\"回帰モデルがデータの変動をどれくらい説明できているかを表し、最大値は 1 である。\"",
        "Opt2": "\"分類モデルの正解率（Accuracy）と同じ意味である。\"",
        "Opt3": "\"常に 0 から 100 の値をとる。\"",
        "Opt4": "\"説明変数の数が増えるほど必ず減少する。\"",
        "Opt5": "\"相関係数の値を 2 倍したものである。\"",
        "Opt6": "\"誤差の絶対値の平均を表す。\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】1 に近いほどモデルの当てはまりが良いことを示します。ただし、説明変数を増やすだけで値が上がってしまう欠点があるため、通常は「自由度調整済み決定係数」が使われます。\""
    },
    {
        "ID": "\"7-067\"",
        "Category": "\"数理・統計\"",
        "Question": "\"「ある特定の期間において、次に事象が発生するまでの時間」が従う連続確率分布はどれか？\"",
        "Opt1": "\"指数分布\"",
        "Opt2": "\"ポアソン分布\"",
        "Opt3": "\"ベルヌーイ分布\"",
        "Opt4": "\"二項分布\"",
        "Opt5": "\"一様分布\"",
        "Opt6": "\"t分布\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】ポアソン分布が「回数」を扱うのに対し、指数分布は「待ち時間」を扱います。これらは裏返しの関係にあり、どちらも「無記憶性（過去の経過が未来に影響しない）」という特徴を持ちます。\""
    },
    {
        "ID": "\"7-068\"",
        "Category": "\"数理・統計\"",
        "Question": "\"正規分布において、平均 $\\mu$、標準偏差 $\\sigma$ としたとき、データ $x$ を $z = (x - \\mu) / \\sigma$ で変換する操作を何というか？\"",
        "Opt1": "\"標準化（Zスコア正規化）\"",
        "Opt2": "\"スケーリング（Min-Max正規化）\"",
        "Opt3": "\"白色化\"",
        "Opt4": "\"主成分分析\"",
        "Opt5": "\"正則化\"",
        "Opt6": "\"量子化\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】平均を 0、分散を 1 に変換する操作です。異なる単位のデータ（身長と体重など）を同じ尺度で比較したり、ニューラルネットワークの学習を安定させたりするために必須の処理です。\""
    },
    {
        "ID": "\"7-069\"",
        "Category": "\"数理・統計\"",
        "Question": "\"事象 A と事象 B が「独立」であるとき、積事象の確率 $P(A \\cap B)$ について成り立つ式は？\"",
        "Opt1": "\"$P(A) \\times P(B)$\"",
        "Opt2": "\"$P(A) + P(B)$\"",
        "Opt3": "\"$P(A) + P(B) - P(A \\cap B)$\"",
        "Opt4": "\"$P(A|B)$\"",
        "Opt5": "\"$P(B|A)$\"",
        "Opt6": "\"$P(A) / P(B)$\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】独立とは「一方が起きたことが、もう一方が起きる確率に影響しない」状態です。この時のみ、単純な掛け算で同時確率が求まります。\""
    },
    {
        "ID": "\"7-070\"",
        "Category": "\"数理・統計\"",
        "Question": "\"ディープラーニングの「誤差逆伝播法」において、多変数関数の勾配（各変数の偏微分を並べたベクトル）を記号 $\\nabla$ を使って表すとき、この記号の読み方は？\"",
        "Opt1": "\"ナブラ\"",
        "Opt2": "\"デルタ\"",
        "Opt3": "\"シグマ\"",
        "Opt4": "\"パイ\"",
        "Opt5": "\"オメガ\"",
        "Opt6": "\"ラムダ\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】数学的にはハミルトン演算子と呼ばれます。$\\nabla f$ は関数 $f$ が最も急激に増加する方向を指し、その逆方向に重みを更新するのが勾配降下法です。\""
    },
    {
        "ID": "\"7-071\"",
        "Category": "\"数理・統計\"",
        "Question": "\"大標本（サンプル数が多い）において、母集団の分布に関わらず、標本平均の分布が正規分布に近づくという定理を何というか？\"",
        "Opt1": "\"中心極限定理\"",
        "Opt2": "\"大数の法則\"",
        "Opt3": "\"ベイズの定理\"",
        "Opt4": "\"チェビシェフの不等式\"",
        "Opt5": "\"ガウス・マルコフの定理\"",
        "Opt6": "\"平均値の定理\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】統計学の最も強力な定理の一つです。元のデータがサイコロのような一様分布でも、その「平均値」を何度も取れば、その分布は美しい釣鐘型の正規分布になります。\""
    },
    {
        "ID": "\"7-072\"",
        "Category": "\"数理・統計\"",
        "Question": "\"行列 A の転置行列を $A^T$ としたとき、積の転置 $(AB)^T$ を展開した正しい形は？\"",
        "Opt1": "\"$B^T A^T$\"",
        "Opt2": "\"$A^T B^T$\"",
        "Opt3": "\"$AB$\"",
        "Opt4": "\"$B A$\"",
        "Opt5": "\"$A^T + B^T$\"",
        "Opt6": "\"$(BA)^T$\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】積の転置をとると、順序が入れ替わります。$(AB)^T = B^T A^T$ です。逆行列 $(AB)^{-1} = B^{-1} A^{-1}$ も同様に順序が入れ替わります。計算問題のひっかけで頻出です。\""
    },
    {
        "ID": "\"7-073\"",
        "Category": "\"数理・統計\"",
        "Question": "\"統計的な推定において、信頼区間の幅（例：95%信頼区間）を狭くして精度を高めるために、最も直接的に有効な手段は？\"",
        "Opt1": "\"サンプルサイズ（標本数）を大きくする。\"",
        "Opt2": "\"有意水準 $\\alpha$ を大きくする。\"",
        "Opt3": "\"標本分散をわざと大きく計算する。\"",
        "Opt4": "\"母平均の値を書き換える。\"",
        "Opt5": "\"正規分布以外の分布を仮定する。\"",
        "Opt6": "\"計算を途中で打ち切る。\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】信頼区間の幅は $1/\\sqrt{n}$ に比例して狭くなります。つまり、データを 4 倍にすれば、区間の幅（誤差の範囲）を半分に絞り込むことができます。\""
    },
    {
        "ID": "\"7-074\"",
        "Category": "\"数理・統計\"",
        "Question": "\"ある変数が「対数をとると正規分布に従う」ような、右に長く裾を引く分布を何というか？\"",
        "Opt1": "\"対数正規分布\"",
        "Opt2": "\"指数分布\"",
        "Opt3": "\"パレート分布\"",
        "Opt4": "\"カイ二乗分布\"",
        "Opt5": "\"コーシー分布\"",
        "Opt6": "\"一様分布\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】所得の分布や、Webサイトへのアクセス数など、自然界や社会現象で「少数の大きな値と、多数の小さな値」が見られる場面でよく登場します。\""
    },
    {
        "ID": "\"7-075\"",
        "Category": "\"数理・統計\"",
        "Question": "\"確率論において、ある試行の結果が「それ以前の試行の結果のみに依存して決まる」という性質を何というか？\"",
        "Opt1": "\"マルコフ性\"",
        "Opt2": "\"独立性\"",
        "Opt3": "\"定常性\"",
        "Opt4": "\"エルゴード性\"",
        "Opt5": "\"線形性\"",
        "Opt6": "\"一様性\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】「過去の経緯は関係なく、現在の状態だけで次の状態が決まる」という性質です。強化学習（MDP：マルコフ決定過程）の基礎となる概念です。\""
    },
    {
        "ID": "\"7-076\"",
        "Category": "\"数理・統計\"",
        "Question": "\"2つの変数の関係を示す「共分散」の欠点として、適切なものはどれか？\"",
        "Opt1": "\"データの単位（スケール）によって値が大きく変わるため、相関の強さを比較しにくい。\"",
        "Opt2": "\"負の値をとることができない。\"",
        "Opt3": "\"直線的な関係しか表すことができない。\"",
        "Opt4": "\"計算に必ず複素数が必要になる。\"",
        "Opt5": "\"常に 0 から 1 の間に収まってしまう。\"",
        "Opt6": "\"サンプルサイズが 100 以下のときは計算できない。\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】共分散をそれぞれの標準偏差で割って（標準化して）、単位に依存しないようにしたものが「相関係数」です。共分散単体では、値が大きくても「単に数字が大きいだけ」なのか「強い相関がある」のか判別できません。\""
    },
    {
        "ID": "\"7-077\"",
        "Category": "\"数理・統計\"",
        "Question": "\"線形代数における「単位行列」の特徴として誤っているものはどれか？\"",
        "Opt1": "\"任意の行列 A との積は常に 0 になる。\"",
        "Opt2": "\"対角成分がすべて 1 である。\"",
        "Opt3": "\"非対角成分がすべて 0 である。\"",
        "Opt4": "\"任意の行列 A との積 AI は A になる。\"",
        "Opt5": "\"逆行列は自分自身（I）である。\"",
        "Opt6": "\"正方行列である。\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】単位行列は数値で言うところの「1」の役割を果たします。掛けて 0 になるのは「零行列」です。\""
    },
    {
        "ID": "\"7-078\"",
        "Category": "\"数理・統計\"",
        "Question": "\"情報理論において、2つの確率分布 $P",
        "Opt1": "Q$ が一致しているとき、KLダイバージェンス（カルバック・ライブラー情報量）の値はどうなるか？\"",
        "Opt2": "\"0\"",
        "Opt3": "\"1\"",
        "Opt4": "\"無限大\"",
        "Opt5": "\"0.5\"",
        "Opt6": "\"-1\"",
        "Answer_Idx": "\"$\\log 2$\"",
        "Explanation": 0
    },
    {
        "ID": "\"7-079\"",
        "Category": "\"数理・統計\"",
        "Question": "\"統計的仮説検定において、帰無仮説を棄却するかどうかを判断するための境界となる値を何というか？\"",
        "Opt1": "\"棄却域（または臨界値）\"",
        "Opt2": "\"p値\"",
        "Opt3": "\"期待値\"",
        "Opt4": "\"中央値\"",
        "Opt5": "\"標準誤差\"",
        "Opt6": "\"自由度\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】有意水準（例えば 5%）に対応する統計量の境界点です。計算した統計量がこの「棄却域」に入れば、帰無仮説を捨てて「有意な差がある」と結論づけます。\""
    },
    {
        "ID": "\"7-080\"",
        "Category": "\"数理・統計\"",
        "Question": "\"機械学習の「数理・統計」を学ぶ最終的な目的として、G検定の文脈で最も適切なものは？\"",
        "Opt1": "\"データの背後にある確率的な法則を理解し、モデルの挙動を正しく解釈・評価するため。\"",
        "Opt2": "\"複雑な微分方程式を手計算で解けるようにするため。\"",
        "Opt3": "\"プログラミング言語を使わずにAIを構築するため。\"",
        "Opt4": "\"統計学者が作成した論文の誤字を見つけるため。\"",
        "Opt5": "\"コンピュータの計算速度を物理的に向上させるため。\"",
        "Opt6": "\"すべての予測を 100% の確率で当てるため。\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】AIは魔法ではなく、確率と統計に基づく数学的モデルです。その限界（標本誤差、バイアス、不確実性）を正しく理解することが、ジェネラリストとして最も重要な素養です。\""
    },
        {
        "ID": "\"L-001\"",
        "Category": "\"法律・倫理\"",
        "Question": "\"AIの開発において、人間が意思決定のプロセスに関与し、最終的な責任を負うべきであるという考え方を「Human in the [ (1) ]」と呼ぶ。\"",
        "Opt1": "\"Loop\"",
        "Opt2": "\"Circle\"",
        "Opt3": "\"Chain\"",
        "Opt4": "\"Middle\"",
        "Opt5": "\"Process\"",
        "Opt6": "\"Side\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】「Human in the Loop」は、AIシステムが自律的に動くのではなく、重要な判断に人間が介在することを指します。信頼できるAIの基本原則の一つです。\""
    },
    {
        "ID": "\"L-002\"",
        "Category": "\"法律・倫理\"",
        "Question": "\"2024年に成立した欧州の「EU AI法」において、個人の行動を監視する「社会信用スコアリング」などは、[ (1) ] 階層に分類され、原則禁止とされる。\"",
        "Opt1": "\"許容できないリスク\"",
        "Opt2": "\"高いリスク\"",
        "Opt3": "\"限定的なリスク\"",
        "Opt4": "\"最小限のリスク\"",
        "Opt5": "\"不明確なリスク\"",
        "Opt6": "\"管理可能なリスク\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】EU AI法では基本的権利を侵害する「許容できないリスク（Unacceptable Risk）」は厳格に禁止されています。\""
    },
    {
        "ID": "\"L-003\"",
        "Category": "\"法律・倫理\"",
        "Question": "\"生成AIが他者の著作物と類似性があり、かつ元となる著作物に基づいたという「[ (1) ]」が認められる場合、著作権侵害となる。\"",
        "Opt1": "\"依拠性\"",
        "Opt2": "\"公共性\"",
        "Opt3": "\"市場性\"",
        "Opt4": "\"創作性\"",
        "Opt5": "\"同一性\"",
        "Opt6": "\"目的性\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】日本の著作権法では「類似性」と「依拠性」の2点が揃った場合に侵害とみなされます。\""
    },
    {
        "ID": "\"L-004\"",
        "Category": "\"法律・倫理\"",
        "Question": "\"個人情報保護法において、本人の人種、信条、病歴などは「[ (1) ]」として、取得には原則として本人の同意が必要である。\"",
        "Opt1": "\"要配慮個人情報\"",
        "Opt2": "\"匿名加工情報\"",
        "Opt3": "\"仮名加工情報\"",
        "Opt4": "\"個人識別符号\"",
        "Opt5": "\"個人関連情報\"",
        "Opt6": "\"機微データ\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】「要配慮個人情報」は、不当な差別や偏見が生じる可能性がある情報を指します。\""
    },
    {
        "ID": "\"L-005\"",
        "Category": "\"法律・倫理\"",
        "Question": "\"AIモデルの判断根拠を人間に説明できる状態にすることを [ (1) ] と呼ぶ。\"",
        "Opt1": "\"XAI（説明可能なAI）\"",
        "Opt2": "\"AGI（汎用人工知能）\"",
        "Opt3": "\"MLOps\"",
        "Opt4": "\"AIガバナンス\"",
        "Opt5": "\"エッジAI\"",
        "Opt6": "\"連合学習\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】eXplainable AIの略。透明性と説明責任を担保するための技術です。\""
    },
    {
        "ID": "\"L-006\"",
        "Category": "\"法律・倫理\"",
        "Question": "\"学習済みモデル自体を他者に提供する場合、その権利関係は主に著作権ではなく [ (1) ] で規定されることが多い。\"",
        "Opt1": "\"契約（利用規約）\"",
        "Opt2": "\"特許権\"",
        "Opt3": "\"意匠権\"",
        "Opt4": "\"商標権\"",
        "Opt5": "\"肖像権\"",
        "Opt6": "\"不正競争防止法\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】実務上はライセンス契約によって利用制限や権利を定めるのが一般的です。\""
    },
    {
        "ID": "\"L-007\"",
        "Category": "\"法律・倫理\"",
        "Question": "\"特定のグループに対して不当に不利な結果を出してしまう現象を「[ (1) ]」と呼び、公平性の観点から問題視される。\"",
        "Opt1": "\"バイアス\"",
        "Opt2": "\"過学習\"",
        "Opt3": "\"勾配消失\"",
        "Opt4": "\"ハルシネーション\"",
        "Opt5": "\"モード崩壊\"",
        "Opt6": "\"データの不均衡\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】データの偏りなどにより生じる「バイアス（偏見）」は、差別的な判定を生むリスクがあります。\""
    },
    {
        "ID": "\"L-008\"",
        "Category": "\"法律・倫理\"",
        "Question": "\"2024年に総務省・経産省が公開した、開発者・利用者すべてが参照すべき指針を「[ (1) ]」という。\"",
        "Opt1": "\"AI利活用ガイドライン\"",
        "Opt2": "\"AI戦略会議資料\"",
        "Opt3": "\"AI倫理原則\"",
        "Opt4": "\"信頼できるAI指針\"",
        "Opt5": "\"AI開発宣言\"",
        "Opt6": "\"AIホワイトペーパー\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】「AI利活用ガイドライン」として統合され、リスク管理のあり方が示されています。\""
    },
    {
        "ID": "\"L-009\"",
        "Category": "\"法律・倫理\"",
        "Question": "\"著作権法第30条の4では、[ (1) ] の目的であれば、著作権者の承諾なく著作物を利用できると規定されている。\"",
        "Opt1": "\"情報解析\"",
        "Opt2": "\"商用利用\"",
        "Opt3": "\"バックアップ\"",
        "Opt4": "\"教育利用\"",
        "Opt5": "\"私的複製\"",
        "Opt6": "\"パロディ\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】日本は「情報解析」目的であれば原則として許諾なく学習利用が可能です。\""
    },
    {
        "ID": "\"L-010\"",
        "Category": "\"法律・倫理\"",
        "Question": "\"AIが事実とは異なるもっともらしい嘘をつく現象を「[ (1) ]」と呼ぶ。\"",
        "Opt1": "\"ハルシネーション（幻覚）\"",
        "Opt2": "\"オーバーフィッティング\"",
        "Opt3": "\"アンダーフィッティング\"",
        "Opt4": "\"データドリフト\"",
        "Opt5": "\"プロンプトインジェクション\"",
        "Opt6": "\"シャドーAI\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】生成AIの大きな課題の一つであり、ファクトチェックが重要です。\""
    },
    {
        "ID": "\"L-011\"",
        "Category": "\"法律・倫理\"",
        "Question": "\"倫理的・法的・社会的課題を総称して [ (1) ] と呼ぶ。\"",
        "Opt1": "\"ELSI\"",
        "Opt2": "\"OECD\"",
        "Opt3": "\"FAT\"",
        "Opt4": "\"GDPR\"",
        "Opt5": "\"WIPO\"",
        "Opt6": "\"NIST\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】Ethical"
    },
    {
        "ID": "\"L-012\"",
        "Category": "\"法律・倫理\"",
        "Question": "\"著作権者の利益を「[ (1) ]」に害する場合は、法第30条の4の例外として認められない。\"",
        "Opt1": "\"不当\"",
        "Opt2": "\"直接\"",
        "Opt3": "\"間接\"",
        "Opt4": "\"意図的\"",
        "Opt5": "\"潜在的\"",
        "Opt6": "\"恒久的に\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】「不当に害する場合」は、法的に許容される範囲を超えた利用を指します。\""
    },
    {
        "ID": "\"L-013\"",
        "Category": "\"法律・倫理\"",
        "Question": "\"AIによる自動化により、既存の職業が失われる懸念を [ (1) ] という。\"",
        "Opt1": "\"労働代替\"",
        "Opt2": "\"ディスラプション\"",
        "Opt3": "\"リスキリング\"",
        "Opt4": "\"デジタルデバイド\"",
        "Opt5": "\"技術的特異点\"",
        "Opt6": "\"産業空洞化\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】AIが人間の仕事を代行する社会的な影響を指します。\""
    },
    {
        "ID": "\"L-014\"",
        "Category": "\"法律・倫理\"",
        "Question": "\"復元できないように加工し、本人の同意なく第三者提供を可能にした情報を [ (1) ] という。\"",
        "Opt1": "\"匿名加工情報\"",
        "Opt2": "\"仮名加工情報\"",
        "Opt3": "\"個人関連情報\"",
        "Opt4": "\"非識別情報\"",
        "Opt5": "\"秘密計算情報\"",
        "Opt6": "\"統計情報\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】「匿名加工情報」は特定の個人を識別できないよう不可逆的に加工したものです。\""
    },
    {
        "ID": "\"L-015\"",
        "Category": "\"法律・倫理\"",
        "Question": "\"悪意のある入力を行って制限を回避したり情報を引き出したりする攻撃を [ (1) ] という。\"",
        "Opt1": "\"プロンプトインジェクション\"",
        "Opt2": "\"バックドア攻撃\"",
        "Opt3": "\"モデル反転攻撃\"",
        "Opt4": "\"ポイズニング攻撃\"",
        "Opt5": "\"アドバーサリアルエグザンプル\"",
        "Opt6": "\"踏み台攻撃\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】対話型AIの安全性を脅かす代表的な攻撃手法です。\""
    },
    {
        "ID": "\"L-016\"",
        "Category": "\"法律・倫理\"",
        "Question": "\"日本のガイドラインでは、提供者や利用者が共通して「[ (1) ]」を構築することが推奨されている。\"",
        "Opt1": "\"AIガバナンス\"",
        "Opt2": "\"AIファイアウォール\"",
        "Opt3": "\"AIセキュリティ\"",
        "Opt4": "\"AIレジリエンス\"",
        "Opt5": "\"AIコンプライアンス\"",
        "Opt6": "\"AI監査システム\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】組織全体でAIを適切に管理・運用する仕組みを指します。\""
    },
    {
        "ID": "\"L-017\"",
        "Category": "\"法律・倫理\"",
        "Question": "\"ネット上の情報を自動取得する [ (1) ] は、サイトの利用規約で禁止されている場合がある。\"",
        "Opt1": "\"スクレイピング\"",
        "Opt2": "\"ラベリング\"",
        "Opt3": "\"アノテーション\"",
        "Opt4": "\"クレンジング\"",
        "Opt5": "\"サンプリング\"",
        "Opt6": "\"オーグメンテーション\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】技術的には可能ですが、法的な規約違反のリスクがあります。\""
    },
    {
        "ID": "\"L-018\"",
        "Category": "\"法律・倫理\"",
        "Question": "\"設計段階からプライバシー保護機能を組み込む考え方を [ (1) ] という。\"",
        "Opt1": "\"Privacy by Design\"",
        "Opt2": "\"Security by Default\"",
        "Opt3": "\"Privacy Impact Assessment\"",
        "Opt4": "\"Privacy First\"",
        "Opt5": "\"Zero Trust\"",
        "Opt6": "\"Data Privacy Act\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】後付けではなく初期段階からのプライバシー考慮を推奨する原則です。\""
    },
    {
        "ID": "\"L-019\"",
        "Category": "\"法律・倫理\"",
        "Question": "\"GDPRにおいて、自分のデータを他サービスへ移動させる権利を [ (1) ] という。\"",
        "Opt1": "\"データポータビリティ\"",
        "Opt2": "\"忘れられる権利\"",
        "Opt3": "\"異議申し立て権\"",
        "Opt4": "\"自動化された決定を受けない権利\"",
        "Opt5": "\"アクセス権\"",
        "Opt6": "\"修正権\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】利用者が自分のデータを自由に持ち運べる権利です。\""
    },
    {
        "ID": "\"L-020\"",
        "Category": "\"法律・倫理\"",
        "Question": "\"人間に「[ (1) ]」が認められない限り、AI生成物に著作権は発生しない。\"",
        "Opt1": "\"創作的寄与\"",
        "Opt2": "\"金銭的対価\"",
        "Opt3": "\"排他的権利\"",
        "Opt4": "\"時間的労力\"",
        "Opt5": "\"技術的理解\"",
        "Opt6": "\"物理的接触\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】人間がAIを道具として使い、創作的な意図を反映させる必要があります。\""
    },
    {
        "ID": "\"L-021\"",
        "Category": "\"法律・倫理\"",
        "Question": "\"不正競争防止法では、他社の「[ (1) ]」を不正に取得・使用する行為を禁じているが、AIの学習データがこれに該当するかは管理状況による。\"",
        "Opt1": "\"営業秘密\"",
        "Opt2": "\"公知情報\"",
        "Opt3": "\"特許明細\"",
        "Opt4": "\"意匠原案\"",
        "Opt5": "\"商標リスト\"",
        "Opt6": "\"ドメイン名\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】「秘密として管理されていること」「有用な情報であること」「非公知であること」の3条件を満たすと営業秘密として保護されます。\""
    },
    {
        "ID": "\"L-022\"",
        "Category": "\"法律・倫理\"",
        "Question": "\"AIが生成したコンテンツが、既存の商標やブランドロゴと酷似している場合、[ (1) ] 侵害となる可能性がある。\"",
        "Opt1": "\"商標権\"",
        "Opt2": "\"特許権\"",
        "Opt3": "\"意匠権\"",
        "Opt4": "\"肖像権\"",
        "Opt5": "\"パブリシティ権\"",
        "Opt6": "\"実用新案権\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】商品やサービスの識別標識を保護するのが商標権です。意図せず生成されたものでも、商用利用時には注意が必要です。\""
    },
    {
        "ID": "\"L-023\"",
        "Category": "\"法律・倫理\"",
        "Question": "\"プロンプトに特定の著名人の名前を入れて、その人物にそっくりの画像を生成・公開する行為は、[ (1) ] を侵害するリスクが高い。\"",
        "Opt1": "\"肖像権\"",
        "Opt2": "\"特許権\"",
        "Opt3": "\"商標権\"",
        "Opt4": "\"著作隣接権\"",
        "Opt5": "\"意匠権\"",
        "Opt6": "\"実用新案権\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】個人の容姿を勝手に公表されない権利（肖像権）や、著名人の顧客吸引力を保護するパブリシティ権が問題になります。\""
    },
    {
        "ID": "\"L-024\"",
        "Category": "\"法律・倫理\"",
        "Question": "\"景品表示法において、AIを用いて作成した広告であっても、事実と異なる著しく優良であると誤認させる表示は「[ (1) ]」として禁止される。\"",
        "Opt1": "\"優良誤認\"",
        "Opt2": "\"有利誤認\"",
        "Opt3": "\"おとり広告\"",
        "Opt4": "\"不当表示\"",
        "Opt5": "\"誇大広告\"",
        "Opt6": "\"ステルスマーケティング\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】実際よりも優れていると偽る表示は優良誤認となります。生成AIによる架空の実績表示などはこれに該当する恐れがあります。\""
    },
    {
        "ID": "\"L-025\"",
        "Category": "\"法律・倫理\"",
        "Question": "\"AIの開発・運用に関する国際的な指針として、2019年に [ (1) ] が「AI原則」を採択し、人間中心のAI社会の実現を掲げた。\"",
        "Opt1": "\"OECD（経済協力開発機構）\"",
        "Opt2": "\"UNESCO\"",
        "Opt3": "\"WIPO\"",
        "Opt4": "\"WHO\"",
        "Opt5": "\"WTO\"",
        "Opt6": "\"IMF\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】OECDのAI原則は、日本の「人間中心のAI社会原則」のベースにもなっており、国際標準的な考え方です。\""
    },
    {
        "ID": "\"L-026\"",
        "Category": "\"法律・倫理\"",
        "Question": "\"ソフトウェアのソースコードをAIに学習させる際、そのコードが [ (1) ] ライセンス（GPLなど）で配布されている場合、利用条件に注意が必要である。\"",
        "Opt1": "\"オープンソース\"",
        "Opt2": "\"クローズド\"",
        "Opt3": "\"シェアウェア\"",
        "Opt4": "\"パブリックドメイン\"",
        "Opt5": "\"コピーガード\"",
        "Opt6": "\"プロプライエタリ\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】OSSのライセンス条件（著作権表示義務など）を無視して学習・生成を行うと、ライセンス違反となる可能性があります。\""
    },
    {
        "ID": "\"L-027\"",
        "Category": "\"法律・倫理\"",
        "Question": "\"AIが医療診断を補助する場合、最終的な診断と治療の責任は [ (1) ] が負うべきであることが、現在の日本の法制度における原則である。\"",
        "Opt1": "\"医師\"",
        "Opt2": "\"AI開発者\"",
        "Opt3": "\"病院経営者\"",
        "Opt4": "\"AI販売会社\"",
        "Opt5": "\"厚生労働省\"",
        "Opt6": "\"患者自身\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】AIはあくまで「診断支援」の道具であり、最終的な法的責任は資格を持つ人間（医師）が負います。\""
    },
    {
        "ID": "\"L-028\"",
        "Category": "\"法律・倫理\"",
        "Question": "\"AIモデルに特定の傾向を学習させるため、学習データに偏りを持たせる攻撃を「[ (1) ] 攻撃」という。\"",
        "Opt1": "\"ポイズニング（毒入れ）\"",
        "Opt2": "\"回避\"",
        "Opt3": "\"抽出\"",
        "Opt4": "\"反転\"",
        "Opt5": "\"推論\"",
        "Opt6": "\"踏み台\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】学習データに悪意あるデータを混入させ、特定の条件下で誤作動を起こさせる手法です。\""
    },
    {
        "ID": "\"L-029\"",
        "Category": "\"法律・倫理\"",
        "Question": "\"生成AIの利用において、機密情報をプロンプトに入力してしまうことで、その情報がモデルの学習に取り込まれ、他者へ漏洩するリスクを [ (1) ] という。\"",
        "Opt1": "\"情報漏洩\"",
        "Opt2": "\"ハッキング\"",
        "Opt3": "\"フィッシング\"",
        "Opt4": "\"なりすまし\"",
        "Opt5": "\"改ざん\"",
        "Opt6": "\"否認\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】入力データが再学習に利用される設定の場合、社外秘の情報が他者の回答として出力されるリスクがあります。\""
    },
    {
        "ID": "\"L-030\"",
        "Category": "\"法律・倫理\"",
        "Question": "\"AIの公平性を評価する際、性別や人種などの直接的な項目を除去しても、郵便番号などの他の項目からそれらが推測されてしまう問題を [ (1) ] という。\"",
        "Opt1": "\"プロキシ（代理）変数\"",
        "Opt2": "\"ダミー変数\"",
        "Opt3": "\"説明変数\"",
        "Opt4": "\"目的変数\"",
        "Opt5": "\"共線性\"",
        "Opt6": "\"交差検証\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】人種と居住地域が相関している場合、地域情報を入れることで実質的に人種による差別が継続してしまう問題です。\""
    },
    {
        "ID": "\"L-031\"",
        "Category": "\"法律・倫理\"",
        "Question": "\"生成AIがユーザーの指示を超えて、危険物の製造方法などを回答しないように制限をかけることを「[ (1) ]」と呼ぶ。\"",
        "Opt1": "\"ガードレール\"",
        "Opt2": "\"ファイアウォール\"",
        "Opt3": "\"サンドボックス\"",
        "Opt4": "\"検疫\"",
        "Opt5": "\"暗号化\"",
        "Opt6": "\"ハッシュ化\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】AIの安全性を確保するための倫理的・技術的な制約をガードレールと呼びます。\""
    },
    {
        "ID": "\"L-032\"",
        "Category": "\"法律・倫理\"",
        "Question": "\"AIが発明をした場合、現在の日本の特許法では、[ (1) ] しか「発明者」になれないと規定されている。\"",
        "Opt1": "\"自然人（人間）\"",
        "Opt2": "\"法人\"",
        "Opt3": "\"AI自体\"",
        "Opt4": "\"国\"",
        "Opt5": "\"特許庁長官\"",
        "Opt6": "\"AI開発者\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】発明者は自然人に限られます。法人は「特許出願人」にはなれますが、発明者にはなれません。\""
    },
    {
        "ID": "\"L-033\"",
        "Category": "\"法律・倫理\"",
        "Question": "\"GDPR（一般データ保護規則）において、個人データの処理に際し、データの「[ (1) ]（最小化）」の原則が求められている。\"",
        "Opt1": "\"データ最小化\"",
        "Opt2": "\"データ最大化\"",
        "Opt3": "\"データ多様化\"",
        "Opt4": "\"データ集中化\"",
        "Opt5": "\"データ暗号化\"",
        "Opt6": "\"データ匿名化\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】目的達成に必要な範囲を超えて個人データを収集してはならないという原則です。\""
    },
    {
        "ID": "\"L-034\"",
        "Category": "\"法律・倫理\"",
        "Question": "\"AIの普及により、特定のスキルを持つ人と持たない人の間で経済的・社会的な格差が広がることを [ (1) ] という。\"",
        "Opt1": "\"デジタルデバイド\"",
        "Opt2": "\"デジタルツイン\"",
        "Opt3": "\"デジタル変革\"",
        "Opt4": "\"デジタル田園都市\"",
        "Opt5": "\"デジタル署名\"",
        "Opt6": "\"デジタル著作権\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】情報格差のことです。AIを使いこなせるかどうかで所得や機会に差が出る懸念があります。\""
    },
    {
        "ID": "\"L-035\"",
        "Category": "\"法律・倫理\"",
        "Question": "\"SNS等でAIが生成した偽の動画や音声を使って、世論操作や詐欺を行うことを [ (1) ] という。\"",
        "Opt1": "\"ディープフェイク\"",
        "Opt2": "\"ディープラーニング\"",
        "Opt3": "\"ディープウェブ\"",
        "Opt4": "\"ディープサーチ\"",
        "Opt5": "\"ディープコピー\"",
        "Opt6": "\"ディープスタック\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】Deep LearningとFakeを組み合わせた造語。偽情報拡散による社会不安が深刻な課題です。\""
    },
    {
        "ID": "\"L-036\"",
        "Category": "\"法律・倫理\"",
        "Question": "\"契約において、AIの出力結果が期待に沿わない場合でも、開発者が責任を負わないとする条項を [ (1) ] 条項という。\"",
        "Opt1": "\"免責\"",
        "Opt2": "\"賠償\"",
        "Opt3": "\"保証\"",
        "Opt4": "\"解除\"",
        "Opt5": "\"合意\"",
        "Opt6": "\"管轄\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】AIの出力は確率的で不確実なため、開発契約では免責条項が非常に重要になります。\""
    },
    {
        "ID": "\"L-037\"",
        "Category": "\"法律・倫理\"",
        "Question": "\"AI開発者が、開発したAIが将来的に軍事転用されることを防ぐための議論や管理を [ (1) ] 管理という。\"",
        "Opt1": "\"デュアルユース（両義性）\"",
        "Opt2": "\"シングルユース\"",
        "Opt3": "\"エンドユース\"",
        "Opt4": "\"ライセンス\"",
        "Opt5": "\"輸出\"",
        "Opt6": "\"品質\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】民生技術が軍事技術に転用可能な「デュアルユース」の懸念は、先端AIにおいて常に議論されます。\""
    },
    {
        "ID": "\"L-038\"",
        "Category": "\"法律・倫理\"",
        "Question": "\"インターネット上の公開情報を収集する際、[ (1) ] ファイル（robots.txtなど）の指示に従うことが、マナーおよび法的なリスク回避として重要である。\"",
        "Opt1": "\"クローラ制限\"",
        "Opt2": "\"実行\"",
        "Opt3": "\"設定\"",
        "Opt4": "\"画像\"",
        "Opt5": "\"音声\"",
        "Opt6": "\"バックアップ\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】サイト運営者が自動収集を拒否する意思表示をしている場合、それに従う必要があります。\""
    },
    {
        "ID": "\"L-039\"",
        "Category": "\"法律・倫理\"",
        "Question": "\"AIの学習データに含まれる個人について、その個人の特徴をAIが再現できてしまうことでプライバシーが侵害されることを [ (1) ] という。\"",
        "Opt1": "\"再識別\"",
        "Opt2": "\"再学習\"",
        "Opt3": "\"再定義\"",
        "Opt4": "\"再配布\"",
        "Opt5": "\"再起動\"",
        "Opt6": "\"再審査\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】匿名化が不十分な場合、AIの出力から元の個人が特定（再識別）されるリスクがあります。\""
    },
    {
        "ID": "\"L-040\"",
        "Category": "\"法律・倫理\"",
        "Question": "\"AIによる採用選考において、過去の男性中心の採用データを学習したAIが女性を低く評価してしまった事例は、[ (1) ] の問題として有名である。\"",
        "Opt1": "\"アルゴリズム・バイアス\"",
        "Opt2": "\"アルゴリズム・ブースト\"",
        "Opt3": "\"アルゴリズム・ハック\"",
        "Opt4": "\"アルゴリズム・トレード\"",
        "Opt5": "\"アルゴリズム・監査\"",
        "Opt6": "\"アルゴリズム・透明性\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】過去のデータに含まれる社会的な偏見をAIが固定化・増幅してしまう問題です。\""
    },
    {
        "ID": "\"L-041\"",
        "Category": "\"法律・倫理\"",
        "Question": "\"生成AIの出力が他者の著作権を侵害しているか判断する際、その出力が既存の著作物を「[ (1) ]」したものであるかが争点となる。\"",
        "Opt1": "\"複製または翻案\"",
        "Opt2": "\"参照\"",
        "Opt3": "\"引用\"",
        "Opt4": "\"転載\"",
        "Opt5": "\"配布\"",
        "Opt6": "\"展示\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】著作権法上の侵害は、コピー（複製）や作り替え（翻案）に該当するかどうかが基準となります。\""
    },
    {
        "ID": "\"L-042\"",
        "Category": "\"法律・倫理\"",
        "Question": "\"AIなどの先進技術が社会に普及する際、法整備が技術の進化に追いつかない現象を [ (1) ] という。\"",
        "Opt1": "\"リーガル・ラッグ（法的な遅れ）\"",
        "Opt2": "\"デジタル・タトゥー\"",
        "Opt3": "\"コンプライアンス\"",
        "Opt4": "\"デプロイメント\"",
        "Opt5": "\"イノベーション\"",
        "Opt6": "\"リーガル・テック\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】技術の進歩速度に対して、法規制や倫理的合意の形成が遅れる課題を指します。\""
    },
    {
        "ID": "\"L-043\"",
        "Category": "\"法律・倫理\"",
        "Question": "\"個人情報保護法における「[ (1) ]」とは、特定の個人を識別できるメールアドレスやSNSのIDなどが含まれる。\"",
        "Opt1": "\"個人識別符号\"",
        "Opt2": "\"個人関連情報\"",
        "Opt3": "\"要配慮個人情報\"",
        "Opt4": "\"匿名加工情報\"",
        "Opt5": "\"仮名加工情報\"",
        "Opt6": "\"機微データ\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】指紋データやパスポート番号など、それ単体で個人を識別できる記号や番号のことです。\""
    },
    {
        "ID": "\"L-044\"",
        "Category": "\"法律・倫理\"",
        "Question": "\"AIシステムの提供者が、利用者に対してシステムの使用方法やリスクについて説明する義務を [ (1) ] 責任という。\"",
        "Opt1": "\"説明責任（アカウンタビリティ）\"",
        "Opt2": "\"製造物責任\"",
        "Opt3": "\"瑕疵担保責任\"",
        "Opt4": "\"善管注意義務\"",
        "Opt5": "\"守秘義務\"",
        "Opt6": "\"自己責任\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】適切な運用が行われるよう、透明性を持って説明する社会的・法的な責任です。\""
    },
    {
        "ID": "\"L-045\"",
        "Category": "\"法律・倫理\"",
        "Question": "\"2023年にG7広島サミットで合意された、生成AIに関する国際的なルール作りの枠組みを [ (1) ] という。\"",
        "Opt1": "\"広島AIプロセス\"",
        "Opt2": "\"東京AI宣言\"",
        "Opt3": "\"京都AI原則\"",
        "Opt4": "\"大阪トラック\"",
        "Opt5": "\"パリ協定\"",
        "Opt6": "\"ジュネーブ条約\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】信頼できるAIの実現に向けた国際的な議論の枠組みです。\""
    },
    {
        "ID": "\"L-046\"",
        "Category": "\"法律・倫理\"",
        "Question": "\"AIモデルが特定の個人の顔を生成できる場合、その個人の「[ (1) ]」に基づく利用制限がかかる。\"",
        "Opt1": "\"パブリシティ権\"",
        "Opt2": "\"特許権\"",
        "Opt3": "\"商標権\"",
        "Opt4": "\"著作権\"",
        "Opt5": "\"意匠権\"",
        "Opt6": "\"実用新案権\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】著名人の名前や肖像が持つ経済的価値を独占的に利用できる権利です。\""
    },
    {
        "ID": "\"L-047\"",
        "Category": "\"法律・倫理\"",
        "Question": "\"ソフトウェア開発において、AIが生成したコードの中に [ (1) ] な脆弱性が含まれている可能性を考慮し、人間によるコードレビューが必要である。\"",
        "Opt1": "\"セキュリティ\"",
        "Opt2": "\"アクセシビリティ\"",
        "Opt3": "\"ユーザビリティ\"",
        "Opt4": "\"スケーラビリティ\"",
        "Opt5": "\"ポータビリティ\"",
        "Opt6": "\"リライアビリティ\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】AIは脆弱性のあるコードも学習しているため、そのまま利用するとセキュリティホールになる恐れがあります。\""
    },
    {
        "ID": "\"L-048\"",
        "Category": "\"法律・倫理\"",
        "Question": "\"AIを悪用して、ターゲットに合わせた精巧なフィッシングメールを作成する攻撃を [ (1) ] という。\"",
        "Opt1": "\"スピアフィッシング\"",
        "Opt2": "\"ランサムウェア\"",
        "Opt3": "\"DDoS攻撃\"",
        "Opt4": "\"SQLインジェクション\"",
        "Opt5": "\"クロスサイトスクリプティング\"",
        "Opt6": "\"バッファオーバフロー\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】AIにより、文脈が自然で騙されやすい「標的型（スピア）」攻撃が容易になっています。\""
    },
    {
        "ID": "\"L-049\"",
        "Category": "\"法律・倫理\"",
        "Question": "\"企業がAIを導入する際、倫理的な問題を検討するために設置される委員会を一般に [ (1) ] という。\"",
        "Opt1": "\"AI倫理委員会\"",
        "Opt2": "\"AI技術委員会\"",
        "Opt3": "\"AI営業部\"",
        "Opt4": "\"AIコンプライアンス室\"",
        "Opt5": "\"AIリスク管理課\"",
        "Opt6": "\"AI監査法人\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】社内でELSI（倫理的・法的・社会的課題）を議論するための合議体です。\""
    },
    {
        "ID": "\"L-050\"",
        "Category": "\"法律・倫理\"",
        "Question": "\"AIの開発から利用まで、すべての段階において「[ (1) ]」を最優先に考えるべきとするのが日本の基本的な考え方である。\"",
        "Opt1": "\"人間中心\"",
        "Opt2": "\"技術中心\"",
        "Opt3": "\"利益中心\"",
        "Opt4": "\"効率中心\"",
        "Opt5": "\"データ中心\"",
        "Opt6": "\"アルゴリズム中心\"",
        "Answer_Idx": 0,
        "Explanation": "\"【解説】「人間中心のAI社会原則」に基づき、AIは人間の生活を豊かにするための道具であるべきとしています。\""
    }
]
